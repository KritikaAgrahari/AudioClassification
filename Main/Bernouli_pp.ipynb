{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f2cd1e-9134-46d5-9325-33bcf4b5295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Compression-Classification Pipeline\n",
      "======================================================================\n",
      "Running quick compression test...\n",
      "Original signal shape: (1024,)\n",
      "\n",
      "1. Bernoulli Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "2. DWT (Haar) Compression (50%):\n",
      "   Compressed coefficients: 512\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "3. Hybrid Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "Quick test completed successfully!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to run full compression-classification experiments? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\n",
      "======================================================================\n",
      "======================================================================\n",
      "COMPRESSION-CLASSIFICATION EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "Preparing data...\n",
      "Found 7079 valid audio files out of 6448 expected\n",
      "Found 1653 valid audio files out of 1674 expected\n",
      "\n",
      "============================================================\n",
      "Compression Method: BERNOULLI\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.25: 100%|████████████████████████████████████████████████████████| 7079/7079 [17:07<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.25: 100%|█████████████████████████████████████████████████████████| 1653/1653 [04:23<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.4997, F1: 0.4937, Time: 5.29s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5306, F1: 0.5182, Time: 41.20s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5493, F1: 0.5477, Time: 52.81s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.5: 100%|█████████████████████████████████████████████████████████| 7079/7079 [22:40<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.5: 100%|██████████████████████████████████████████████████████████| 1653/1653 [04:27<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5070, F1: 0.5007, Time: 7.31s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5269, F1: 0.5141, Time: 71.06s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5590, F1: 0.5558, Time: 73.13s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.75: 100%|████████████████████████████████████████████████████████| 7079/7079 [20:30<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.75: 100%|█████████████████████████████████████████████████████████| 1653/1653 [04:46<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5045, F1: 0.4968, Time: 9.02s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5293, F1: 0.5169, Time: 102.10s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5626, F1: 0.5605, Time: 77.28s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 1.0: 100%|█████████████████████████████████████████████████████████| 7079/7079 [14:41<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 1.0: 100%|██████████████████████████████████████████████████████████| 1653/1653 [03:29<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7387, F1: 0.7353, Time: 8.17s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7586, F1: 0.7555, Time: 123.94s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7623, F1: 0.7598, Time: 77.12s\n",
      "\n",
      "============================================================\n",
      "Compression Method: DWT\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.25: 100%|██████████████████████████████████████████████████████████████| 7079/7079 [16:15<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.25: 100%|███████████████████████████████████████████████████████████████| 1653/1653 [03:45<00:00,  7.34it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along axis 0; size of axis is 1287 but size of corresponding boolean axis is 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1219\u001b[39m\n\u001b[32m   1217\u001b[39m response = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDo you want to run full compression-classification experiments? (yes/no): \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m1219\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1221\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuick test completed. Run full experiments when ready.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1142\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1139\u001b[39m experiment = CompressionExperiment(DATASET_PATH)\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m results = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 714\u001b[39m, in \u001b[36mCompressionExperiment.run_experiments\u001b[39m\u001b[34m(self, train_folds, test_folds)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;66;03m# Calculate reconstruction error if not 'none'\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method != \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ratio < \u001b[32m1.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m     avg_error = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_reconstruction_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_compressed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp_info\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m     reconstruction_errors.append({\n\u001b[32m    718\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m: method,\n\u001b[32m    719\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mratio\u001b[39m\u001b[33m'\u001b[39m: ratio,\n\u001b[32m    720\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mreconstruction_error\u001b[39m\u001b[33m'\u001b[39m: avg_error\n\u001b[32m    721\u001b[39m     })\n\u001b[32m    723\u001b[39m \u001b[38;5;66;03m# Train and evaluate classifiers\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 809\u001b[39m, in \u001b[36mCompressionExperiment.calculate_reconstruction_error\u001b[39m\u001b[34m(self, original_features, compressed_features, method, ratio, comp_info)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcompressor\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m comp_info \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m comp_info:\n\u001b[32m    807\u001b[39m     \u001b[38;5;66;03m# Reconstruct compressed coefficients\u001b[39;00m\n\u001b[32m    808\u001b[39m     full_coeffs = np.zeros(\u001b[38;5;28mlen\u001b[39m(orig))\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     \u001b[43mfull_coeffs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomp_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m = comp\n\u001b[32m    810\u001b[39m     reconstructed = comp_info[\u001b[33m'\u001b[39m\u001b[33mcompressor\u001b[39m\u001b[33m'\u001b[39m].reconstruct(full_coeffs)\n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# Calculate error\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: boolean index did not match indexed array along axis 0; size of axis is 1287 but size of corresponding boolean axis is 2048"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score)\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Enhanced Feature Extraction (Base Class)\n",
    "# ============================================\n",
    "\n",
    "class RobustMFCCExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced MFCC feature extraction with multiple robust techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=40, n_fft=2048, hop_length=512):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_mfcc_features(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract comprehensive MFCC-based features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with robust loading\n",
    "            y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "            \n",
    "            # Zero-padding or truncation for consistent length\n",
    "            target_length = self.sr * 4  # 4 seconds\n",
    "            if len(y) < target_length:\n",
    "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:target_length]\n",
    "            \n",
    "            # Apply pre-emphasis filter\n",
    "            y = librosa.effects.preemphasis(y)\n",
    "            \n",
    "            # Extract base MFCCs\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Extract delta and delta-delta features\n",
    "            mfcc_delta = librosa.feature.delta(mfccs)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "            \n",
    "            # Extract other complementary features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Root Mean Square Energy\n",
    "            rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Spectral Centroid and Rolloff\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.vstack([\n",
    "                mfccs,\n",
    "                mfcc_delta,\n",
    "                mfcc_delta2,\n",
    "                chroma,\n",
    "                spectral_contrast,\n",
    "                rms,\n",
    "                zcr,\n",
    "                spectral_centroid,\n",
    "                spectral_rolloff\n",
    "            ])\n",
    "            \n",
    "            # Extract statistical features\n",
    "            feature_stats = self._extract_statistics(features)\n",
    "            \n",
    "            return feature_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            # Return zeros with appropriate dimension\n",
    "            return np.zeros(1026)  # Fixed dimension\n",
    "    \n",
    "    def _extract_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Extract statistical features from feature matrix\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for feature in features:\n",
    "            stats.extend([\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.median(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 75),\n",
    "                np.mean(np.diff(feature)),  # Average change\n",
    "                np.std(np.diff(feature))    # Std of changes\n",
    "            ])\n",
    "        return np.array(stats)\n",
    "    \n",
    "    def extract_all_features(self, audio_paths, labels=None):\n",
    "        \"\"\"\n",
    "        Extract features from multiple audio files\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, path in enumerate(tqdm(audio_paths, desc=\"Extracting features\")):\n",
    "            feat = self.extract_mfcc_features(path)\n",
    "            if np.sum(feat) != 0 and not np.isnan(feat).any():  # Skip zero or NaN features\n",
    "                features.append(feat)\n",
    "                if labels is not None:\n",
    "                    valid_labels.append(labels[idx])\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"WARNING: No valid features extracted!\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Normalize features\n",
    "        features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Encode labels if provided\n",
    "        if labels is not None and len(valid_labels) > 0:\n",
    "            encoded_labels = self.label_encoder.fit_transform(valid_labels)\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        \n",
    "        return features, encoded_labels\n",
    "\n",
    "# ============================================\n",
    "# 2. UrbanSound8K Processor\n",
    "# ============================================\n",
    "\n",
    "class UrbanSound8KProcessor:\n",
    "    \"\"\"\n",
    "    Process UrbanSound8K dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            # Try alternative path structure\n",
    "            self.metadata_path = os.path.join(dataset_path, 'UrbanSound8K.csv')\n",
    "        \n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {self.metadata_path}\")\n",
    "        \n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        \n",
    "    def prepare_data(self, folds=None):\n",
    "        \"\"\"\n",
    "        Prepare data for specific folds or all folds\n",
    "        \"\"\"\n",
    "        if folds is None:\n",
    "            folds = list(range(1, 11))\n",
    "        elif isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        \n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        fold_numbers = []\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_data = self.metadata[self.metadata['fold'] == fold]\n",
    "            \n",
    "            for _, row in fold_data.iterrows():\n",
    "                # Try different possible paths\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.dataset_path, 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, 'audio', 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, str(row['fold']), row['slice_file_name'])\n",
    "                ]\n",
    "                \n",
    "                audio_file = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        audio_file = path\n",
    "                        break\n",
    "                \n",
    "                if audio_file:\n",
    "                    audio_paths.append(audio_file)\n",
    "                    labels.append(row['class'])\n",
    "                    fold_numbers.append(fold)\n",
    "                else:\n",
    "                    print(f\"WARNING: File not found: {row['slice_file_name']} in fold {fold}\")\n",
    "        \n",
    "        print(f\"Found {len(audio_paths)} valid audio files out of {len(fold_data) * len(folds)} expected\")\n",
    "        return audio_paths, labels, fold_numbers\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Get class distribution statistics\n",
    "        \"\"\"\n",
    "        return self.metadata['class'].value_counts()\n",
    "\n",
    "# ============================================\n",
    "# 3. Compression Sensing Modules\n",
    "# ============================================\n",
    "\n",
    "class BernoulliCompressor:\n",
    "    \"\"\"Bernoulli Random Matrix Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize Bernoulli compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.measurement_matrix = None\n",
    "        self.reconstruction_matrix = None\n",
    "        \n",
    "    def create_bernoulli_matrix(self, n_original, n_compressed):\n",
    "        \"\"\"\n",
    "        Create Bernoulli random measurement matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_original : int\n",
    "            Original signal dimension\n",
    "        n_compressed : int\n",
    "            Compressed dimension\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        measurement_matrix : np.array\n",
    "            Bernoulli random matrix\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Bernoulli matrix with entries +1/sqrt(n_compressed) and -1/sqrt(n_compressed)\n",
    "        bernoulli_values = np.random.choice([1, -1], size=(n_compressed, n_original))\n",
    "        bernoulli_matrix = bernoulli_values / np.sqrt(n_compressed)\n",
    "        return bernoulli_matrix\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using Bernoulli random matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (1D array)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        \"\"\"\n",
    "        n_original = len(signal)\n",
    "        n_compressed = int(n_original * self.compression_ratio)\n",
    "        \n",
    "        # Create measurement matrix\n",
    "        self.measurement_matrix = self.create_bernoulli_matrix(n_original, n_compressed)\n",
    "        \n",
    "        # Compress signal\n",
    "        compressed_signal = np.dot(self.measurement_matrix, signal)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct_l1(self, compressed_signal, max_iter=100):\n",
    "        \"\"\"\n",
    "        Reconstruct original signal using L1 minimization (Basis Pursuit)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        max_iter : int\n",
    "            Maximum iterations for reconstruction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        if self.measurement_matrix is None:\n",
    "            raise ValueError(\"Measurement matrix not created. Run compress() first.\")\n",
    "        \n",
    "        n_original = self.measurement_matrix.shape[1]\n",
    "        \n",
    "        # Use least squares with L1 regularization (simplified)\n",
    "        reconstructed_signal = lsqr(self.measurement_matrix, compressed_signal, iter_lim=max_iter)[0]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class DWTCompressor:\n",
    "    \"\"\"Discrete Wavelet Transform (Haar) Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, wavelet='haar'):\n",
    "        \"\"\"\n",
    "        Initialize DWT compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        wavelet : str\n",
    "            Wavelet type (default: 'haar')\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.wavelet = wavelet\n",
    "        \n",
    "    def haar_transform(self, signal):\n",
    "        \"\"\"\n",
    "        Apply Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (length must be power of 2)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "        \"\"\"\n",
    "        n = len(signal)\n",
    "        coeffs = np.zeros_like(signal, dtype=float)\n",
    "        \n",
    "        # Simple Haar transform implementation\n",
    "        temp = signal.copy()\n",
    "        length = n\n",
    "        \n",
    "        while length > 1:\n",
    "            for i in range(length // 2):\n",
    "                avg = (temp[2*i] + temp[2*i+1]) / np.sqrt(2)\n",
    "                diff = (temp[2*i] - temp[2*i+1]) / np.sqrt(2)\n",
    "                coeffs[i] = avg\n",
    "                coeffs[length // 2 + i] = diff\n",
    "            temp[:length] = coeffs[:length]\n",
    "            length //= 2\n",
    "            \n",
    "        return coeffs\n",
    "    \n",
    "    def inverse_haar_transform(self, coeffs):\n",
    "        \"\"\"\n",
    "        Apply inverse Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        n = len(coeffs)\n",
    "        signal = coeffs.copy()\n",
    "        length = 2\n",
    "        \n",
    "        while length <= n:\n",
    "            temp = signal.copy()\n",
    "            for i in range(length // 2):\n",
    "                signal[2*i] = (temp[i] + temp[length // 2 + i]) / np.sqrt(2)\n",
    "                signal[2*i+1] = (temp[i] - temp[length // 2 + i]) / np.sqrt(2)\n",
    "            length *= 2\n",
    "            \n",
    "        return signal\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using DWT and thresholding\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal (thresholded coefficients)\n",
    "        compression_mask : np.array\n",
    "            Mask indicating which coefficients were kept\n",
    "        \"\"\"\n",
    "        # Apply wavelet transform\n",
    "        coeffs = self.haar_transform(signal)\n",
    "        \n",
    "        # Keep only largest coefficients based on compression ratio\n",
    "        n_coeffs = len(coeffs)\n",
    "        n_keep = int(n_coeffs * self.compression_ratio)\n",
    "        \n",
    "        # Get indices of largest absolute coefficients\n",
    "        indices = np.argsort(np.abs(coeffs))[-n_keep:]\n",
    "        compression_mask = np.zeros(n_coeffs, dtype=bool)\n",
    "        compression_mask[indices] = True\n",
    "        \n",
    "        # Create compressed signal (only keep selected coefficients)\n",
    "        compressed_coeffs = np.zeros_like(coeffs)\n",
    "        compressed_coeffs[indices] = coeffs[indices]\n",
    "        \n",
    "        return compressed_coeffs, compression_mask\n",
    "    \n",
    "    def reconstruct(self, compressed_coeffs):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from compressed coefficients\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_coeffs : np.array\n",
    "            Compressed wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Apply inverse transform\n",
    "        reconstructed_signal = self.inverse_haar_transform(compressed_coeffs)\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class HybridCompressor:\n",
    "    \"\"\"Hybrid Bernoulli + DWT Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize hybrid compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Overall compression ratio\n",
    "        seed : int\n",
    "            Random seed for Bernoulli matrix\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.bernoulli_compressor = BernoulliCompressor(compression_ratio, seed)\n",
    "        self.dwt_compressor = DWTCompressor(1.0)  # No compression in DWT stage\n",
    "        \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Apply DWT then Bernoulli compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Apply DWT\n",
    "        dwt_coeffs = self.dwt_compressor.haar_transform(signal)\n",
    "        \n",
    "        # Step 2: Apply Bernoulli compression on DWT coefficients\n",
    "        compressed_signal = self.bernoulli_compressor.compress(dwt_coeffs)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct(self, compressed_signal):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from hybrid compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Reconstruct DWT coefficients\n",
    "        dwt_coeffs = self.bernoulli_compressor.reconstruct_l1(compressed_signal)\n",
    "        \n",
    "        # Step 2: Apply inverse DWT\n",
    "        reconstructed_signal = self.dwt_compressor.inverse_haar_transform(dwt_coeffs)\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "# ============================================\n",
    "# 4. Enhanced Feature Extraction with Compression\n",
    "# ============================================\n",
    "\n",
    "class CompressedFeatureExtractor(RobustMFCCExtractor):\n",
    "    \"\"\"\n",
    "    Feature extractor with compression capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=40, n_fft=2048, hop_length=512):\n",
    "        super().__init__(sr, n_mfcc, n_fft, hop_length)\n",
    "        \n",
    "    def extract_features_with_compression(self, audio_path, compression_method='bernoulli', \n",
    "                                          compression_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Extract features with optional compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        audio_path : str\n",
    "            Path to audio file\n",
    "        compression_method : str\n",
    "            'bernoulli', 'dwt', 'hybrid', or 'none'\n",
    "        compression_ratio : float\n",
    "            Compression ratio (0-1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : np.array\n",
    "            Extracted features (compressed or original)\n",
    "        original_features : np.array\n",
    "            Original features before compression\n",
    "        compression_info : dict\n",
    "            Compression metadata\n",
    "        \"\"\"\n",
    "        # Extract original features\n",
    "        original_features = self.extract_mfcc_features(audio_path)\n",
    "        \n",
    "        if compression_method == 'none' or compression_ratio >= 1.0:\n",
    "            return original_features, original_features, {'method': 'none', 'ratio': 1.0}\n",
    "        \n",
    "        # Ensure features are 1D and have appropriate length\n",
    "        features_1d = original_features.flatten()\n",
    "        n_original = len(features_1d)\n",
    "        \n",
    "        # Make length a power of 2 for DWT\n",
    "        if compression_method in ['dwt', 'hybrid']:\n",
    "            n_padded = 2 ** int(np.ceil(np.log2(n_original)))\n",
    "            features_1d = np.pad(features_1d, (0, n_padded - n_original), 'constant')\n",
    "        \n",
    "        compression_info = {\n",
    "            'method': compression_method,\n",
    "            'ratio': compression_ratio,\n",
    "            'original_length': len(features_1d),\n",
    "            'compressed_length': int(len(features_1d) * compression_ratio)\n",
    "        }\n",
    "        \n",
    "        # Apply compression\n",
    "        if compression_method == 'bernoulli':\n",
    "            compressor = BernoulliCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_1d)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        elif compression_method == 'dwt':\n",
    "            compressor = DWTCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_coeffs, mask = compressor.compress(features_1d)\n",
    "            compressed_features = compressed_coeffs[mask]\n",
    "            compression_info['compressor'] = compressor\n",
    "            compression_info['mask'] = mask\n",
    "            \n",
    "        elif compression_method == 'hybrid':\n",
    "            compressor = HybridCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_1d)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown compression method: {compression_method}\")\n",
    "        \n",
    "        return compressed_features, original_features, compression_info\n",
    "\n",
    "# ============================================\n",
    "# 5. Compression Experiment Pipeline\n",
    "# ============================================\n",
    "\n",
    "class CompressionExperiment:\n",
    "    \"\"\"\n",
    "    Pipeline for compression and classification experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, output_dir='compression_results'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Experiment configurations\n",
    "        self.compression_methods = ['bernoulli', 'dwt', 'hybrid', 'none']\n",
    "        self.compression_ratios = [0.25, 0.50, 0.75, 1.0]  # 1.0 = no compression\n",
    "        self.classifiers = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'svm': SVC(probability=True, random_state=42),\n",
    "            'xgboost': XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        }\n",
    "        \n",
    "    def run_experiments(self, train_folds=list(range(1, 9)), test_folds=[9, 10]):\n",
    "        \"\"\"\n",
    "        Run full compression-classification experiments\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_folds : list\n",
    "            Folds to use for training\n",
    "        test_folds : list\n",
    "            Folds to use for testing\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"COMPRESSION-CLASSIFICATION EXPERIMENTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize processor and extractor\n",
    "        processor = UrbanSound8KProcessor(self.dataset_path)\n",
    "        extractor = CompressedFeatureExtractor()\n",
    "        \n",
    "        # Prepare data\n",
    "        print(\"\\nPreparing data...\")\n",
    "        train_paths, train_labels, _ = processor.prepare_data(train_folds)\n",
    "        test_paths, test_labels, _ = processor.prepare_data(test_folds)\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_full = label_encoder.fit_transform(train_labels)\n",
    "        y_test_full = label_encoder.transform(test_labels)\n",
    "        \n",
    "        # Store all results\n",
    "        all_results = []\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        # Run experiments for each compression configuration\n",
    "        for method in self.compression_methods:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Compression Method: {method.upper()}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            for ratio in self.compression_ratios:\n",
    "                print(f\"\\nCompression Ratio: {ratio*100:.0f}%\")\n",
    "                print(\"-\"*40)\n",
    "                \n",
    "                # Extract features with compression\n",
    "                X_train_compressed = []\n",
    "                X_test_compressed = []\n",
    "                X_train_original = []\n",
    "                X_test_original = []\n",
    "                \n",
    "                # Process training data\n",
    "                print(\"Processing training data...\")\n",
    "                for path in tqdm(train_paths, desc=f\"Train {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, _ = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_train_compressed.append(compressed_feat)\n",
    "                    X_train_original.append(original_feat)\n",
    "                \n",
    "                # Process test data\n",
    "                print(\"Processing test data...\")\n",
    "                for path in tqdm(test_paths, desc=f\"Test {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, comp_info = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_test_compressed.append(compressed_feat)\n",
    "                    X_test_original.append(original_feat)\n",
    "                \n",
    "                # Convert to arrays\n",
    "                X_train = np.array(X_train_compressed)\n",
    "                X_test = np.array(X_test_compressed)\n",
    "                X_train_orig = np.array(X_train_original)\n",
    "                X_test_orig = np.array(X_test_original)\n",
    "                \n",
    "                # Pad sequences if necessary (for variable length from compression)\n",
    "                if X_train.ndim == 1 or X_test.ndim == 1:\n",
    "                    # Find max length\n",
    "                    max_len = max(\n",
    "                        max([len(x) for x in X_train]) if len(X_train) > 0 else 0,\n",
    "                        max([len(x) for x in X_test]) if len(X_test) > 0 else 0\n",
    "                    )\n",
    "                    \n",
    "                    # Pad sequences\n",
    "                    X_train = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                      for x in X_train])\n",
    "                    X_test = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                     for x in X_test])\n",
    "                \n",
    "                # Normalize features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Calculate reconstruction error if not 'none'\n",
    "                if method != 'none' and ratio < 1.0:\n",
    "                    avg_error = self.calculate_reconstruction_error(\n",
    "                        X_train_orig, X_train_compressed, method, ratio, comp_info\n",
    "                    )\n",
    "                    reconstruction_errors.append({\n",
    "                        'method': method,\n",
    "                        'ratio': ratio,\n",
    "                        'reconstruction_error': avg_error\n",
    "                    })\n",
    "                \n",
    "                # Train and evaluate classifiers\n",
    "                for clf_name, clf in self.classifiers.items():\n",
    "                    print(f\"  Training {clf_name}...\")\n",
    "                    \n",
    "                    # Train classifier\n",
    "                    start_time = time.time()\n",
    "                    clf.fit(X_train_scaled, y_train_full)\n",
    "                    train_time = time.time() - start_time\n",
    "                    \n",
    "                    # Predict\n",
    "                    start_time = time.time()\n",
    "                    y_pred = clf.predict(X_test_scaled)\n",
    "                    test_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    accuracy = accuracy_score(y_test_full, y_pred)\n",
    "                    precision = precision_score(y_test_full, y_pred, average='weighted')\n",
    "                    recall = recall_score(y_test_full, y_pred, average='weighted')\n",
    "                    f1 = f1_score(y_test_full, y_pred, average='weighted')\n",
    "                    \n",
    "                    # ROC-AUC if available\n",
    "                    try:\n",
    "                        if hasattr(clf, 'predict_proba'):\n",
    "                            y_proba = clf.predict_proba(X_test_scaled)\n",
    "                            roc_auc = roc_auc_score(y_test_full, y_proba, multi_class='ovr', average='weighted')\n",
    "                        else:\n",
    "                            roc_auc = np.nan\n",
    "                    except:\n",
    "                        roc_auc = np.nan\n",
    "                    \n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'compression_method': method,\n",
    "                        'compression_ratio': ratio,\n",
    "                        'classifier': clf_name,\n",
    "                        'accuracy': accuracy,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1_score': f1,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'train_time': train_time,\n",
    "                        'test_time': test_time,\n",
    "                        'feature_dim_original': X_train_orig.shape[1] if len(X_train_orig.shape) > 1 else X_train_orig.shape[0],\n",
    "                        'feature_dim_compressed': X_train.shape[1] if len(X_train.shape) > 1 else X_train.shape[0],\n",
    "                        'compression_rate': (1 - ratio) * 100,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    print(f\"    Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Time: {train_time:.2f}s\")\n",
    "        \n",
    "        # Save all results\n",
    "        self.save_results(all_results, reconstruction_errors)\n",
    "        \n",
    "        # Generate comprehensive reports\n",
    "        self.generate_reports(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_reconstruction_error(self, original_features, compressed_features, \n",
    "                                      method, ratio, comp_info):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error between original and compressed features\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for orig, comp in zip(original_features, compressed_features):\n",
    "            if method == 'bernoulli':\n",
    "                # Reconstruct using compressor\n",
    "                if 'compressor' in comp_info:\n",
    "                    reconstructed = comp_info['compressor'].reconstruct_l1(comp[:int(len(orig) * ratio)])\n",
    "                    # Pad or truncate to match original length\n",
    "                    if len(reconstructed) < len(orig):\n",
    "                        reconstructed = np.pad(reconstructed, (0, len(orig) - len(reconstructed)), 'constant')\n",
    "                    elif len(reconstructed) > len(orig):\n",
    "                        reconstructed = reconstructed[:len(orig)]\n",
    "                    \n",
    "                    # Calculate error\n",
    "                    error = np.mean((orig - reconstructed) ** 2)\n",
    "                    errors.append(error)\n",
    "            \n",
    "            elif method == 'dwt':\n",
    "                if 'compressor' in comp_info and 'mask' in comp_info:\n",
    "                    # Reconstruct compressed coefficients\n",
    "                    full_coeffs = np.zeros(len(orig))\n",
    "                    full_coeffs[comp_info['mask']] = comp\n",
    "                    reconstructed = comp_info['compressor'].reconstruct(full_coeffs)\n",
    "                    \n",
    "                    # Calculate error\n",
    "                    error = np.mean((orig - reconstructed) ** 2)\n",
    "                    errors.append(error)\n",
    "        \n",
    "        return np.mean(errors) if errors else np.nan\n",
    "    \n",
    "    def save_results(self, all_results, reconstruction_errors):\n",
    "        \"\"\"\n",
    "        Save experiment results to files\n",
    "        \"\"\"\n",
    "        # Save main results\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv(os.path.join(self.output_dir, 'compression_results.csv'), index=False)\n",
    "        \n",
    "        # Save reconstruction errors\n",
    "        if reconstruction_errors:\n",
    "            recon_df = pd.DataFrame(reconstruction_errors)\n",
    "            recon_df.to_csv(os.path.join(self.output_dir, 'reconstruction_errors.csv'), index=False)\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary = self.create_summary_statistics(results_df)\n",
    "        with open(os.path.join(self.output_dir, 'summary.json'), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults saved to {self.output_dir}/\")\n",
    "    \n",
    "    def create_summary_statistics(self, results_df):\n",
    "        \"\"\"\n",
    "        Create comprehensive summary statistics\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'experiment_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'total_experiments': len(results_df),\n",
    "            'compression_methods_tested': list(results_df['compression_method'].unique()),\n",
    "            'compression_ratios_tested': list(results_df['compression_ratio'].unique()),\n",
    "            'classifiers_tested': list(results_df['classifier'].unique()),\n",
    "        }\n",
    "        \n",
    "        # Best results by compression method\n",
    "        best_by_method = {}\n",
    "        for method in summary['compression_methods_tested']:\n",
    "            method_df = results_df[results_df['compression_method'] == method]\n",
    "            if not method_df.empty:\n",
    "                best_idx = method_df['accuracy'].idxmax()\n",
    "                best_by_method[method] = method_df.loc[best_idx].to_dict()\n",
    "        \n",
    "        summary['best_by_method'] = best_by_method\n",
    "        \n",
    "        # Compression vs Accuracy analysis\n",
    "        compression_analysis = {}\n",
    "        for ratio in summary['compression_ratios_tested']:\n",
    "            ratio_df = results_df[results_df['compression_ratio'] == ratio]\n",
    "            if not ratio_df.empty:\n",
    "                compression_analysis[f'ratio_{ratio}'] = {\n",
    "                    'avg_accuracy': ratio_df['accuracy'].mean(),\n",
    "                    'avg_f1': ratio_df['f1_score'].mean(),\n",
    "                    'avg_feature_dim': ratio_df['feature_dim_compressed'].mean(),\n",
    "                    'compression_rate': (1 - ratio) * 100\n",
    "                }\n",
    "        \n",
    "        summary['compression_analysis'] = compression_analysis\n",
    "        \n",
    "        # Overall best configuration\n",
    "        overall_best_idx = results_df['accuracy'].idxmax()\n",
    "        summary['overall_best'] = results_df.loc[overall_best_idx].to_dict()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_reports(self, all_results):\n",
    "        \"\"\"\n",
    "        Generate visual reports and analysis\n",
    "        \"\"\"\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # 1. Accuracy vs Compression Ratio plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for method in results_df['compression_method'].unique():\n",
    "            method_df = results_df[results_df['compression_method'] == method]\n",
    "            for clf in results_df['classifier'].unique():\n",
    "                clf_df = method_df[method_df['classifier'] == clf]\n",
    "                if not clf_df.empty:\n",
    "                    plt.plot(clf_df['compression_ratio'], clf_df['accuracy'], \n",
    "                            marker='o', label=f'{method}-{clf}')\n",
    "        \n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title('Classification Accuracy vs Compression Ratio', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'accuracy_vs_compression.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Feature Dimension Reduction plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        unique_ratios = sorted(results_df['compression_ratio'].unique())\n",
    "        avg_dims = []\n",
    "        \n",
    "        for ratio in unique_ratios:\n",
    "            ratio_df = results_df[results_df['compression_ratio'] == ratio]\n",
    "            avg_dim = ratio_df['feature_dim_compressed'].mean()\n",
    "            avg_dims.append(avg_dim)\n",
    "        \n",
    "        plt.plot(unique_ratios, avg_dims, 'b-o', linewidth=2)\n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Average Feature Dimension', fontsize=12)\n",
    "        plt.title('Feature Dimension Reduction', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'feature_dimension_reduction.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Heatmap of accuracy by method and ratio\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot_table = results_df.pivot_table(\n",
    "            values='accuracy',\n",
    "            index='compression_method',\n",
    "            columns='compression_ratio',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'Accuracy'})\n",
    "        plt.title('Average Accuracy by Compression Method and Ratio', fontsize=14)\n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Compression Method', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'accuracy_heatmap.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Training Time Comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for clf in results_df['classifier'].unique():\n",
    "            clf_df = results_df[results_df['classifier'] == clf]\n",
    "            for method in results_df['compression_method'].unique():\n",
    "                method_clf_df = clf_df[clf_df['compression_method'] == method]\n",
    "                if not method_clf_df.empty:\n",
    "                    plt.plot(method_clf_df['compression_ratio'], \n",
    "                            method_clf_df['train_time'], \n",
    "                            marker='s', label=f'{method}-{clf}')\n",
    "        \n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "        plt.title('Training Time vs Compression Ratio', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'training_time.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 5. Create detailed report HTML\n",
    "        self.create_html_report(results_df)\n",
    "    \n",
    "    def create_html_report(self, results_df):\n",
    "        \"\"\"\n",
    "        Create HTML report of experiment results\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Compression-Classification Experiment Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                h1, h2, h3 {{ color: #333; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "                .best {{ background-color: #d4edda; }}\n",
    "                .summary {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; }}\n",
    "                .image {{ max-width: 100%; height: auto; margin: 20px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Audio Compression-Classification Experiment Report</h1>\n",
    "            <p>Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "            \n",
    "            <div class=\"summary\">\n",
    "                <h2>Experiment Summary</h2>\n",
    "                <p><strong>Total Experiments:</strong> {len(results_df)}</p>\n",
    "                <p><strong>Compression Methods:</strong> {', '.join(results_df['compression_method'].unique())}</p>\n",
    "                <p><strong>Compression Ratios:</strong> {', '.join([str(r) for r in sorted(results_df['compression_ratio'].unique())])}</p>\n",
    "                <p><strong>Classifiers:</strong> {', '.join(results_df['classifier'].unique())}</p>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Overall Best Configuration</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Metric</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find overall best\n",
    "        best_idx = results_df['accuracy'].idxmax()\n",
    "        best_row = results_df.loc[best_idx]\n",
    "        \n",
    "        for metric in ['compression_method', 'compression_ratio', 'classifier', \n",
    "                      'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc',\n",
    "                      'train_time', 'test_time', 'feature_dim_compressed']:\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{metric.replace('_', ' ').title()}</td>\n",
    "                    <td>{best_row[metric]:.4f if isinstance(best_row[metric], float) else best_row[metric]}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Top 10 Performances</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Rank</th>\n",
    "                    <th>Method</th>\n",
    "                    <th>Ratio</th>\n",
    "                    <th>Classifier</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>F1 Score</th>\n",
    "                    <th>Feature Dim</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sort by accuracy and get top 10\n",
    "        top_results = results_df.sort_values('accuracy', ascending=False).head(10)\n",
    "        for idx, (_, row) in enumerate(top_results.iterrows(), 1):\n",
    "            html_content += f\"\"\"\n",
    "                <tr class=\"{'best' if idx == 1 else ''}\">\n",
    "                    <td>{idx}</td>\n",
    "                    <td>{row['compression_method']}</td>\n",
    "                    <td>{row['compression_ratio']}</td>\n",
    "                    <td>{row['classifier']}</td>\n",
    "                    <td>{row['accuracy']:.4f}</td>\n",
    "                    <td>{row['f1_score']:.4f}</td>\n",
    "                    <td>{int(row['feature_dim_compressed'])}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Visualizations</h2>\n",
    "            <p>The following visualizations have been generated:</p>\n",
    "            <ul>\n",
    "                <li>accuracy_vs_compression.png - Accuracy vs Compression Ratio</li>\n",
    "                <li>feature_dimension_reduction.png - Feature Dimension Reduction</li>\n",
    "                <li>accuracy_heatmap.png - Accuracy Heatmap</li>\n",
    "                <li>training_time.png - Training Time Analysis</li>\n",
    "            </ul>\n",
    "            \n",
    "            <h2>Detailed Results Table</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Method</th>\n",
    "                    <th>Ratio</th>\n",
    "                    <th>Classifier</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>Precision</th>\n",
    "                    <th>Recall</th>\n",
    "                    <th>F1</th>\n",
    "                    <th>Train Time</th>\n",
    "                    <th>Test Time</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in results_df.iterrows():\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{row['compression_method']}</td>\n",
    "                    <td>{row['compression_ratio']}</td>\n",
    "                    <td>{row['classifier']}</td>\n",
    "                    <td>{row['accuracy']:.4f}</td>\n",
    "                    <td>{row['precision']:.4f}</td>\n",
    "                    <td>{row['recall']:.4f}</td>\n",
    "                    <td>{row['f1_score']:.4f}</td>\n",
    "                    <td>{row['train_time']:.2f}s</td>\n",
    "                    <td>{row['test_time']:.2f}s</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <footer>\n",
    "                <p>Experiment conducted using UrbanSound8K dataset</p>\n",
    "                <p>Compression methods: Bernoulli, DWT (Haar), Hybrid</p>\n",
    "            </footer>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save HTML report\n",
    "        with open(os.path.join(self.output_dir, 'experiment_report.html'), 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"\\nHTML report generated: {self.output_dir}/experiment_report.html\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Main Execution\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize paths\n",
    "    DATASET_PATH = \"UrbanSound8K\"\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "        print(\"\\nPlease download UrbanSound8K dataset from:\")\n",
    "        print(\"https://urbansounddataset.weebly.com/urbansound8k.html\")\n",
    "        print(\"\\nExtract it to the current directory as 'UrbanSound8K'\")\n",
    "        return\n",
    "    \n",
    "    # Create experiment instance\n",
    "    experiment = CompressionExperiment(DATASET_PATH)\n",
    "    \n",
    "    # Run experiments\n",
    "    results = experiment.run_experiments()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load and display summary\n",
    "    summary_path = os.path.join('compression_results', 'summary.json')\n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\nTotal experiments conducted: {summary['total_experiments']}\")\n",
    "        \n",
    "        # Show overall best\n",
    "        best = summary['overall_best']\n",
    "        print(\"\\nOVERALL BEST CONFIGURATION:\")\n",
    "        print(f\"  Method: {best['compression_method']}\")\n",
    "        print(f\"  Ratio: {best['compression_ratio']}\")\n",
    "        print(f\"  Classifier: {best['classifier']}\")\n",
    "        print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "        print(f\"  Feature Dimension: {int(best['feature_dim_compressed'])}\")\n",
    "        print(f\"  Compression Rate: {best['compression_rate']:.1f}%\")\n",
    "        \n",
    "        # Show best by method\n",
    "        print(\"\\nBEST BY COMPRESSION METHOD:\")\n",
    "        for method, config in summary['best_by_method'].items():\n",
    "            print(f\"  {method}: {config['accuracy']:.4f} \"\n",
    "                  f\"(Ratio: {config['compression_ratio']}, \"\n",
    "                  f\"Classifier: {config['classifier']})\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"\n",
    "    Quick test to verify compression algorithms\n",
    "    \"\"\"\n",
    "    print(\"Running quick compression test...\")\n",
    "    \n",
    "    # Generate test signal\n",
    "    np.random.seed(42)\n",
    "    test_signal = np.random.randn(1024)\n",
    "    \n",
    "    print(f\"Original signal shape: {test_signal.shape}\")\n",
    "    \n",
    "    # Test Bernoulli compression\n",
    "    print(\"\\n1. Bernoulli Compression (50%):\")\n",
    "    bernoulli = BernoulliCompressor(compression_ratio=0.5)\n",
    "    compressed = bernoulli.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {compressed.shape}\")\n",
    "    print(f\"   Compression: {len(compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test DWT compression\n",
    "    print(\"\\n2. DWT (Haar) Compression (50%):\")\n",
    "    dwt = DWTCompressor(compression_ratio=0.5)\n",
    "    compressed_coeffs, mask = dwt.compress(test_signal)\n",
    "    print(f\"   Compressed coefficients: {np.sum(mask)}\")\n",
    "    print(f\"   Compression: {np.sum(mask)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test hybrid compression\n",
    "    print(\"\\n3. Hybrid Compression (50%):\")\n",
    "    hybrid = HybridCompressor(compression_ratio=0.5)\n",
    "    hybrid_compressed = hybrid.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {hybrid_compressed.shape}\")\n",
    "    print(f\"   Compression: {len(hybrid_compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    print(\"\\nQuick test completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Audio Compression-Classification Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # First run quick test\n",
    "    quick_test()\n",
    "    \n",
    "    # Ask user if they want to run full experiments\n",
    "    response = input(\"\\nDo you want to run full compression-classification experiments? (yes/no): \")\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nQuick test completed. Run full experiments when ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7966830-69e5-4159-bc65-45c5f12ee87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Compression-Classification Pipeline\n",
      "======================================================================\n",
      "NOTE: This version is optimized for faster testing.\n",
      "For full experiments, modify parameters in the code.\n",
      "======================================================================\n",
      "Running quick compression test...\n",
      "Original signal shape: (1024,)\n",
      "\n",
      "1. Bernoulli Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "2. DWT (Haar) Compression (50%):\n",
      "   Compressed coefficients: 512\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "3. Hybrid Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "Quick test completed successfully!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to run compression-classification experiments? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\n",
      "======================================================================\n",
      "NOTE: Using reduced sample size and feature dimensions for faster testing\n",
      "For full experiments, adjust max_samples in run_experiments() method\n",
      "======================================================================\n",
      "======================================================================\n",
      "COMPRESSION-CLASSIFICATION EXPERIMENTS\n",
      "Using max 200 samples per set for faster testing\n",
      "======================================================================\n",
      "\n",
      "Preparing data...\n",
      "Found 200 valid audio files\n",
      "Found 66 valid audio files\n",
      "Training samples: 200\n",
      "Testing samples: 66\n",
      "\n",
      "============================================================\n",
      "Compression Method: BERNOULLI\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.25: 100%|██████████████████████████████████████████████████████████| 200/200 [00:34<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.25: 100%|█████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3030, F1: 0.3034, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.3788, F1: 0.3565, Time: 0.03s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3333, F1: 0.3897, Time: 2.40s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.5: 100%|███████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.5: 100%|██████████████████████████████████████████████████████████████| 66/66 [00:06<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.2879, F1: 0.2994, Time: 0.12s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4091, F1: 0.3760, Time: 0.04s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.4545, F1: 0.5005, Time: 1.52s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.75: 100%|██████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.75: 100%|█████████████████████████████████████████████████████████████| 66/66 [00:06<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.2879, F1: 0.2925, Time: 0.12s\n",
      "  Training svm...\n",
      "    Accuracy: 0.3939, F1: 0.3664, Time: 0.05s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5000, F1: 0.5374, Time: 1.83s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 1.0: 100%|███████████████████████████████████████████████████████████| 200/200 [00:18<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 1.0: 100%|██████████████████████████████████████████████████████████████| 66/66 [00:06<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.12s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.12s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.69s\n",
      "\n",
      "============================================================\n",
      "Compression Method: DWT\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.25: 100%|████████████████████████████████████████████████████████████████| 200/200 [00:18<00:00, 11.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.25: 100%|███████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5606, F1: 0.5272, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4091, F1: 0.4024, Time: 0.07s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3182, F1: 0.2915, Time: 1.23s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.5: 100%|█████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.5: 100%|████████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5303, F1: 0.5047, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5606, F1: 0.5610, Time: 0.10s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.4697, F1: 0.3686, Time: 2.02s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.75: 100%|████████████████████████████████████████████████████████████████| 200/200 [00:22<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.75: 100%|███████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3030, F1: 0.3460, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.2273, F1: 0.1934, Time: 0.12s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.4545, F1: 0.4655, Time: 2.98s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 1.0: 100%|█████████████████████████████████████████████████████████████████| 200/200 [00:21<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 1.0: 100%|████████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.13s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.81s\n",
      "\n",
      "============================================================\n",
      "Compression Method: HYBRID\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 0.25: 100%|█████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 0.25: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:07<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.2879, F1: 0.2930, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.3333, F1: 0.3242, Time: 0.04s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3333, F1: 0.2551, Time: 1.30s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 0.5: 100%|██████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 0.5: 100%|█████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3333, F1: 0.3322, Time: 0.11s\n",
      "  Training svm...\n",
      "    Accuracy: 0.3182, F1: 0.3126, Time: 0.04s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3182, F1: 0.2470, Time: 1.82s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 0.75: 100%|█████████████████████████████████████████████████████████████| 200/200 [00:24<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 0.75: 100%|████████████████████████████████████████████████████████████████| 66/66 [00:08<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3030, F1: 0.2708, Time: 0.14s\n",
      "  Training svm...\n",
      "    Accuracy: 0.3333, F1: 0.3242, Time: 0.07s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3636, F1: 0.3201, Time: 2.54s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 1.0: 100%|██████████████████████████████████████████████████████████████| 200/200 [00:21<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 1.0: 100%|█████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.13s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.81s\n",
      "\n",
      "============================================================\n",
      "Compression Method: NONE\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 0.25: 100%|███████████████████████████████████████████████████████████████| 200/200 [00:21<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 0.25: 100%|██████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.13s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.80s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 0.5: 100%|████████████████████████████████████████████████████████████████| 200/200 [00:21<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 0.5: 100%|███████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.14s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.82s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 0.75: 100%|███████████████████████████████████████████████████████████████| 200/200 [00:21<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 0.75: 100%|██████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.13s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.80s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 200 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 1.0: 100%|████████████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 66 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 1.0: 100%|███████████████████████████████████████████████████████████████████| 66/66 [00:06<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.3182, F1: 0.2414, Time: 0.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4242, F1: 0.4736, Time: 0.13s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.3485, F1: 0.3160, Time: 1.77s\n",
      "\n",
      "Results saved to compression_results/compression_results.csv\n",
      "Reconstruction errors saved to compression_results/reconstruction_errors.csv\n",
      "Summary saved to compression_results/summary.json\n",
      "Warning: Error generating reports: Invalid format specifier '.4f if isinstance(best_row[metric], float) else best_row[metric]' for object of type 'str'\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Total experiments conducted: 48\n",
      "\n",
      "OVERALL BEST CONFIGURATION:\n",
      "  Method: dwt\n",
      "  Ratio: 0.25\n",
      "  Classifier: random_forest\n",
      "  Accuracy: 0.5606\n",
      "  Feature Dimension: 256\n",
      "  Compression Rate: 75.0%\n",
      "\n",
      "BEST BY COMPRESSION METHOD:\n",
      "  bernoulli: 0.5000 (Ratio: 0.75, Classifier: xgboost)\n",
      "  dwt: 0.5606 (Ratio: 0.25, Classifier: random_forest)\n",
      "  hybrid: 0.4242 (Ratio: 1.0, Classifier: svm)\n",
      "  none: 0.4242 (Ratio: 0.25, Classifier: svm)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score)\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Enhanced Feature Extraction (Base Class)\n",
    "# ============================================\n",
    "\n",
    "class RobustMFCCExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced MFCC feature extraction with multiple robust techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=20, n_fft=2048, hop_length=512):  # Reduced n_mfcc for faster processing\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_mfcc_features(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract comprehensive MFCC-based features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with robust loading\n",
    "            y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "            \n",
    "            # Zero-padding or truncation for consistent length\n",
    "            target_length = self.sr * 4  # 4 seconds\n",
    "            if len(y) < target_length:\n",
    "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:target_length]\n",
    "            \n",
    "            # Apply pre-emphasis filter\n",
    "            y = librosa.effects.preemphasis(y)\n",
    "            \n",
    "            # Extract base MFCCs\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Extract delta and delta-delta features\n",
    "            mfcc_delta = librosa.feature.delta(mfccs)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "            \n",
    "            # Extract other complementary features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Root Mean Square Energy\n",
    "            rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Spectral Centroid and Rolloff\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.vstack([\n",
    "                mfccs,\n",
    "                mfcc_delta,\n",
    "                mfcc_delta2,\n",
    "                chroma,\n",
    "                spectral_contrast,\n",
    "                rms,\n",
    "                zcr,\n",
    "                spectral_centroid,\n",
    "                spectral_rolloff\n",
    "            ])\n",
    "            \n",
    "            # Extract statistical features\n",
    "            feature_stats = self._extract_statistics(features)\n",
    "            \n",
    "            return feature_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            # Return zeros with appropriate dimension\n",
    "            # Calculate expected dimension: (20 + 20 + 20 + 12 + 7 + 1 + 1 + 1 + 1) * 9 = 558\n",
    "            return np.zeros(558)  # Reduced dimension\n",
    "    \n",
    "    def _extract_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Extract statistical features from feature matrix\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for feature in features:\n",
    "            stats.extend([\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.median(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 75),\n",
    "                np.mean(np.diff(feature)),  # Average change\n",
    "                np.std(np.diff(feature))    # Std of changes\n",
    "            ])\n",
    "        return np.array(stats)\n",
    "    \n",
    "    def extract_all_features(self, audio_paths, labels=None):\n",
    "        \"\"\"\n",
    "        Extract features from multiple audio files\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, path in enumerate(tqdm(audio_paths, desc=\"Extracting features\")):\n",
    "            feat = self.extract_mfcc_features(path)\n",
    "            if np.sum(feat) != 0 and not np.isnan(feat).any():  # Skip zero or NaN features\n",
    "                features.append(feat)\n",
    "                if labels is not None:\n",
    "                    valid_labels.append(labels[idx])\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"WARNING: No valid features extracted!\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Normalize features\n",
    "        features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Encode labels if provided\n",
    "        if labels is not None and len(valid_labels) > 0:\n",
    "            encoded_labels = self.label_encoder.fit_transform(valid_labels)\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        \n",
    "        return features, encoded_labels\n",
    "\n",
    "# ============================================\n",
    "# 2. UrbanSound8K Processor\n",
    "# ============================================\n",
    "\n",
    "class UrbanSound8KProcessor:\n",
    "    \"\"\"\n",
    "    Process UrbanSound8K dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            # Try alternative path structure\n",
    "            self.metadata_path = os.path.join(dataset_path, 'UrbanSound8K.csv')\n",
    "        \n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {self.metadata_path}\")\n",
    "        \n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        \n",
    "    def prepare_data(self, folds=None, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Prepare data for specific folds or all folds\n",
    "        \"\"\"\n",
    "        if folds is None:\n",
    "            folds = list(range(1, 11))\n",
    "        elif isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        \n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        fold_numbers = []\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_data = self.metadata[self.metadata['fold'] == fold]\n",
    "            \n",
    "            for _, row in fold_data.iterrows():\n",
    "                # Try different possible paths\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.dataset_path, 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, 'audio', 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, str(row['fold']), row['slice_file_name'])\n",
    "                ]\n",
    "                \n",
    "                audio_file = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        audio_file = path\n",
    "                        break\n",
    "                \n",
    "                if audio_file:\n",
    "                    audio_paths.append(audio_file)\n",
    "                    labels.append(row['class'])\n",
    "                    fold_numbers.append(fold)\n",
    "                    \n",
    "                    # Limit samples for faster testing\n",
    "                    if max_samples and len(audio_paths) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if max_samples and len(audio_paths) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(audio_paths)} valid audio files\")\n",
    "        return audio_paths[:max_samples] if max_samples else audio_paths, \\\n",
    "               labels[:max_samples] if max_samples else labels, \\\n",
    "               fold_numbers[:max_samples] if max_samples else fold_numbers\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Get class distribution statistics\n",
    "        \"\"\"\n",
    "        return self.metadata['class'].value_counts()\n",
    "\n",
    "# ============================================\n",
    "# 3. Compression Sensing Modules\n",
    "# ============================================\n",
    "\n",
    "class BernoulliCompressor:\n",
    "    \"\"\"Bernoulli Random Matrix Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize Bernoulli compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.measurement_matrix = None\n",
    "        self.reconstruction_matrix = None\n",
    "        \n",
    "    def create_bernoulli_matrix(self, n_original, n_compressed):\n",
    "        \"\"\"\n",
    "        Create Bernoulli random measurement matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_original : int\n",
    "            Original signal dimension\n",
    "        n_compressed : int\n",
    "            Compressed dimension\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        measurement_matrix : np.array\n",
    "            Bernoulli random matrix\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Bernoulli matrix with entries +1/sqrt(n_compressed) and -1/sqrt(n_compressed)\n",
    "        bernoulli_values = np.random.choice([1, -1], size=(n_compressed, n_original))\n",
    "        bernoulli_matrix = bernoulli_values / np.sqrt(n_compressed)\n",
    "        return bernoulli_matrix\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using Bernoulli random matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (1D array)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        \"\"\"\n",
    "        n_original = len(signal)\n",
    "        n_compressed = int(n_original * self.compression_ratio)\n",
    "        \n",
    "        # Create measurement matrix\n",
    "        self.measurement_matrix = self.create_bernoulli_matrix(n_original, n_compressed)\n",
    "        \n",
    "        # Compress signal\n",
    "        compressed_signal = np.dot(self.measurement_matrix, signal)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct_l1(self, compressed_signal, max_iter=100):\n",
    "        \"\"\"\n",
    "        Reconstruct original signal using L1 minimization (Basis Pursuit)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        max_iter : int\n",
    "            Maximum iterations for reconstruction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        if self.measurement_matrix is None:\n",
    "            raise ValueError(\"Measurement matrix not created. Run compress() first.\")\n",
    "        \n",
    "        n_original = self.measurement_matrix.shape[1]\n",
    "        \n",
    "        # Use least squares with L1 regularization (simplified)\n",
    "        reconstructed_signal = lsqr(self.measurement_matrix, compressed_signal, iter_lim=max_iter)[0]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class DWTCompressor:\n",
    "    \"\"\"Discrete Wavelet Transform (Haar) Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, wavelet='haar'):\n",
    "        \"\"\"\n",
    "        Initialize DWT compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        wavelet : str\n",
    "            Wavelet type (default: 'haar')\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.wavelet = wavelet\n",
    "        \n",
    "    def haar_transform(self, signal):\n",
    "        \"\"\"\n",
    "        Apply Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (length must be power of 2)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "        \"\"\"\n",
    "        n = len(signal)\n",
    "        coeffs = np.zeros_like(signal, dtype=float)\n",
    "        \n",
    "        # Simple Haar transform implementation\n",
    "        temp = signal.copy()\n",
    "        length = n\n",
    "        \n",
    "        while length > 1:\n",
    "            for i in range(length // 2):\n",
    "                avg = (temp[2*i] + temp[2*i+1]) / np.sqrt(2)\n",
    "                diff = (temp[2*i] - temp[2*i+1]) / np.sqrt(2)\n",
    "                coeffs[i] = avg\n",
    "                coeffs[length // 2 + i] = diff\n",
    "            temp[:length] = coeffs[:length]\n",
    "            length //= 2\n",
    "            \n",
    "        return coeffs\n",
    "    \n",
    "    def inverse_haar_transform(self, coeffs):\n",
    "        \"\"\"\n",
    "        Apply inverse Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        n = len(coeffs)\n",
    "        signal = coeffs.copy()\n",
    "        length = 2\n",
    "        \n",
    "        while length <= n:\n",
    "            temp = signal.copy()\n",
    "            for i in range(length // 2):\n",
    "                signal[2*i] = (temp[i] + temp[length // 2 + i]) / np.sqrt(2)\n",
    "                signal[2*i+1] = (temp[i] - temp[length // 2 + i]) / np.sqrt(2)\n",
    "            length *= 2\n",
    "            \n",
    "        return signal\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using DWT and thresholding\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal (thresholded coefficients)\n",
    "        compression_mask : np.array\n",
    "            Mask indicating which coefficients were kept\n",
    "        \"\"\"\n",
    "        # Apply wavelet transform\n",
    "        coeffs = self.haar_transform(signal)\n",
    "        \n",
    "        # Keep only largest coefficients based on compression ratio\n",
    "        n_coeffs = len(coeffs)\n",
    "        n_keep = int(n_coeffs * self.compression_ratio)\n",
    "        \n",
    "        # Get indices of largest absolute coefficients\n",
    "        indices = np.argsort(np.abs(coeffs))[-n_keep:]\n",
    "        compression_mask = np.zeros(n_coeffs, dtype=bool)\n",
    "        compression_mask[indices] = True\n",
    "        \n",
    "        # Create compressed signal (only keep selected coefficients)\n",
    "        compressed_coeffs = np.zeros_like(coeffs)\n",
    "        compressed_coeffs[indices] = coeffs[indices]\n",
    "        \n",
    "        return compressed_coeffs, compression_mask\n",
    "    \n",
    "    def reconstruct(self, compressed_coeffs, original_length=None):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from compressed coefficients\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_coeffs : np.array\n",
    "            Compressed wavelet coefficients\n",
    "        original_length : int\n",
    "            Original signal length (for trimming)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Apply inverse transform\n",
    "        reconstructed_signal = self.inverse_haar_transform(compressed_coeffs)\n",
    "        \n",
    "        # Trim to original length if provided\n",
    "        if original_length is not None and len(reconstructed_signal) > original_length:\n",
    "            reconstructed_signal = reconstructed_signal[:original_length]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class HybridCompressor:\n",
    "    \"\"\"Hybrid Bernoulli + DWT Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize hybrid compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Overall compression ratio\n",
    "        seed : int\n",
    "            Random seed for Bernoulli matrix\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.bernoulli_compressor = BernoulliCompressor(compression_ratio, seed)\n",
    "        self.dwt_compressor = DWTCompressor(1.0)  # No compression in DWT stage\n",
    "        \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Apply DWT then Bernoulli compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Apply DWT\n",
    "        dwt_coeffs = self.dwt_compressor.haar_transform(signal)\n",
    "        \n",
    "        # Step 2: Apply Bernoulli compression on DWT coefficients\n",
    "        compressed_signal = self.bernoulli_compressor.compress(dwt_coeffs)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct(self, compressed_signal, original_length=None):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from hybrid compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "        original_length : int\n",
    "            Original signal length\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Reconstruct DWT coefficients\n",
    "        dwt_coeffs = self.bernoulli_compressor.reconstruct_l1(compressed_signal)\n",
    "        \n",
    "        # Step 2: Apply inverse DWT\n",
    "        reconstructed_signal = self.dwt_compressor.inverse_haar_transform(dwt_coeffs)\n",
    "        \n",
    "        # Trim to original length if provided\n",
    "        if original_length is not None and len(reconstructed_signal) > original_length:\n",
    "            reconstructed_signal = reconstructed_signal[:original_length]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "# ============================================\n",
    "# 4. Enhanced Feature Extraction with Compression\n",
    "# ============================================\n",
    "\n",
    "class CompressedFeatureExtractor(RobustMFCCExtractor):\n",
    "    \"\"\"\n",
    "    Feature extractor with compression capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=20, n_fft=2048, hop_length=512):\n",
    "        super().__init__(sr, n_mfcc, n_fft, hop_length)\n",
    "        \n",
    "    def extract_features_with_compression(self, audio_path, compression_method='bernoulli', \n",
    "                                          compression_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Extract features with optional compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        audio_path : str\n",
    "            Path to audio file\n",
    "        compression_method : str\n",
    "            'bernoulli', 'dwt', 'hybrid', or 'none'\n",
    "        compression_ratio : float\n",
    "            Compression ratio (0-1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : np.array\n",
    "            Extracted features (compressed or original)\n",
    "        original_features : np.array\n",
    "            Original features before compression\n",
    "        compression_info : dict\n",
    "            Compression metadata\n",
    "        \"\"\"\n",
    "        # Extract original features\n",
    "        original_features = self.extract_mfcc_features(audio_path)\n",
    "        \n",
    "        if compression_method == 'none' or compression_ratio >= 1.0:\n",
    "            return original_features, original_features, {'method': 'none', 'ratio': 1.0}\n",
    "        \n",
    "        # Ensure features are 1D and have appropriate length\n",
    "        features_1d = original_features.flatten()\n",
    "        original_length = len(features_1d)\n",
    "        \n",
    "        compression_info = {\n",
    "            'method': compression_method,\n",
    "            'ratio': compression_ratio,\n",
    "            'original_length': original_length,\n",
    "            'compressed_length': int(original_length * compression_ratio),\n",
    "            'padded': False\n",
    "        }\n",
    "        \n",
    "        # Apply compression\n",
    "        if compression_method == 'bernoulli':\n",
    "            compressor = BernoulliCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_1d)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        elif compression_method == 'dwt':\n",
    "            # Pad to nearest power of 2 for DWT\n",
    "            n_padded = 2 ** int(np.ceil(np.log2(original_length)))\n",
    "            features_padded = np.pad(features_1d, (0, n_padded - original_length), 'constant')\n",
    "            compression_info['padded'] = True\n",
    "            compression_info['padded_length'] = n_padded\n",
    "            \n",
    "            compressor = DWTCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_coeffs, mask = compressor.compress(features_padded)\n",
    "            compressed_features = compressed_coeffs[mask]\n",
    "            compression_info['compressor'] = compressor\n",
    "            compression_info['mask'] = mask\n",
    "            compression_info['compressed_indices'] = np.where(mask)[0]\n",
    "            \n",
    "        elif compression_method == 'hybrid':\n",
    "            # Pad to nearest power of 2 for hybrid\n",
    "            n_padded = 2 ** int(np.ceil(np.log2(original_length)))\n",
    "            features_padded = np.pad(features_1d, (0, n_padded - original_length), 'constant')\n",
    "            compression_info['padded'] = True\n",
    "            compression_info['padded_length'] = n_padded\n",
    "            \n",
    "            compressor = HybridCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_padded)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown compression method: {compression_method}\")\n",
    "        \n",
    "        return compressed_features, original_features, compression_info\n",
    "\n",
    "# ============================================\n",
    "# 5. Compression Experiment Pipeline (FIXED)\n",
    "# ============================================\n",
    "\n",
    "class CompressionExperiment:\n",
    "    \"\"\"\n",
    "    Pipeline for compression and classification experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, output_dir='compression_results'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Experiment configurations - using smaller sample size for testing\n",
    "        self.compression_methods = ['bernoulli', 'dwt', 'hybrid', 'none']\n",
    "        self.compression_ratios = [0.25, 0.50, 0.75, 1.0]  # 1.0 = no compression\n",
    "        self.classifiers = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),  # Reduced estimators\n",
    "            'svm': SVC(probability=True, random_state=42),\n",
    "            'xgboost': XGBClassifier(n_estimators=50, random_state=42, n_jobs=-1),  # Reduced estimators\n",
    "        }\n",
    "        \n",
    "    def run_experiments(self, train_folds=list(range(1, 9)), test_folds=[9, 10], max_samples=500):\n",
    "        \"\"\"\n",
    "        Run full compression-classification experiments\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_folds : list\n",
    "            Folds to use for training\n",
    "        test_folds : list\n",
    "            Folds to use for testing\n",
    "        max_samples : int\n",
    "            Maximum number of samples to use (for faster testing)\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"COMPRESSION-CLASSIFICATION EXPERIMENTS\")\n",
    "        print(f\"Using max {max_samples} samples per set for faster testing\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize processor and extractor\n",
    "        processor = UrbanSound8KProcessor(self.dataset_path)\n",
    "        extractor = CompressedFeatureExtractor()\n",
    "        \n",
    "        # Prepare data with limited samples for testing\n",
    "        print(\"\\nPreparing data...\")\n",
    "        train_paths, train_labels, _ = processor.prepare_data(train_folds, max_samples=max_samples)\n",
    "        test_paths, test_labels, _ = processor.prepare_data(test_folds, max_samples=max_samples//3)\n",
    "        \n",
    "        print(f\"Training samples: {len(train_paths)}\")\n",
    "        print(f\"Testing samples: {len(test_paths)}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_full = label_encoder.fit_transform(train_labels)\n",
    "        y_test_full = label_encoder.transform(test_labels)\n",
    "        \n",
    "        # Store all results\n",
    "        all_results = []\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        # Run experiments for each compression configuration\n",
    "        for method in self.compression_methods:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Compression Method: {method.upper()}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            for ratio in self.compression_ratios:\n",
    "                print(f\"\\nCompression Ratio: {ratio*100:.0f}%\")\n",
    "                print(\"-\"*40)\n",
    "                \n",
    "                # Extract features with compression\n",
    "                X_train_compressed = []\n",
    "                X_test_compressed = []\n",
    "                X_train_original = []\n",
    "                X_test_original = []\n",
    "                compression_infos = []\n",
    "                \n",
    "                # Process training data\n",
    "                print(f\"Processing {len(train_paths)} training samples...\")\n",
    "                for path in tqdm(train_paths, desc=f\"Train {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, comp_info = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_train_compressed.append(compressed_feat)\n",
    "                    X_train_original.append(original_feat)\n",
    "                    compression_infos.append(comp_info)\n",
    "                \n",
    "                # Process test data\n",
    "                print(f\"Processing {len(test_paths)} test samples...\")\n",
    "                for path in tqdm(test_paths, desc=f\"Test {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, comp_info = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_test_compressed.append(compressed_feat)\n",
    "                    X_test_original.append(original_feat)\n",
    "                \n",
    "                # Convert to arrays\n",
    "                X_train = np.array(X_train_compressed)\n",
    "                X_test = np.array(X_test_compressed)\n",
    "                X_train_orig = np.array(X_train_original)\n",
    "                X_test_orig = np.array(X_test_original)\n",
    "                \n",
    "                # Check if we have valid data\n",
    "                if len(X_train) == 0 or len(X_test) == 0:\n",
    "                    print(\"  WARNING: No valid features extracted. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Pad sequences to consistent length if necessary\n",
    "                train_max_len = max(len(x) for x in X_train) if len(X_train) > 0 else 0\n",
    "                test_max_len = max(len(x) for x in X_test) if len(X_test) > 0 else 0\n",
    "                max_len = max(train_max_len, test_max_len)\n",
    "                \n",
    "                if max_len > 0:\n",
    "                    X_train = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                      for x in X_train])\n",
    "                    X_test = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                     for x in X_test])\n",
    "                \n",
    "                # Normalize features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Calculate reconstruction error if not 'none'\n",
    "                if method != 'none' and ratio < 1.0:\n",
    "                    # Use first compression info as representative\n",
    "                    comp_info = compression_infos[0] if compression_infos else {}\n",
    "                    if comp_info:\n",
    "                        avg_error = self.calculate_reconstruction_error(\n",
    "                            X_train_orig[:10], X_train_compressed[:10], method, ratio, comp_info\n",
    "                        )\n",
    "                        reconstruction_errors.append({\n",
    "                            'method': method,\n",
    "                            'ratio': ratio,\n",
    "                            'reconstruction_error': avg_error\n",
    "                        })\n",
    "                \n",
    "                # Train and evaluate classifiers\n",
    "                for clf_name, clf in self.classifiers.items():\n",
    "                    print(f\"  Training {clf_name}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Train classifier\n",
    "                        start_time = time.time()\n",
    "                        clf.fit(X_train_scaled, y_train_full)\n",
    "                        train_time = time.time() - start_time\n",
    "                        \n",
    "                        # Predict\n",
    "                        start_time = time.time()\n",
    "                        y_pred = clf.predict(X_test_scaled)\n",
    "                        test_time = time.time() - start_time\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        accuracy = accuracy_score(y_test_full, y_pred)\n",
    "                        precision = precision_score(y_test_full, y_pred, average='weighted', zero_division=0)\n",
    "                        recall = recall_score(y_test_full, y_pred, average='weighted', zero_division=0)\n",
    "                        f1 = f1_score(y_test_full, y_pred, average='weighted', zero_division=0)\n",
    "                        \n",
    "                        # ROC-AUC if available\n",
    "                        try:\n",
    "                            if hasattr(clf, 'predict_proba'):\n",
    "                                y_proba = clf.predict_proba(X_test_scaled)\n",
    "                                roc_auc = roc_auc_score(y_test_full, y_proba, multi_class='ovr', average='weighted')\n",
    "                            else:\n",
    "                                roc_auc = np.nan\n",
    "                        except:\n",
    "                            roc_auc = np.nan\n",
    "                        \n",
    "                        # Store results\n",
    "                        result = {\n",
    "                            'compression_method': method,\n",
    "                            'compression_ratio': ratio,\n",
    "                            'classifier': clf_name,\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'f1_score': f1,\n",
    "                            'roc_auc': roc_auc,\n",
    "                            'train_time': train_time,\n",
    "                            'test_time': test_time,\n",
    "                            'feature_dim_original': X_train_orig.shape[1] if len(X_train_orig.shape) > 1 else X_train_orig.shape[0],\n",
    "                            'feature_dim_compressed': X_train.shape[1] if len(X_train.shape) > 1 else X_train.shape[0],\n",
    "                            'compression_rate': (1 - ratio) * 100,\n",
    "                            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        }\n",
    "                        \n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        print(f\"    Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Time: {train_time:.2f}s\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    ERROR training {clf_name}: {str(e)}\")\n",
    "                        continue\n",
    "        \n",
    "        # Save all results\n",
    "        self.save_results(all_results, reconstruction_errors)\n",
    "        \n",
    "        # Generate comprehensive reports\n",
    "        if all_results:\n",
    "            self.generate_reports(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_reconstruction_error(self, original_features, compressed_features, \n",
    "                                      method, ratio, comp_info):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error between original and compressed features\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for orig, comp in zip(original_features, compressed_features):\n",
    "            try:\n",
    "                if method == 'bernoulli':\n",
    "                    # Reconstruct using compressor\n",
    "                    if 'compressor' in comp_info:\n",
    "                        reconstructed = comp_info['compressor'].reconstruct_l1(comp[:int(len(orig) * ratio)])\n",
    "                        # Pad or truncate to match original length\n",
    "                        if len(reconstructed) < len(orig):\n",
    "                            reconstructed = np.pad(reconstructed, (0, len(orig) - len(reconstructed)), 'constant')\n",
    "                        elif len(reconstructed) > len(orig):\n",
    "                            reconstructed = reconstructed[:len(orig)]\n",
    "                        \n",
    "                        # Calculate error\n",
    "                        error = np.mean((orig - reconstructed) ** 2)\n",
    "                        errors.append(error)\n",
    "                \n",
    "                elif method == 'dwt':\n",
    "                    if 'compressor' in comp_info and 'mask' in comp_info:\n",
    "                        # Get padded length\n",
    "                        padded_length = comp_info.get('padded_length', len(comp_info['mask']))\n",
    "                        \n",
    "                        # Create full coefficient array at padded length\n",
    "                        full_coeffs = np.zeros(padded_length)\n",
    "                        \n",
    "                        # Get the actual compressed values and their positions\n",
    "                        if 'compressed_indices' in comp_info:\n",
    "                            indices = comp_info['compressed_indices']\n",
    "                            if len(comp) == len(indices):\n",
    "                                full_coeffs[indices] = comp\n",
    "                            else:\n",
    "                                # If lengths don't match, use mask directly\n",
    "                                mask = comp_info['mask']\n",
    "                                if len(comp) == np.sum(mask):\n",
    "                                    full_coeffs[mask] = comp\n",
    "                        else:\n",
    "                            # Fallback to using mask\n",
    "                            mask = comp_info['mask']\n",
    "                            if len(comp) == np.sum(mask):\n",
    "                                full_coeffs[mask] = comp\n",
    "                        \n",
    "                        # Reconstruct\n",
    "                        reconstructed = comp_info['compressor'].reconstruct(full_coeffs, original_length=len(orig))\n",
    "                        \n",
    "                        # Trim to original length\n",
    "                        if len(reconstructed) > len(orig):\n",
    "                            reconstructed = reconstructed[:len(orig)]\n",
    "                        \n",
    "                        # Calculate error\n",
    "                        error = np.mean((orig - reconstructed) ** 2)\n",
    "                        errors.append(error)\n",
    "                \n",
    "                elif method == 'hybrid':\n",
    "                    if 'compressor' in comp_info:\n",
    "                        reconstructed = comp_info['compressor'].reconstruct(comp, original_length=len(orig))\n",
    "                        \n",
    "                        # Trim to original length\n",
    "                        if len(reconstructed) > len(orig):\n",
    "                            reconstructed = reconstructed[:len(orig)]\n",
    "                        \n",
    "                        # Calculate error\n",
    "                        error = np.mean((orig - reconstructed) ** 2)\n",
    "                        errors.append(error)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: Error calculating reconstruction: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return np.mean(errors) if errors else np.nan\n",
    "    \n",
    "    def save_results(self, all_results, reconstruction_errors):\n",
    "        \"\"\"\n",
    "        Save experiment results to files\n",
    "        \"\"\"\n",
    "        # Save main results\n",
    "        if all_results:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_csv(os.path.join(self.output_dir, 'compression_results.csv'), index=False)\n",
    "            print(f\"\\nResults saved to {self.output_dir}/compression_results.csv\")\n",
    "        else:\n",
    "            print(\"\\nWARNING: No results to save!\")\n",
    "        \n",
    "        # Save reconstruction errors\n",
    "        if reconstruction_errors:\n",
    "            recon_df = pd.DataFrame(reconstruction_errors)\n",
    "            recon_df.to_csv(os.path.join(self.output_dir, 'reconstruction_errors.csv'), index=False)\n",
    "            print(f\"Reconstruction errors saved to {self.output_dir}/reconstruction_errors.csv\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        if all_results:\n",
    "            summary = self.create_summary_statistics(results_df)\n",
    "            with open(os.path.join(self.output_dir, 'summary.json'), 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            print(f\"Summary saved to {self.output_dir}/summary.json\")\n",
    "    \n",
    "    def create_summary_statistics(self, results_df):\n",
    "        \"\"\"\n",
    "        Create comprehensive summary statistics\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'experiment_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'total_experiments': len(results_df),\n",
    "            'compression_methods_tested': list(results_df['compression_method'].unique()),\n",
    "            'compression_ratios_tested': list(results_df['compression_ratio'].unique()),\n",
    "            'classifiers_tested': list(results_df['classifier'].unique()),\n",
    "        }\n",
    "        \n",
    "        # Best results by compression method\n",
    "        best_by_method = {}\n",
    "        for method in summary['compression_methods_tested']:\n",
    "            method_df = results_df[results_df['compression_method'] == method]\n",
    "            if not method_df.empty:\n",
    "                best_idx = method_df['accuracy'].idxmax()\n",
    "                best_by_method[method] = method_df.loc[best_idx].to_dict()\n",
    "        \n",
    "        summary['best_by_method'] = best_by_method\n",
    "        \n",
    "        # Compression vs Accuracy analysis\n",
    "        compression_analysis = {}\n",
    "        for ratio in summary['compression_ratios_tested']:\n",
    "            ratio_df = results_df[results_df['compression_ratio'] == ratio]\n",
    "            if not ratio_df.empty:\n",
    "                compression_analysis[f'ratio_{ratio}'] = {\n",
    "                    'avg_accuracy': ratio_df['accuracy'].mean(),\n",
    "                    'avg_f1': ratio_df['f1_score'].mean(),\n",
    "                    'avg_feature_dim': ratio_df['feature_dim_compressed'].mean(),\n",
    "                    'compression_rate': (1 - ratio) * 100\n",
    "                }\n",
    "        \n",
    "        summary['compression_analysis'] = compression_analysis\n",
    "        \n",
    "        # Overall best configuration\n",
    "        overall_best_idx = results_df['accuracy'].idxmax()\n",
    "        summary['overall_best'] = results_df.loc[overall_best_idx].to_dict()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_reports(self, all_results):\n",
    "        \"\"\"\n",
    "        Generate visual reports and analysis\n",
    "        \"\"\"\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        try:\n",
    "            # 1. Accuracy vs Compression Ratio plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            for method in results_df['compression_method'].unique():\n",
    "                method_df = results_df[results_df['compression_method'] == method]\n",
    "                for clf in results_df['classifier'].unique():\n",
    "                    clf_df = method_df[method_df['classifier'] == clf]\n",
    "                    if not clf_df.empty:\n",
    "                        # Sort by ratio for proper line plotting\n",
    "                        clf_df = clf_df.sort_values('compression_ratio')\n",
    "                        plt.plot(clf_df['compression_ratio'], clf_df['accuracy'], \n",
    "                                marker='o', label=f'{method}-{clf}')\n",
    "            \n",
    "            plt.xlabel('Compression Ratio', fontsize=12)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.title('Classification Accuracy vs Compression Ratio', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'accuracy_vs_compression.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Create detailed report HTML\n",
    "            self.create_html_report(results_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error generating reports: {str(e)}\")\n",
    "    \n",
    "    def create_html_report(self, results_df):\n",
    "        \"\"\"\n",
    "        Create HTML report of experiment results\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Compression-Classification Experiment Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                h1, h2, h3 {{ color: #333; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "                .best {{ background-color: #d4edda; }}\n",
    "                .summary {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; }}\n",
    "                .image {{ max-width: 100%; height: auto; margin: 20px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Audio Compression-Classification Experiment Report</h1>\n",
    "            <p>Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "            \n",
    "            <div class=\"summary\">\n",
    "                <h2>Experiment Summary</h2>\n",
    "                <p><strong>Total Experiments:</strong> {len(results_df)}</p>\n",
    "                <p><strong>Compression Methods:</strong> {', '.join(results_df['compression_method'].unique())}</p>\n",
    "                <p><strong>Compression Ratios:</strong> {', '.join([str(r) for r in sorted(results_df['compression_ratio'].unique())])}</p>\n",
    "                <p><strong>Classifiers:</strong> {', '.join(results_df['classifier'].unique())}</p>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find overall best\n",
    "        if not results_df.empty:\n",
    "            best_idx = results_df['accuracy'].idxmax()\n",
    "            best_row = results_df.loc[best_idx]\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                <h2>Overall Best Configuration</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Metric</th>\n",
    "                        <th>Value</th>\n",
    "                    </tr>\n",
    "            \"\"\"\n",
    "            \n",
    "            for metric in ['compression_method', 'compression_ratio', 'classifier', \n",
    "                          'accuracy', 'precision', 'recall', 'f1_score',\n",
    "                          'train_time', 'test_time', 'feature_dim_compressed']:\n",
    "                html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{metric.replace('_', ' ').title()}</td>\n",
    "                        <td>{best_row[metric]:.4f if isinstance(best_row[metric], float) else best_row[metric]}</td>\n",
    "                    </tr>\n",
    "                \"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                </table>\n",
    "                \n",
    "                <h2>Top 10 Performances</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Rank</th>\n",
    "                        <th>Method</th>\n",
    "                        <th>Ratio</th>\n",
    "                        <th>Classifier</th>\n",
    "                        <th>Accuracy</th>\n",
    "                        <th>F1 Score</th>\n",
    "                        <th>Feature Dim</th>\n",
    "                    </tr>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Sort by accuracy and get top 10\n",
    "            top_results = results_df.sort_values('accuracy', ascending=False).head(10)\n",
    "            for idx, (_, row) in enumerate(top_results.iterrows(), 1):\n",
    "                html_content += f\"\"\"\n",
    "                    <tr class=\"{'best' if idx == 1 else ''}\">\n",
    "                        <td>{idx}</td>\n",
    "                        <td>{row['compression_method']}</td>\n",
    "                        <td>{row['compression_ratio']}</td>\n",
    "                        <td>{row['classifier']}</td>\n",
    "                        <td>{row['accuracy']:.4f}</td>\n",
    "                        <td>{row['f1_score']:.4f}</td>\n",
    "                        <td>{int(row['feature_dim_compressed'])}</td>\n",
    "                    </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Detailed Results Table</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Method</th>\n",
    "                    <th>Ratio</th>\n",
    "                    <th>Classifier</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>Precision</th>\n",
    "                    <th>Recall</th>\n",
    "                    <th>F1</th>\n",
    "                    <th>Train Time</th>\n",
    "                    <th>Test Time</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in results_df.iterrows():\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{row['compression_method']}</td>\n",
    "                    <td>{row['compression_ratio']}</td>\n",
    "                    <td>{row['classifier']}</td>\n",
    "                    <td>{row['accuracy']:.4f}</td>\n",
    "                    <td>{row['precision']:.4f}</td>\n",
    "                    <td>{row['recall']:.4f}</td>\n",
    "                    <td>{row['f1_score']:.4f}</td>\n",
    "                    <td>{row['train_time']:.2f}s</td>\n",
    "                    <td>{row['test_time']:.2f}s</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Visualizations</h2>\n",
    "            <p>The following visualization has been generated:</p>\n",
    "            <ul>\n",
    "                <li>accuracy_vs_compression.png - Accuracy vs Compression Ratio</li>\n",
    "            </ul>\n",
    "            \n",
    "            <footer>\n",
    "                <p>Experiment conducted using UrbanSound8K dataset</p>\n",
    "                <p>Compression methods: Bernoulli, DWT (Haar), Hybrid</p>\n",
    "            </footer>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save HTML report\n",
    "        with open(os.path.join(self.output_dir, 'experiment_report.html'), 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"HTML report generated: {self.output_dir}/experiment_report.html\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Main Execution (OPTIMIZED)\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function - optimized for faster testing\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"NOTE: Using reduced sample size and feature dimensions for faster testing\")\n",
    "    print(\"For full experiments, adjust max_samples in run_experiments() method\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize paths\n",
    "    DATASET_PATH = \"UrbanSound8K\"\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "        print(\"\\nPlease download UrbanSound8K dataset from:\")\n",
    "        print(\"https://urbansounddataset.weebly.com/urbansound8k.html\")\n",
    "        print(\"\\nExtract it to the current directory as 'UrbanSound8K'\")\n",
    "        return\n",
    "    \n",
    "    # Create experiment instance\n",
    "    experiment = CompressionExperiment(DATASET_PATH)\n",
    "    \n",
    "    # Run experiments with limited samples for testing\n",
    "    results = experiment.run_experiments(max_samples=200)  # Reduced for faster testing\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load and display summary\n",
    "    summary_path = os.path.join('compression_results', 'summary.json')\n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\nTotal experiments conducted: {summary['total_experiments']}\")\n",
    "        \n",
    "        if 'overall_best' in summary:\n",
    "            best = summary['overall_best']\n",
    "            print(\"\\nOVERALL BEST CONFIGURATION:\")\n",
    "            print(f\"  Method: {best['compression_method']}\")\n",
    "            print(f\"  Ratio: {best['compression_ratio']}\")\n",
    "            print(f\"  Classifier: {best['classifier']}\")\n",
    "            print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "            print(f\"  Feature Dimension: {int(best['feature_dim_compressed'])}\")\n",
    "            print(f\"  Compression Rate: {best['compression_rate']:.1f}%\")\n",
    "        \n",
    "        if 'best_by_method' in summary:\n",
    "            print(\"\\nBEST BY COMPRESSION METHOD:\")\n",
    "            for method, config in summary['best_by_method'].items():\n",
    "                print(f\"  {method}: {config['accuracy']:.4f} \"\n",
    "                      f\"(Ratio: {config['compression_ratio']}, \"\n",
    "                      f\"Classifier: {config['classifier']})\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"\n",
    "    Quick test to verify compression algorithms\n",
    "    \"\"\"\n",
    "    print(\"Running quick compression test...\")\n",
    "    \n",
    "    # Generate test signal\n",
    "    np.random.seed(42)\n",
    "    test_signal = np.random.randn(1024)\n",
    "    \n",
    "    print(f\"Original signal shape: {test_signal.shape}\")\n",
    "    \n",
    "    # Test Bernoulli compression\n",
    "    print(\"\\n1. Bernoulli Compression (50%):\")\n",
    "    bernoulli = BernoulliCompressor(compression_ratio=0.5)\n",
    "    compressed = bernoulli.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {compressed.shape}\")\n",
    "    print(f\"   Compression: {len(compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test DWT compression\n",
    "    print(\"\\n2. DWT (Haar) Compression (50%):\")\n",
    "    dwt = DWTCompressor(compression_ratio=0.5)\n",
    "    compressed_coeffs, mask = dwt.compress(test_signal)\n",
    "    print(f\"   Compressed coefficients: {np.sum(mask)}\")\n",
    "    print(f\"   Compression: {np.sum(mask)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test hybrid compression\n",
    "    print(\"\\n3. Hybrid Compression (50%):\")\n",
    "    hybrid = HybridCompressor(compression_ratio=0.5)\n",
    "    hybrid_compressed = hybrid.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {hybrid_compressed.shape}\")\n",
    "    print(f\"   Compression: {len(hybrid_compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    print(\"\\nQuick test completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Audio Compression-Classification Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"NOTE: This version is optimized for faster testing.\")\n",
    "    print(\"For full experiments, modify parameters in the code.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # First run quick test\n",
    "    quick_test()\n",
    "    \n",
    "    # Ask user if they want to run experiments\n",
    "    response = input(\"\\nDo you want to run compression-classification experiments? (yes/no): \")\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nQuick test completed. Run experiments when ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05609874-e2c3-456d-871d-f579edaec8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Compression-Classification Pipeline\n",
      "======================================================================\n",
      "NOTE: This version is optimized for faster testing.\n",
      "For full experiments, modify parameters in the code.\n",
      "======================================================================\n",
      "Running quick compression test...\n",
      "Original signal shape: (1024,)\n",
      "\n",
      "1. Bernoulli Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "2. DWT (Haar) Compression (50%):\n",
      "   Compressed coefficients: 512\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "3. Hybrid Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "Quick test completed successfully!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to run compression-classification experiments? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\n",
      "======================================================================\n",
      "NOTE: Using reduced sample size and feature dimensions for faster testing\n",
      "For full experiments, adjust max_samples in run_experiments() method\n",
      "======================================================================\n",
      "======================================================================\n",
      "COMPRESSION-CLASSIFICATION EXPERIMENTS\n",
      "Using max 8732 samples per set for faster testing\n",
      "======================================================================\n",
      "\n",
      "Preparing data...\n",
      "Found 7079 valid audio files\n",
      "Found 1653 valid audio files\n",
      "Training samples: 7079\n",
      "Testing samples: 1653\n",
      "\n",
      "============================================================\n",
      "Compression Method: BERNOULLI\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.25: 100%|████████████████████████████████████████████████████████| 7079/7079 [15:16<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.25: 100%|█████████████████████████████████████████████████████████| 1653/1653 [03:29<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.4918, F1: 0.4858, Time: 1.76s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5269, F1: 0.5137, Time: 29.43s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5245, F1: 0.5214, Time: 12.83s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.5: 100%|█████████████████████████████████████████████████████████| 7079/7079 [12:50<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.5: 100%|██████████████████████████████████████████████████████████| 1653/1653 [03:19<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.4918, F1: 0.4848, Time: 2.69s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5245, F1: 0.5120, Time: 49.81s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5324, F1: 0.5284, Time: 24.85s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.75: 100%|████████████████████████████████████████████████████████| 7079/7079 [13:55<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 0.75: 100%|█████████████████████████████████████████████████████████| 1653/1653 [03:25<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5021, F1: 0.4952, Time: 2.88s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5318, F1: 0.5198, Time: 47.52s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5330, F1: 0.5297, Time: 24.37s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 1.0: 100%|█████████████████████████████████████████████████████████| 7079/7079 [10:28<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test bernoulli 1.0: 100%|██████████████████████████████████████████████████████████| 1653/1653 [02:14<00:00, 12.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 2.96s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 47.02s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 26.84s\n",
      "\n",
      "============================================================\n",
      "Compression Method: DWT\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.25: 100%|██████████████████████████████████████████████████████████████| 7079/7079 [09:38<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.25: 100%|███████████████████████████████████████████████████████████████| 1653/1653 [02:14<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5880, F1: 0.5851, Time: 1.97s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5009, F1: 0.4969, Time: 39.72s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.6515, F1: 0.6475, Time: 13.73s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.5: 100%|███████████████████████████████████████████████████████████████| 7079/7079 [09:53<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.5: 100%|████████████████████████████████████████████████████████████████| 1653/1653 [02:27<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5989, F1: 0.5934, Time: 3.04s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4882, F1: 0.4743, Time: 75.75s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.6376, F1: 0.6301, Time: 29.46s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 0.75: 100%|██████████████████████████████████████████████████████████████| 7079/7079 [10:44<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 0.75: 100%|███████████████████████████████████████████████████████████████| 1653/1653 [02:27<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.5457, F1: 0.5447, Time: 4.01s\n",
      "  Training svm...\n",
      "    Accuracy: 0.4876, F1: 0.4808, Time: 118.71s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.6340, F1: 0.6348, Time: 39.34s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train dwt 1.0: 100%|███████████████████████████████████████████████████████████████| 7079/7079 [09:52<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test dwt 1.0: 100%|████████████████████████████████████████████████████████████████| 1653/1653 [02:17<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 3.29s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 57.89s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 33.59s\n",
      "\n",
      "============================================================\n",
      "Compression Method: HYBRID\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 0.25: 100%|███████████████████████████████████████████████████████████| 7079/7079 [10:57<00:00, 10.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 0.25: 100%|████████████████████████████████████████████████████████████| 1653/1653 [02:32<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.4906, F1: 0.4842, Time: 1.96s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5336, F1: 0.5219, Time: 22.56s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5547, F1: 0.5531, Time: 14.01s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 0.5: 100%|████████████████████████████████████████████████████████████| 7079/7079 [10:48<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 0.5: 100%|█████████████████████████████████████████████████████████████| 1653/1653 [02:58<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.4882, F1: 0.4819, Time: 2.78s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5299, F1: 0.5182, Time: 37.22s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5511, F1: 0.5483, Time: 23.66s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 0.75: 100%|███████████████████████████████████████████████████████████| 7079/7079 [12:43<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 0.75: 100%|████████████████████████████████████████████████████████████| 1653/1653 [02:52<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.4985, F1: 0.4916, Time: 3.40s\n",
      "  Training svm...\n",
      "    Accuracy: 0.5306, F1: 0.5185, Time: 56.33s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.5354, F1: 0.5323, Time: 34.26s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train hybrid 1.0: 100%|████████████████████████████████████████████████████████████| 7079/7079 [11:14<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test hybrid 1.0: 100%|█████████████████████████████████████████████████████████████| 1653/1653 [05:58<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 3.87s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 86.84s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 29.09s\n",
      "\n",
      "============================================================\n",
      "Compression Method: NONE\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 0.25: 100%|█████████████████████████████████████████████████████████████| 7079/7079 [08:30<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 0.25: 100%|██████████████████████████████████████████████████████████████| 1653/1653 [01:59<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 3.13s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 43.42s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 29.62s\n",
      "\n",
      "Compression Ratio: 50%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 0.5: 100%|██████████████████████████████████████████████████████████████| 7079/7079 [08:36<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 0.5: 100%|███████████████████████████████████████████████████████████████| 1653/1653 [02:01<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 3.11s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 44.10s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 29.09s\n",
      "\n",
      "Compression Ratio: 75%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 0.75: 100%|█████████████████████████████████████████████████████████████| 7079/7079 [08:29<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 0.75: 100%|██████████████████████████████████████████████████████████████| 1653/1653 [01:58<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 3.08s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 43.97s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 29.25s\n",
      "\n",
      "Compression Ratio: 100%\n",
      "----------------------------------------\n",
      "Processing 7079 training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train none 1.0: 100%|██████████████████████████████████████████████████████████████| 7079/7079 [08:30<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1653 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test none 1.0: 100%|███████████████████████████████████████████████████████████████| 1653/1653 [01:59<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training random_forest...\n",
      "    Accuracy: 0.7260, F1: 0.7232, Time: 3.14s\n",
      "  Training svm...\n",
      "    Accuracy: 0.7568, F1: 0.7521, Time: 44.38s\n",
      "  Training xgboost...\n",
      "    Accuracy: 0.7417, F1: 0.7385, Time: 29.19s\n",
      "\n",
      "Results saved to compression_results/compression_results.csv\n",
      "Reconstruction errors saved to compression_results/reconstruction_errors.csv\n",
      "Summary saved to compression_results/summary.json\n",
      "Warning: Error generating reports: Invalid format specifier '.4f if isinstance(best_row[metric], float) else best_row[metric]' for object of type 'str'\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Total experiments conducted: 48\n",
      "\n",
      "OVERALL BEST CONFIGURATION:\n",
      "  Method: bernoulli\n",
      "  Ratio: 1.0\n",
      "  Classifier: svm\n",
      "  Accuracy: 0.7568\n",
      "  Feature Dimension: 747\n",
      "  Compression Rate: 0.0%\n",
      "\n",
      "BEST BY COMPRESSION METHOD:\n",
      "  bernoulli: 0.7568 (Ratio: 1.0, Classifier: svm)\n",
      "  dwt: 0.7568 (Ratio: 1.0, Classifier: svm)\n",
      "  hybrid: 0.7568 (Ratio: 1.0, Classifier: svm)\n",
      "  none: 0.7568 (Ratio: 0.25, Classifier: svm)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score)\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Enhanced Feature Extraction (Base Class)\n",
    "# ============================================\n",
    "\n",
    "class RobustMFCCExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced MFCC feature extraction with multiple robust techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=20, n_fft=2048, hop_length=512):  # Reduced n_mfcc for faster processing\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_mfcc_features(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract comprehensive MFCC-based features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with robust loading\n",
    "            y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "            \n",
    "            # Zero-padding or truncation for consistent length\n",
    "            target_length = self.sr * 4  # 4 seconds\n",
    "            if len(y) < target_length:\n",
    "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:target_length]\n",
    "            \n",
    "            # Apply pre-emphasis filter\n",
    "            y = librosa.effects.preemphasis(y)\n",
    "            \n",
    "            # Extract base MFCCs\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Extract delta and delta-delta features\n",
    "            mfcc_delta = librosa.feature.delta(mfccs)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "            \n",
    "            # Extract other complementary features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Root Mean Square Energy\n",
    "            rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Spectral Centroid and Rolloff\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.vstack([\n",
    "                mfccs,\n",
    "                mfcc_delta,\n",
    "                mfcc_delta2,\n",
    "                chroma,\n",
    "                spectral_contrast,\n",
    "                rms,\n",
    "                zcr,\n",
    "                spectral_centroid,\n",
    "                spectral_rolloff\n",
    "            ])\n",
    "            \n",
    "            # Extract statistical features\n",
    "            feature_stats = self._extract_statistics(features)\n",
    "            \n",
    "            return feature_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            # Return zeros with appropriate dimension\n",
    "            # Calculate expected dimension: (20 + 20 + 20 + 12 + 7 + 1 + 1 + 1 + 1) * 9 = 558\n",
    "            return np.zeros(558)  # Reduced dimension\n",
    "    \n",
    "    def _extract_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Extract statistical features from feature matrix\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for feature in features:\n",
    "            stats.extend([\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.median(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 75),\n",
    "                np.mean(np.diff(feature)),  # Average change\n",
    "                np.std(np.diff(feature))    # Std of changes\n",
    "            ])\n",
    "        return np.array(stats)\n",
    "    \n",
    "    def extract_all_features(self, audio_paths, labels=None):\n",
    "        \"\"\"\n",
    "        Extract features from multiple audio files\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, path in enumerate(tqdm(audio_paths, desc=\"Extracting features\")):\n",
    "            feat = self.extract_mfcc_features(path)\n",
    "            if np.sum(feat) != 0 and not np.isnan(feat).any():  # Skip zero or NaN features\n",
    "                features.append(feat)\n",
    "                if labels is not None:\n",
    "                    valid_labels.append(labels[idx])\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"WARNING: No valid features extracted!\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Normalize features\n",
    "        features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Encode labels if provided\n",
    "        if labels is not None and len(valid_labels) > 0:\n",
    "            encoded_labels = self.label_encoder.fit_transform(valid_labels)\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        \n",
    "        return features, encoded_labels\n",
    "\n",
    "# ============================================\n",
    "# 2. UrbanSound8K Processor\n",
    "# ============================================\n",
    "\n",
    "class UrbanSound8KProcessor:\n",
    "    \"\"\"\n",
    "    Process UrbanSound8K dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            # Try alternative path structure\n",
    "            self.metadata_path = os.path.join(dataset_path, 'UrbanSound8K.csv')\n",
    "        \n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {self.metadata_path}\")\n",
    "        \n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        \n",
    "    def prepare_data(self, folds=None, max_samples=1000):\n",
    "        \"\"\"\n",
    "        Prepare data for specific folds or all folds\n",
    "        \"\"\"\n",
    "        if folds is None:\n",
    "            folds = list(range(1, 11))\n",
    "        elif isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        \n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        fold_numbers = []\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_data = self.metadata[self.metadata['fold'] == fold]\n",
    "            \n",
    "            for _, row in fold_data.iterrows():\n",
    "                # Try different possible paths\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.dataset_path, 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, 'audio', 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, str(row['fold']), row['slice_file_name'])\n",
    "                ]\n",
    "                \n",
    "                audio_file = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        audio_file = path\n",
    "                        break\n",
    "                \n",
    "                if audio_file:\n",
    "                    audio_paths.append(audio_file)\n",
    "                    labels.append(row['class'])\n",
    "                    fold_numbers.append(fold)\n",
    "                    \n",
    "                    # Limit samples for faster testing\n",
    "                    if max_samples and len(audio_paths) >= max_samples:\n",
    "                        break\n",
    "            \n",
    "            if max_samples and len(audio_paths) >= max_samples:\n",
    "                break\n",
    "        \n",
    "        print(f\"Found {len(audio_paths)} valid audio files\")\n",
    "        return audio_paths[:max_samples] if max_samples else audio_paths, \\\n",
    "               labels[:max_samples] if max_samples else labels, \\\n",
    "               fold_numbers[:max_samples] if max_samples else fold_numbers\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Get class distribution statistics\n",
    "        \"\"\"\n",
    "        return self.metadata['class'].value_counts()\n",
    "\n",
    "# ============================================\n",
    "# 3. Compression Sensing Modules\n",
    "# ============================================\n",
    "\n",
    "class BernoulliCompressor:\n",
    "    \"\"\"Bernoulli Random Matrix Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize Bernoulli compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.measurement_matrix = None\n",
    "        self.reconstruction_matrix = None\n",
    "        \n",
    "    def create_bernoulli_matrix(self, n_original, n_compressed):\n",
    "        \"\"\"\n",
    "        Create Bernoulli random measurement matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_original : int\n",
    "            Original signal dimension\n",
    "        n_compressed : int\n",
    "            Compressed dimension\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        measurement_matrix : np.array\n",
    "            Bernoulli random matrix\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Bernoulli matrix with entries +1/sqrt(n_compressed) and -1/sqrt(n_compressed)\n",
    "        bernoulli_values = np.random.choice([1, -1], size=(n_compressed, n_original))\n",
    "        bernoulli_matrix = bernoulli_values / np.sqrt(n_compressed)\n",
    "        return bernoulli_matrix\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using Bernoulli random matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (1D array)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        \"\"\"\n",
    "        n_original = len(signal)\n",
    "        n_compressed = int(n_original * self.compression_ratio)\n",
    "        \n",
    "        # Create measurement matrix\n",
    "        self.measurement_matrix = self.create_bernoulli_matrix(n_original, n_compressed)\n",
    "        \n",
    "        # Compress signal\n",
    "        compressed_signal = np.dot(self.measurement_matrix, signal)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct_l1(self, compressed_signal, max_iter=100):\n",
    "        \"\"\"\n",
    "        Reconstruct original signal using L1 minimization (Basis Pursuit)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        max_iter : int\n",
    "            Maximum iterations for reconstruction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        if self.measurement_matrix is None:\n",
    "            raise ValueError(\"Measurement matrix not created. Run compress() first.\")\n",
    "        \n",
    "        n_original = self.measurement_matrix.shape[1]\n",
    "        \n",
    "        # Use least squares with L1 regularization (simplified)\n",
    "        reconstructed_signal = lsqr(self.measurement_matrix, compressed_signal, iter_lim=max_iter)[0]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class DWTCompressor:\n",
    "    \"\"\"Discrete Wavelet Transform (Haar) Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, wavelet='haar'):\n",
    "        \"\"\"\n",
    "        Initialize DWT compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        wavelet : str\n",
    "            Wavelet type (default: 'haar')\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.wavelet = wavelet\n",
    "        \n",
    "    def haar_transform(self, signal):\n",
    "        \"\"\"\n",
    "        Apply Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (length must be power of 2)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "        \"\"\"\n",
    "        n = len(signal)\n",
    "        coeffs = np.zeros_like(signal, dtype=float)\n",
    "        \n",
    "        # Simple Haar transform implementation\n",
    "        temp = signal.copy()\n",
    "        length = n\n",
    "        \n",
    "        while length > 1:\n",
    "            for i in range(length // 2):\n",
    "                avg = (temp[2*i] + temp[2*i+1]) / np.sqrt(2)\n",
    "                diff = (temp[2*i] - temp[2*i+1]) / np.sqrt(2)\n",
    "                coeffs[i] = avg\n",
    "                coeffs[length // 2 + i] = diff\n",
    "            temp[:length] = coeffs[:length]\n",
    "            length //= 2\n",
    "            \n",
    "        return coeffs\n",
    "    \n",
    "    def inverse_haar_transform(self, coeffs):\n",
    "        \"\"\"\n",
    "        Apply inverse Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        n = len(coeffs)\n",
    "        signal = coeffs.copy()\n",
    "        length = 2\n",
    "        \n",
    "        while length <= n:\n",
    "            temp = signal.copy()\n",
    "            for i in range(length // 2):\n",
    "                signal[2*i] = (temp[i] + temp[length // 2 + i]) / np.sqrt(2)\n",
    "                signal[2*i+1] = (temp[i] - temp[length // 2 + i]) / np.sqrt(2)\n",
    "            length *= 2\n",
    "            \n",
    "        return signal\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using DWT and thresholding\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal (thresholded coefficients)\n",
    "        compression_mask : np.array\n",
    "            Mask indicating which coefficients were kept\n",
    "        \"\"\"\n",
    "        # Apply wavelet transform\n",
    "        coeffs = self.haar_transform(signal)\n",
    "        \n",
    "        # Keep only largest coefficients based on compression ratio\n",
    "        n_coeffs = len(coeffs)\n",
    "        n_keep = int(n_coeffs * self.compression_ratio)\n",
    "        \n",
    "        # Get indices of largest absolute coefficients\n",
    "        indices = np.argsort(np.abs(coeffs))[-n_keep:]\n",
    "        compression_mask = np.zeros(n_coeffs, dtype=bool)\n",
    "        compression_mask[indices] = True\n",
    "        \n",
    "        # Create compressed signal (only keep selected coefficients)\n",
    "        compressed_coeffs = np.zeros_like(coeffs)\n",
    "        compressed_coeffs[indices] = coeffs[indices]\n",
    "        \n",
    "        return compressed_coeffs, compression_mask\n",
    "    \n",
    "    def reconstruct(self, compressed_coeffs, original_length=None):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from compressed coefficients\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_coeffs : np.array\n",
    "            Compressed wavelet coefficients\n",
    "        original_length : int\n",
    "            Original signal length (for trimming)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Apply inverse transform\n",
    "        reconstructed_signal = self.inverse_haar_transform(compressed_coeffs)\n",
    "        \n",
    "        # Trim to original length if provided\n",
    "        if original_length is not None and len(reconstructed_signal) > original_length:\n",
    "            reconstructed_signal = reconstructed_signal[:original_length]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class HybridCompressor:\n",
    "    \"\"\"Hybrid Bernoulli + DWT Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize hybrid compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Overall compression ratio\n",
    "        seed : int\n",
    "            Random seed for Bernoulli matrix\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.bernoulli_compressor = BernoulliCompressor(compression_ratio, seed)\n",
    "        self.dwt_compressor = DWTCompressor(1.0)  # No compression in DWT stage\n",
    "        \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Apply DWT then Bernoulli compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Apply DWT\n",
    "        dwt_coeffs = self.dwt_compressor.haar_transform(signal)\n",
    "        \n",
    "        # Step 2: Apply Bernoulli compression on DWT coefficients\n",
    "        compressed_signal = self.bernoulli_compressor.compress(dwt_coeffs)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct(self, compressed_signal, original_length=None):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from hybrid compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "        original_length : int\n",
    "            Original signal length\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Reconstruct DWT coefficients\n",
    "        dwt_coeffs = self.bernoulli_compressor.reconstruct_l1(compressed_signal)\n",
    "        \n",
    "        # Step 2: Apply inverse DWT\n",
    "        reconstructed_signal = self.dwt_compressor.inverse_haar_transform(dwt_coeffs)\n",
    "        \n",
    "        # Trim to original length if provided\n",
    "        if original_length is not None and len(reconstructed_signal) > original_length:\n",
    "            reconstructed_signal = reconstructed_signal[:original_length]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "# ============================================\n",
    "# 4. Enhanced Feature Extraction with Compression\n",
    "# ============================================\n",
    "\n",
    "class CompressedFeatureExtractor(RobustMFCCExtractor):\n",
    "    \"\"\"\n",
    "    Feature extractor with compression capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=20, n_fft=2048, hop_length=512):\n",
    "        super().__init__(sr, n_mfcc, n_fft, hop_length)\n",
    "        \n",
    "    def extract_features_with_compression(self, audio_path, compression_method='bernoulli', \n",
    "                                          compression_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Extract features with optional compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        audio_path : str\n",
    "            Path to audio file\n",
    "        compression_method : str\n",
    "            'bernoulli', 'dwt', 'hybrid', or 'none'\n",
    "        compression_ratio : float\n",
    "            Compression ratio (0-1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : np.array\n",
    "            Extracted features (compressed or original)\n",
    "        original_features : np.array\n",
    "            Original features before compression\n",
    "        compression_info : dict\n",
    "            Compression metadata\n",
    "        \"\"\"\n",
    "        # Extract original features\n",
    "        original_features = self.extract_mfcc_features(audio_path)\n",
    "        \n",
    "        if compression_method == 'none' or compression_ratio >= 1.0:\n",
    "            return original_features, original_features, {'method': 'none', 'ratio': 1.0}\n",
    "        \n",
    "        # Ensure features are 1D and have appropriate length\n",
    "        features_1d = original_features.flatten()\n",
    "        original_length = len(features_1d)\n",
    "        \n",
    "        compression_info = {\n",
    "            'method': compression_method,\n",
    "            'ratio': compression_ratio,\n",
    "            'original_length': original_length,\n",
    "            'compressed_length': int(original_length * compression_ratio),\n",
    "            'padded': False\n",
    "        }\n",
    "        \n",
    "        # Apply compression\n",
    "        if compression_method == 'bernoulli':\n",
    "            compressor = BernoulliCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_1d)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        elif compression_method == 'dwt':\n",
    "            # Pad to nearest power of 2 for DWT\n",
    "            n_padded = 2 ** int(np.ceil(np.log2(original_length)))\n",
    "            features_padded = np.pad(features_1d, (0, n_padded - original_length), 'constant')\n",
    "            compression_info['padded'] = True\n",
    "            compression_info['padded_length'] = n_padded\n",
    "            \n",
    "            compressor = DWTCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_coeffs, mask = compressor.compress(features_padded)\n",
    "            compressed_features = compressed_coeffs[mask]\n",
    "            compression_info['compressor'] = compressor\n",
    "            compression_info['mask'] = mask\n",
    "            compression_info['compressed_indices'] = np.where(mask)[0]\n",
    "            \n",
    "        elif compression_method == 'hybrid':\n",
    "            # Pad to nearest power of 2 for hybrid\n",
    "            n_padded = 2 ** int(np.ceil(np.log2(original_length)))\n",
    "            features_padded = np.pad(features_1d, (0, n_padded - original_length), 'constant')\n",
    "            compression_info['padded'] = True\n",
    "            compression_info['padded_length'] = n_padded\n",
    "            \n",
    "            compressor = HybridCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_padded)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown compression method: {compression_method}\")\n",
    "        \n",
    "        return compressed_features, original_features, compression_info\n",
    "\n",
    "# ============================================\n",
    "# 5. Compression Experiment Pipeline (FIXED)\n",
    "# ============================================\n",
    "\n",
    "class CompressionExperiment:\n",
    "    \"\"\"\n",
    "    Pipeline for compression and classification experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, output_dir='compression_results'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Experiment configurations - using smaller sample size for testing\n",
    "        self.compression_methods = ['bernoulli', 'dwt', 'hybrid', 'none']\n",
    "        self.compression_ratios = [0.25, 0.50, 0.75, 1.0]  # 1.0 = no compression\n",
    "        self.classifiers = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1),  # Reduced estimators\n",
    "            'svm': SVC(probability=True, random_state=42),\n",
    "            'xgboost': XGBClassifier(n_estimators=50, random_state=42, n_jobs=-1),  # Reduced estimators\n",
    "        }\n",
    "        \n",
    "    def run_experiments(self, train_folds=list(range(1, 9)), test_folds=[9, 10], max_samples=500):\n",
    "        \"\"\"\n",
    "        Run full compression-classification experiments\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_folds : list\n",
    "            Folds to use for training\n",
    "        test_folds : list\n",
    "            Folds to use for testing\n",
    "        max_samples : int\n",
    "            Maximum number of samples to use (for faster testing)\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"COMPRESSION-CLASSIFICATION EXPERIMENTS\")\n",
    "        print(f\"Using max {max_samples} samples per set for faster testing\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize processor and extractor\n",
    "        processor = UrbanSound8KProcessor(self.dataset_path)\n",
    "        extractor = CompressedFeatureExtractor()\n",
    "        \n",
    "        # Prepare data with limited samples for testing\n",
    "        print(\"\\nPreparing data...\")\n",
    "        train_paths, train_labels, _ = processor.prepare_data(train_folds, max_samples=max_samples)\n",
    "        test_paths, test_labels, _ = processor.prepare_data(test_folds, max_samples=max_samples//3)\n",
    "        \n",
    "        print(f\"Training samples: {len(train_paths)}\")\n",
    "        print(f\"Testing samples: {len(test_paths)}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_full = label_encoder.fit_transform(train_labels)\n",
    "        y_test_full = label_encoder.transform(test_labels)\n",
    "        \n",
    "        # Store all results\n",
    "        all_results = []\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        # Run experiments for each compression configuration\n",
    "        for method in self.compression_methods:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Compression Method: {method.upper()}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            for ratio in self.compression_ratios:\n",
    "                print(f\"\\nCompression Ratio: {ratio*100:.0f}%\")\n",
    "                print(\"-\"*40)\n",
    "                \n",
    "                # Extract features with compression\n",
    "                X_train_compressed = []\n",
    "                X_test_compressed = []\n",
    "                X_train_original = []\n",
    "                X_test_original = []\n",
    "                compression_infos = []\n",
    "                \n",
    "                # Process training data\n",
    "                print(f\"Processing {len(train_paths)} training samples...\")\n",
    "                for path in tqdm(train_paths, desc=f\"Train {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, comp_info = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_train_compressed.append(compressed_feat)\n",
    "                    X_train_original.append(original_feat)\n",
    "                    compression_infos.append(comp_info)\n",
    "                \n",
    "                # Process test data\n",
    "                print(f\"Processing {len(test_paths)} test samples...\")\n",
    "                for path in tqdm(test_paths, desc=f\"Test {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, comp_info = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_test_compressed.append(compressed_feat)\n",
    "                    X_test_original.append(original_feat)\n",
    "                \n",
    "                # Convert to arrays\n",
    "                X_train = np.array(X_train_compressed)\n",
    "                X_test = np.array(X_test_compressed)\n",
    "                X_train_orig = np.array(X_train_original)\n",
    "                X_test_orig = np.array(X_test_original)\n",
    "                \n",
    "                # Check if we have valid data\n",
    "                if len(X_train) == 0 or len(X_test) == 0:\n",
    "                    print(\"  WARNING: No valid features extracted. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Pad sequences to consistent length if necessary\n",
    "                train_max_len = max(len(x) for x in X_train) if len(X_train) > 0 else 0\n",
    "                test_max_len = max(len(x) for x in X_test) if len(X_test) > 0 else 0\n",
    "                max_len = max(train_max_len, test_max_len)\n",
    "                \n",
    "                if max_len > 0:\n",
    "                    X_train = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                      for x in X_train])\n",
    "                    X_test = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                     for x in X_test])\n",
    "                \n",
    "                # Normalize features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Calculate reconstruction error if not 'none'\n",
    "                if method != 'none' and ratio < 1.0:\n",
    "                    # Use first compression info as representative\n",
    "                    comp_info = compression_infos[0] if compression_infos else {}\n",
    "                    if comp_info:\n",
    "                        avg_error = self.calculate_reconstruction_error(\n",
    "                            X_train_orig[:10], X_train_compressed[:10], method, ratio, comp_info\n",
    "                        )\n",
    "                        reconstruction_errors.append({\n",
    "                            'method': method,\n",
    "                            'ratio': ratio,\n",
    "                            'reconstruction_error': avg_error\n",
    "                        })\n",
    "                \n",
    "                # Train and evaluate classifiers\n",
    "                for clf_name, clf in self.classifiers.items():\n",
    "                    print(f\"  Training {clf_name}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Train classifier\n",
    "                        start_time = time.time()\n",
    "                        clf.fit(X_train_scaled, y_train_full)\n",
    "                        train_time = time.time() - start_time\n",
    "                        \n",
    "                        # Predict\n",
    "                        start_time = time.time()\n",
    "                        y_pred = clf.predict(X_test_scaled)\n",
    "                        test_time = time.time() - start_time\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        accuracy = accuracy_score(y_test_full, y_pred)\n",
    "                        precision = precision_score(y_test_full, y_pred, average='weighted', zero_division=0)\n",
    "                        recall = recall_score(y_test_full, y_pred, average='weighted', zero_division=0)\n",
    "                        f1 = f1_score(y_test_full, y_pred, average='weighted', zero_division=0)\n",
    "                        \n",
    "                        # ROC-AUC if available\n",
    "                        try:\n",
    "                            if hasattr(clf, 'predict_proba'):\n",
    "                                y_proba = clf.predict_proba(X_test_scaled)\n",
    "                                roc_auc = roc_auc_score(y_test_full, y_proba, multi_class='ovr', average='weighted')\n",
    "                            else:\n",
    "                                roc_auc = np.nan\n",
    "                        except:\n",
    "                            roc_auc = np.nan\n",
    "                        \n",
    "                        # Store results\n",
    "                        result = {\n",
    "                            'compression_method': method,\n",
    "                            'compression_ratio': ratio,\n",
    "                            'classifier': clf_name,\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'f1_score': f1,\n",
    "                            'roc_auc': roc_auc,\n",
    "                            'train_time': train_time,\n",
    "                            'test_time': test_time,\n",
    "                            'feature_dim_original': X_train_orig.shape[1] if len(X_train_orig.shape) > 1 else X_train_orig.shape[0],\n",
    "                            'feature_dim_compressed': X_train.shape[1] if len(X_train.shape) > 1 else X_train.shape[0],\n",
    "                            'compression_rate': (1 - ratio) * 100,\n",
    "                            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        }\n",
    "                        \n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        print(f\"    Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Time: {train_time:.2f}s\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    ERROR training {clf_name}: {str(e)}\")\n",
    "                        continue\n",
    "        \n",
    "        # Save all results\n",
    "        self.save_results(all_results, reconstruction_errors)\n",
    "        \n",
    "        # Generate comprehensive reports\n",
    "        if all_results:\n",
    "            self.generate_reports(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_reconstruction_error(self, original_features, compressed_features, \n",
    "                                      method, ratio, comp_info):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error between original and compressed features\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for orig, comp in zip(original_features, compressed_features):\n",
    "            try:\n",
    "                if method == 'bernoulli':\n",
    "                    # Reconstruct using compressor\n",
    "                    if 'compressor' in comp_info:\n",
    "                        reconstructed = comp_info['compressor'].reconstruct_l1(comp[:int(len(orig) * ratio)])\n",
    "                        # Pad or truncate to match original length\n",
    "                        if len(reconstructed) < len(orig):\n",
    "                            reconstructed = np.pad(reconstructed, (0, len(orig) - len(reconstructed)), 'constant')\n",
    "                        elif len(reconstructed) > len(orig):\n",
    "                            reconstructed = reconstructed[:len(orig)]\n",
    "                        \n",
    "                        # Calculate error\n",
    "                        error = np.mean((orig - reconstructed) ** 2)\n",
    "                        errors.append(error)\n",
    "                \n",
    "                elif method == 'dwt':\n",
    "                    if 'compressor' in comp_info and 'mask' in comp_info:\n",
    "                        # Get padded length\n",
    "                        padded_length = comp_info.get('padded_length', len(comp_info['mask']))\n",
    "                        \n",
    "                        # Create full coefficient array at padded length\n",
    "                        full_coeffs = np.zeros(padded_length)\n",
    "                        \n",
    "                        # Get the actual compressed values and their positions\n",
    "                        if 'compressed_indices' in comp_info:\n",
    "                            indices = comp_info['compressed_indices']\n",
    "                            if len(comp) == len(indices):\n",
    "                                full_coeffs[indices] = comp\n",
    "                            else:\n",
    "                                # If lengths don't match, use mask directly\n",
    "                                mask = comp_info['mask']\n",
    "                                if len(comp) == np.sum(mask):\n",
    "                                    full_coeffs[mask] = comp\n",
    "                        else:\n",
    "                            # Fallback to using mask\n",
    "                            mask = comp_info['mask']\n",
    "                            if len(comp) == np.sum(mask):\n",
    "                                full_coeffs[mask] = comp\n",
    "                        \n",
    "                        # Reconstruct\n",
    "                        reconstructed = comp_info['compressor'].reconstruct(full_coeffs, original_length=len(orig))\n",
    "                        \n",
    "                        # Trim to original length\n",
    "                        if len(reconstructed) > len(orig):\n",
    "                            reconstructed = reconstructed[:len(orig)]\n",
    "                        \n",
    "                        # Calculate error\n",
    "                        error = np.mean((orig - reconstructed) ** 2)\n",
    "                        errors.append(error)\n",
    "                \n",
    "                elif method == 'hybrid':\n",
    "                    if 'compressor' in comp_info:\n",
    "                        reconstructed = comp_info['compressor'].reconstruct(comp, original_length=len(orig))\n",
    "                        \n",
    "                        # Trim to original length\n",
    "                        if len(reconstructed) > len(orig):\n",
    "                            reconstructed = reconstructed[:len(orig)]\n",
    "                        \n",
    "                        # Calculate error\n",
    "                        error = np.mean((orig - reconstructed) ** 2)\n",
    "                        errors.append(error)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: Error calculating reconstruction: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return np.mean(errors) if errors else np.nan\n",
    "    \n",
    "    def save_results(self, all_results, reconstruction_errors):\n",
    "        \"\"\"\n",
    "        Save experiment results to files\n",
    "        \"\"\"\n",
    "        # Save main results\n",
    "        if all_results:\n",
    "            results_df = pd.DataFrame(all_results)\n",
    "            results_df.to_csv(os.path.join(self.output_dir, 'compression_results.csv'), index=False)\n",
    "            print(f\"\\nResults saved to {self.output_dir}/compression_results.csv\")\n",
    "        else:\n",
    "            print(\"\\nWARNING: No results to save!\")\n",
    "        \n",
    "        # Save reconstruction errors\n",
    "        if reconstruction_errors:\n",
    "            recon_df = pd.DataFrame(reconstruction_errors)\n",
    "            recon_df.to_csv(os.path.join(self.output_dir, 'reconstruction_errors.csv'), index=False)\n",
    "            print(f\"Reconstruction errors saved to {self.output_dir}/reconstruction_errors.csv\")\n",
    "        \n",
    "        # Save summary statistics\n",
    "        if all_results:\n",
    "            summary = self.create_summary_statistics(results_df)\n",
    "            with open(os.path.join(self.output_dir, 'summary.json'), 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            print(f\"Summary saved to {self.output_dir}/summary.json\")\n",
    "    \n",
    "    def create_summary_statistics(self, results_df):\n",
    "        \"\"\"\n",
    "        Create comprehensive summary statistics\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'experiment_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'total_experiments': len(results_df),\n",
    "            'compression_methods_tested': list(results_df['compression_method'].unique()),\n",
    "            'compression_ratios_tested': list(results_df['compression_ratio'].unique()),\n",
    "            'classifiers_tested': list(results_df['classifier'].unique()),\n",
    "        }\n",
    "        \n",
    "        # Best results by compression method\n",
    "        best_by_method = {}\n",
    "        for method in summary['compression_methods_tested']:\n",
    "            method_df = results_df[results_df['compression_method'] == method]\n",
    "            if not method_df.empty:\n",
    "                best_idx = method_df['accuracy'].idxmax()\n",
    "                best_by_method[method] = method_df.loc[best_idx].to_dict()\n",
    "        \n",
    "        summary['best_by_method'] = best_by_method\n",
    "        \n",
    "        # Compression vs Accuracy analysis\n",
    "        compression_analysis = {}\n",
    "        for ratio in summary['compression_ratios_tested']:\n",
    "            ratio_df = results_df[results_df['compression_ratio'] == ratio]\n",
    "            if not ratio_df.empty:\n",
    "                compression_analysis[f'ratio_{ratio}'] = {\n",
    "                    'avg_accuracy': ratio_df['accuracy'].mean(),\n",
    "                    'avg_f1': ratio_df['f1_score'].mean(),\n",
    "                    'avg_feature_dim': ratio_df['feature_dim_compressed'].mean(),\n",
    "                    'compression_rate': (1 - ratio) * 100\n",
    "                }\n",
    "        \n",
    "        summary['compression_analysis'] = compression_analysis\n",
    "        \n",
    "        # Overall best configuration\n",
    "        overall_best_idx = results_df['accuracy'].idxmax()\n",
    "        summary['overall_best'] = results_df.loc[overall_best_idx].to_dict()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_reports(self, all_results):\n",
    "        \"\"\"\n",
    "        Generate visual reports and analysis\n",
    "        \"\"\"\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        try:\n",
    "            # 1. Accuracy vs Compression Ratio plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            \n",
    "            for method in results_df['compression_method'].unique():\n",
    "                method_df = results_df[results_df['compression_method'] == method]\n",
    "                for clf in results_df['classifier'].unique():\n",
    "                    clf_df = method_df[method_df['classifier'] == clf]\n",
    "                    if not clf_df.empty:\n",
    "                        # Sort by ratio for proper line plotting\n",
    "                        clf_df = clf_df.sort_values('compression_ratio')\n",
    "                        plt.plot(clf_df['compression_ratio'], clf_df['accuracy'], \n",
    "                                marker='o', label=f'{method}-{clf}')\n",
    "            \n",
    "            plt.xlabel('Compression Ratio', fontsize=12)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.title('Classification Accuracy vs Compression Ratio', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'accuracy_vs_compression.png'), dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Create detailed report HTML\n",
    "            self.create_html_report(results_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error generating reports: {str(e)}\")\n",
    "    \n",
    "    def create_html_report(self, results_df):\n",
    "        \"\"\"\n",
    "        Create HTML report of experiment results\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Compression-Classification Experiment Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                h1, h2, h3 {{ color: #333; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "                .best {{ background-color: #d4edda; }}\n",
    "                .summary {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; }}\n",
    "                .image {{ max-width: 100%; height: auto; margin: 20px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Audio Compression-Classification Experiment Report</h1>\n",
    "            <p>Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "            \n",
    "            <div class=\"summary\">\n",
    "                <h2>Experiment Summary</h2>\n",
    "                <p><strong>Total Experiments:</strong> {len(results_df)}</p>\n",
    "                <p><strong>Compression Methods:</strong> {', '.join(results_df['compression_method'].unique())}</p>\n",
    "                <p><strong>Compression Ratios:</strong> {', '.join([str(r) for r in sorted(results_df['compression_ratio'].unique())])}</p>\n",
    "                <p><strong>Classifiers:</strong> {', '.join(results_df['classifier'].unique())}</p>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find overall best\n",
    "        if not results_df.empty:\n",
    "            best_idx = results_df['accuracy'].idxmax()\n",
    "            best_row = results_df.loc[best_idx]\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                <h2>Overall Best Configuration</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Metric</th>\n",
    "                        <th>Value</th>\n",
    "                    </tr>\n",
    "            \"\"\"\n",
    "            \n",
    "            for metric in ['compression_method', 'compression_ratio', 'classifier', \n",
    "                          'accuracy', 'precision', 'recall', 'f1_score',\n",
    "                          'train_time', 'test_time', 'feature_dim_compressed']:\n",
    "                html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{metric.replace('_', ' ').title()}</td>\n",
    "                        <td>{best_row[metric]:.4f if isinstance(best_row[metric], float) else best_row[metric]}</td>\n",
    "                    </tr>\n",
    "                \"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "                </table>\n",
    "                \n",
    "                <h2>Top 10 Performances</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Rank</th>\n",
    "                        <th>Method</th>\n",
    "                        <th>Ratio</th>\n",
    "                        <th>Classifier</th>\n",
    "                        <th>Accuracy</th>\n",
    "                        <th>F1 Score</th>\n",
    "                        <th>Feature Dim</th>\n",
    "                    </tr>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Sort by accuracy and get top 10\n",
    "            top_results = results_df.sort_values('accuracy', ascending=False).head(10)\n",
    "            for idx, (_, row) in enumerate(top_results.iterrows(), 1):\n",
    "                html_content += f\"\"\"\n",
    "                    <tr class=\"{'best' if idx == 1 else ''}\">\n",
    "                        <td>{idx}</td>\n",
    "                        <td>{row['compression_method']}</td>\n",
    "                        <td>{row['compression_ratio']}</td>\n",
    "                        <td>{row['classifier']}</td>\n",
    "                        <td>{row['accuracy']:.4f}</td>\n",
    "                        <td>{row['f1_score']:.4f}</td>\n",
    "                        <td>{int(row['feature_dim_compressed'])}</td>\n",
    "                    </tr>\n",
    "                \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Detailed Results Table</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Method</th>\n",
    "                    <th>Ratio</th>\n",
    "                    <th>Classifier</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>Precision</th>\n",
    "                    <th>Recall</th>\n",
    "                    <th>F1</th>\n",
    "                    <th>Train Time</th>\n",
    "                    <th>Test Time</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in results_df.iterrows():\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{row['compression_method']}</td>\n",
    "                    <td>{row['compression_ratio']}</td>\n",
    "                    <td>{row['classifier']}</td>\n",
    "                    <td>{row['accuracy']:.4f}</td>\n",
    "                    <td>{row['precision']:.4f}</td>\n",
    "                    <td>{row['recall']:.4f}</td>\n",
    "                    <td>{row['f1_score']:.4f}</td>\n",
    "                    <td>{row['train_time']:.2f}s</td>\n",
    "                    <td>{row['test_time']:.2f}s</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Visualizations</h2>\n",
    "            <p>The following visualization has been generated:</p>\n",
    "            <ul>\n",
    "                <li>accuracy_vs_compression.png - Accuracy vs Compression Ratio</li>\n",
    "            </ul>\n",
    "            \n",
    "            <footer>\n",
    "                <p>Experiment conducted using UrbanSound8K dataset</p>\n",
    "                <p>Compression methods: Bernoulli, DWT (Haar), Hybrid</p>\n",
    "            </footer>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save HTML report\n",
    "        with open(os.path.join(self.output_dir, 'experiment_report.html'), 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"HTML report generated: {self.output_dir}/experiment_report.html\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Main Execution (OPTIMIZED)\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function - optimized for faster testing\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"NOTE: Using reduced sample size and feature dimensions for faster testing\")\n",
    "    print(\"For full experiments, adjust max_samples in run_experiments() method\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize paths\n",
    "    DATASET_PATH = \"UrbanSound8K\"\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "        print(\"\\nPlease download UrbanSound8K dataset from:\")\n",
    "        print(\"https://urbansounddataset.weebly.com/urbansound8k.html\")\n",
    "        print(\"\\nExtract it to the current directory as 'UrbanSound8K'\")\n",
    "        return\n",
    "    \n",
    "    # Create experiment instance\n",
    "    experiment = CompressionExperiment(DATASET_PATH)\n",
    "    \n",
    "    # Run experiments with limited samples for testing\n",
    "    results = experiment.run_experiments(max_samples=8732)  # Reduced for faster testing\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load and display summary\n",
    "    summary_path = os.path.join('compression_results', 'summary.json')\n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\nTotal experiments conducted: {summary['total_experiments']}\")\n",
    "        \n",
    "        if 'overall_best' in summary:\n",
    "            best = summary['overall_best']\n",
    "            print(\"\\nOVERALL BEST CONFIGURATION:\")\n",
    "            print(f\"  Method: {best['compression_method']}\")\n",
    "            print(f\"  Ratio: {best['compression_ratio']}\")\n",
    "            print(f\"  Classifier: {best['classifier']}\")\n",
    "            print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "            print(f\"  Feature Dimension: {int(best['feature_dim_compressed'])}\")\n",
    "            print(f\"  Compression Rate: {best['compression_rate']:.1f}%\")\n",
    "        \n",
    "        if 'best_by_method' in summary:\n",
    "            print(\"\\nBEST BY COMPRESSION METHOD:\")\n",
    "            for method, config in summary['best_by_method'].items():\n",
    "                print(f\"  {method}: {config['accuracy']:.4f} \"\n",
    "                      f\"(Ratio: {config['compression_ratio']}, \"\n",
    "                      f\"Classifier: {config['classifier']})\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"\n",
    "    Quick test to verify compression algorithms\n",
    "    \"\"\"\n",
    "    print(\"Running quick compression test...\")\n",
    "    \n",
    "    # Generate test signal\n",
    "    np.random.seed(42)\n",
    "    test_signal = np.random.randn(1024)\n",
    "    \n",
    "    print(f\"Original signal shape: {test_signal.shape}\")\n",
    "    \n",
    "    # Test Bernoulli compression\n",
    "    print(\"\\n1. Bernoulli Compression (50%):\")\n",
    "    bernoulli = BernoulliCompressor(compression_ratio=0.5)\n",
    "    compressed = bernoulli.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {compressed.shape}\")\n",
    "    print(f\"   Compression: {len(compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test DWT compression\n",
    "    print(\"\\n2. DWT (Haar) Compression (50%):\")\n",
    "    dwt = DWTCompressor(compression_ratio=0.5)\n",
    "    compressed_coeffs, mask = dwt.compress(test_signal)\n",
    "    print(f\"   Compressed coefficients: {np.sum(mask)}\")\n",
    "    print(f\"   Compression: {np.sum(mask)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test hybrid compression\n",
    "    print(\"\\n3. Hybrid Compression (50%):\")\n",
    "    hybrid = HybridCompressor(compression_ratio=0.5)\n",
    "    hybrid_compressed = hybrid.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {hybrid_compressed.shape}\")\n",
    "    print(f\"   Compression: {len(hybrid_compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    print(\"\\nQuick test completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Audio Compression-Classification Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"NOTE: This version is optimized for faster testing.\")\n",
    "    print(\"For full experiments, modify parameters in the code.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # First run quick test\n",
    "    quick_test()\n",
    "    \n",
    "    # Ask user if they want to run experiments\n",
    "    response = input(\"\\nDo you want to run compression-classification experiments? (yes/no): \")\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nQuick test completed. Run experiments when ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f1813-790f-49fd-870e-ecff07f8a943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
