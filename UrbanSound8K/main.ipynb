{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086b1543-4ad7-42fb-b460-938d0f90c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RobustMFCCExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced MFCC feature extraction with multiple robust techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=40, n_fft=2048, hop_length=512):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_mfcc_features(self, audio_path, augment=False):\n",
    "        \"\"\"\n",
    "        Extract comprehensive MFCC-based features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with robust loading\n",
    "            y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "            \n",
    "            # Zero-padding or truncation for consistent length\n",
    "            target_length = self.sr * 4  # 4 seconds\n",
    "            if len(y) < target_length:\n",
    "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:target_length]\n",
    "            \n",
    "            # Apply pre-emphasis filter\n",
    "            y = librosa.effects.preemphasis(y)\n",
    "            \n",
    "            # Extract base MFCCs\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Extract delta and delta-delta features\n",
    "            mfcc_delta = librosa.feature.delta(mfccs)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "            \n",
    "            # Extract other complementary features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "            \n",
    "            # Root Mean Square Energy\n",
    "            rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Spectral Centroid and Rolloff\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.vstack([\n",
    "                mfccs,\n",
    "                mfcc_delta,\n",
    "                mfcc_delta2,\n",
    "                chroma,\n",
    "                spectral_contrast,\n",
    "                tonnetz,\n",
    "                rms,\n",
    "                zcr,\n",
    "                spectral_centroid,\n",
    "                spectral_rolloff\n",
    "            ])\n",
    "            \n",
    "            # Extract statistical features\n",
    "            feature_stats = self._extract_statistics(features)\n",
    "            \n",
    "            return feature_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            return np.zeros(280)  # Return zero array with expected feature dimension\n",
    "    \n",
    "    def _extract_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Extract statistical features from feature matrix\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for feature in features:\n",
    "            stats.extend([\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.median(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 75),\n",
    "                np.mean(np.diff(feature)),  # Average change\n",
    "                np.std(np.diff(feature))    # Std of changes\n",
    "            ])\n",
    "        return np.array(stats)\n",
    "    \n",
    "    def extract_log_mel_spectrogram(self, audio_path):\n",
    "        \"\"\"\n",
    "        Alternative: Log-Mel Spectrogram features\n",
    "        \"\"\"\n",
    "        y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "        \n",
    "        # Extract Mel-spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, \n",
    "            sr=sr, \n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            n_mels=128\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        return log_mel\n",
    "    \n",
    "    def extract_all_features(self, audio_paths, labels=None):\n",
    "        \"\"\"\n",
    "        Extract features from multiple audio files\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        valid_paths = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, path in enumerate(audio_paths):\n",
    "            feat = self.extract_mfcc_features(path)\n",
    "            if np.sum(feat) != 0:  # Skip zero features\n",
    "                features.append(feat)\n",
    "                valid_paths.append(path)\n",
    "                if labels is not None:\n",
    "                    valid_labels.append(labels[idx])\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Normalize features\n",
    "        if len(features) > 0:\n",
    "            features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Encode labels if provided\n",
    "        if labels is not None and len(valid_labels) > 0:\n",
    "            encoded_labels = self.label_encoder.fit_transform(valid_labels)\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        \n",
    "        return features, encoded_labels, valid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53eebefc-aba4-4b49-a10c-ce3d4ea2c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "class UrbanSound8KProcessor:\n",
    "    \"\"\"\n",
    "    Process UrbanSound8K dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        \n",
    "    def prepare_data(self, folds=None):\n",
    "        \"\"\"\n",
    "        Prepare data for specific folds or all folds\n",
    "        \"\"\"\n",
    "        if folds is None:\n",
    "            folds = list(range(1, 11))\n",
    "        elif isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        \n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        fold_numbers = []\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_data = self.metadata[self.metadata['fold'] == fold]\n",
    "            \n",
    "            for _, row in tqdm(fold_data.iterrows(), desc=f'Processing Fold {fold}'):\n",
    "                audio_file = os.path.join(\n",
    "                    self.dataset_path,\n",
    "                    'fold' + str(row['fold']),\n",
    "                    row['slice_file_name']\n",
    "                )\n",
    "                \n",
    "                if os.path.exists(audio_file):\n",
    "                    audio_paths.append(audio_file)\n",
    "                    labels.append(row['class'])\n",
    "                    fold_numbers.append(fold)\n",
    "        \n",
    "        return audio_paths, labels, fold_numbers\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Get class distribution statistics\n",
    "        \"\"\"\n",
    "        return self.metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3984876-94e3-4cba-9caa-77add1c12391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "class TraditionalMLClassifier:\n",
    "    \"\"\"\n",
    "    Traditional machine learning classifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "            'svm': SVC(probability=True, random_state=42),\n",
    "            'xgboost': XGBClassifier(n_estimators=200, random_state=42),\n",
    "            'gradient_boosting': GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "        }\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train all models\n",
    "        \"\"\"\n",
    "        trained_models = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            trained_models[name] = model\n",
    "            \n",
    "        return trained_models\n",
    "    \n",
    "    def evaluate(self, models, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate all models\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            accuracy = model.score(X_test, y_test)\n",
    "            results[name] = accuracy\n",
    "            print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17aca480-f7e8-49f8-b214-f4d6b994528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "class CNNClassifier:\n",
    "    \"\"\"\n",
    "    CNN-based classifier for audio features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build CNN model architecture\n",
    "        \"\"\"\n",
    "        model = models.Sequential([\n",
    "            # Reshape for CNN input\n",
    "            layers.Reshape((self.input_shape[0], self.input_shape[1], 1), \n",
    "                          input_shape=self.input_shape),\n",
    "            \n",
    "            # First Conv Block\n",
    "            layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Second Conv Block\n",
    "            layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Third Conv Block\n",
    "            layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Global Pooling and Dense Layers\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            # Output layer\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train CNN model\n",
    "        \"\"\"\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6223b4e1-eb85-4c47-a243-04135ac0a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Processing UrbanSound8K dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Fold 1: 873it [00:00, 17267.43it/s]\n",
      "Processing Fold 2: 888it [00:00, 38398.13it/s]\n",
      "Processing Fold 3: 925it [00:00, 9325.92it/s]\n",
      "Processing Fold 4: 990it [00:00, 14548.66it/s]\n",
      "Processing Fold 5: 936it [00:00, 14790.82it/s]\n",
      "Processing Fold 6: 823it [00:00, 14524.70it/s]\n",
      "Processing Fold 7: 838it [00:00, 13646.00it/s]\n",
      "Processing Fold 8: 806it [00:00, 13961.67it/s]\n",
      "Processing Fold 9: 816it [00:00, 14644.94it/s]\n",
      "Processing Fold 10: 837it [00:00, 13699.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 0\n",
      "Testing samples: 0\n",
      "\n",
      "Step 2: Extracting features...\n",
      "Feature shape - Train: (0,), Test: (0,)\n",
      "\n",
      "Step 3: Training traditional ML models...\n",
      "Training random_forest...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This RandomForestClassifier estimator requires y to be passed, but the target y is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResults saved to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mresults/model_results.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 3: Training traditional ML models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m ml_classifier = TraditionalMLClassifier()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m trained_models = \u001b[43mml_classifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Step 4: Evaluate models\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 4: Evaluating models...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mTraditionalMLClassifier.train\u001b[39m\u001b[34m(self, X_train, y_train)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models.items():\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     trained_models[name] = model\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trained_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\ensemble\\_forest.py:359\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    372\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\validation.py:2932\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2930\u001b[39m tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n\u001b[32m-> \u001b[39m\u001b[32m2932\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2933\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_estimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m estimator \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2934\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequires y to be passed, but the target y is None.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2935\u001b[39m     )\n\u001b[32m   2937\u001b[39m no_val_X = \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m X == \u001b[33m\"\u001b[39m\u001b[33mno_validation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2938\u001b[39m no_val_y = y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(y, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y == \u001b[33m\"\u001b[39m\u001b[33mno_validation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: This RandomForestClassifier estimator requires y to be passed, but the target y is None."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize paths\n",
    "    DATASET_PATH = \"UrbanSound8K/\"  # Update this path\n",
    "    OUTPUT_PATH = \"results\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Process data\n",
    "    print(\"Step 1: Processing UrbanSound8K dataset...\")\n",
    "    processor = UrbanSound8KProcessor(DATASET_PATH)\n",
    "    \n",
    "    # Prepare data for folds 1-8 for training, 9-10 for testing\n",
    "    train_folds = list(range(1, 9))\n",
    "    test_folds = [9, 10]\n",
    "    \n",
    "    train_paths, train_labels, _ = processor.prepare_data(train_folds)\n",
    "    test_paths, test_labels, _ = processor.prepare_data(test_folds)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_paths)}\")\n",
    "    print(f\"Testing samples: {len(test_paths)}\")\n",
    "    \n",
    "    # Step 2: Extract features\n",
    "    print(\"\\nStep 2: Extracting features...\")\n",
    "    extractor = RobustMFCCExtractor()\n",
    "    \n",
    "    X_train, y_train, _ = extractor.extract_all_features(train_paths, train_labels)\n",
    "    X_test, y_test, _ = extractor.extract_all_features(test_paths, test_labels)\n",
    "    \n",
    "    print(f\"Feature shape - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Step 3: Train traditional models\n",
    "    print(\"\\nStep 3: Training traditional ML models...\")\n",
    "    ml_classifier = TraditionalMLClassifier()\n",
    "    trained_models = ml_classifier.train(X_train, y_train)\n",
    "    \n",
    "    # Step 4: Evaluate models\n",
    "    print(\"\\nStep 4: Evaluating models...\")\n",
    "    results = ml_classifier.evaluate(trained_models, X_test, y_test)\n",
    "    \n",
    "    # Step 5: For CNN - reshape features for 2D input\n",
    "    print(\"\\nStep 5: Training CNN model...\")\n",
    "    \n",
    "    # Reshape features to 2D (assuming we use log-mel spectrograms)\n",
    "    X_train_cnn = []\n",
    "    X_test_cnn = []\n",
    "    \n",
    "    for path in tqdm(train_paths, desc=\"Extracting CNN features for train\"):\n",
    "        log_mel = extractor.extract_log_mel_spectrogram(path)\n",
    "        X_train_cnn.append(log_mel)\n",
    "    \n",
    "    for path in tqdm(test_paths, desc=\"Extracting CNN features for test\"):\n",
    "        log_mel = extractor.extract_log_mel_spectrogram(path)\n",
    "        X_test_cnn.append(log_mel)\n",
    "    \n",
    "    X_train_cnn = np.array(X_train_cnn)\n",
    "    X_test_cnn = np.array(X_test_cnn)\n",
    "    \n",
    "    # Encode labels for CNN\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_cnn = label_encoder.fit_transform(train_labels)\n",
    "    y_test_cnn = label_encoder.transform(test_labels)\n",
    "    \n",
    "    # Build and train CNN\n",
    "    cnn_classifier = CNNClassifier(\n",
    "        input_shape=X_train_cnn[0].shape,\n",
    "        num_classes=len(np.unique(y_train_cnn))\n",
    "    )\n",
    "    \n",
    "    history = cnn_classifier.train(\n",
    "        X_train_cnn, y_train_cnn,\n",
    "        X_test_cnn, y_test_cnn,\n",
    "        epochs=50,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Evaluate CNN\n",
    "    cnn_test_loss, cnn_test_acc = cnn_classifier.model.evaluate(X_test_cnn, y_test_cnn)\n",
    "    print(f\"\\nCNN Test Accuracy: {cnn_test_acc:.4f}\")\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()) + ['CNN'],\n",
    "        'Accuracy': list(results.values()) + [cnn_test_acc]\n",
    "    })\n",
    "    \n",
    "    results_df.to_csv(os.path.join(OUTPUT_PATH, 'model_results.csv'), index=False)\n",
    "    print(\"\\nResults saved to 'results/model_results.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4bd3b9-0850-4029-bf3c-8e6a9da8f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedMFCCTechniques:\n",
    "    \"\"\"\n",
    "    Advanced techniques for improved MFCC extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_vtlp(audio, sr, alpha=0.9):\n",
    "        \"\"\"\n",
    "        Vocal Tract Length Perturbation\n",
    "        \"\"\"\n",
    "        # Implement VTLP warping\n",
    "        n_fft = 2048\n",
    "        mel_basis = librosa.filters.mel(sr, n_fft)\n",
    "        warped_mel_basis = AdvancedMFCCTechniques._warp_mel_basis(mel_basis, alpha)\n",
    "        \n",
    "        # Apply warped mel basis\n",
    "        stft = librosa.stft(audio, n_fft=n_fft)\n",
    "        mel_spec = np.dot(warped_mel_basis, np.abs(stft))\n",
    "        \n",
    "        return librosa.power_to_db(mel_spec)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _warp_mel_basis(mel_basis, alpha):\n",
    "        \"\"\"\n",
    "        Warp mel basis for VTLP\n",
    "        \"\"\"\n",
    "        # Simplified warping - implement full VTLP for production\n",
    "        return mel_basis\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_mfcc_with_snr_weighting(audio, sr, snr_threshold=20):\n",
    "        \"\"\"\n",
    "        Extract MFCCs with SNR-based weighting\n",
    "        \"\"\"\n",
    "        # Calculate SNR\n",
    "        signal_power = np.mean(audio**2)\n",
    "        noise_power = np.mean((audio - np.mean(audio))**2)\n",
    "        snr = 10 * np.log10(signal_power / (noise_power + 1e-10))\n",
    "        \n",
    "        # Weight features based on SNR\n",
    "        if snr < snr_threshold:\n",
    "            # Apply noise reduction or use robust features\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "            # Apply additional processing for noisy audio\n",
    "            return mfccs * (snr / snr_threshold)\n",
    "        else:\n",
    "            return librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_frequency_masking(mfcc_features, time_mask=2, freq_mask=2):\n",
    "        \"\"\"\n",
    "        Apply SpecAugment-like time and frequency masking\n",
    "        \"\"\"\n",
    "        # Time masking\n",
    "        if time_mask > 0:\n",
    "            t = np.random.randint(0, mfcc_features.shape[1] - time_mask)\n",
    "            mfcc_features[:, t:t+time_mask] = 0\n",
    "        \n",
    "        # Frequency masking\n",
    "        if freq_mask > 0:\n",
    "            f = np.random.randint(0, mfcc_features.shape[0] - freq_mask)\n",
    "            mfcc_features[f:f+freq_mask, :] = 0\n",
    "        \n",
    "        return mfcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ff61e0d-3c8b-465f-9641-3ae9e8bd0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if len(y_pred.shape) > 1:  # For neural networks\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_pred_proba = y_pred\n",
    "    else:  # For traditional models\n",
    "        y_pred_classes = y_pred\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred_classes, \n",
    "                                target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC-AUC for multi-class\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "        print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "    except:\n",
    "        print(\"\\nROC-AUC calculation skipped (requires probability estimates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f7cbd-53f4-4ece-b4c8-2e2ac3823095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UrbanSound8K Audio Classification System\n",
      "============================================================\n",
      "Running debug version...\n",
      "Using file: UrbanSound8K\\audio\\fold1\\101415-3-0-2.wav\n",
      "Feature extraction test - Shape: (1287,)\n",
      "Features (first 10): [-539.57764898  170.28682325 -606.35736084 -718.22149658 -204.11100769\n",
      " -701.68151855 -365.28274536    1.41649184   43.2259207    43.78187459]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to run the full pipeline? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step 1: Processing UrbanSound8K dataset...\n",
      "============================================================\n",
      "\n",
      "Class Distribution:\n",
      "class\n",
      "dog_bark            1000\n",
      "children_playing    1000\n",
      "air_conditioner     1000\n",
      "street_music        1000\n",
      "jackhammer          1000\n",
      "engine_idling       1000\n",
      "drilling            1000\n",
      "siren                929\n",
      "car_horn             429\n",
      "gun_shot             374\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 8732\n",
      "\n",
      "Preparing training data (folds [1, 2, 3, 4, 5, 6, 7, 8])...\n",
      "Found 7079 valid audio files out of 6448 expected\n",
      "\n",
      "Preparing testing data (folds [9, 10])...\n",
      "Found 1653 valid audio files out of 1674 expected\n",
      "\n",
      "Training samples: 7079\n",
      "Testing samples: 1653\n",
      "\n",
      "============================================================\n",
      "Step 2: Extracting features...\n",
      "============================================================\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|█████████████████████████████████████████████████████████| 7079/7079 [16:52<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting testing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|█████████████████████████████████████████████████████████| 1653/1653 [04:03<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature shape - Train: (7079, 1287)\n",
      "Feature shape - Test: (1653, 1287)\n",
      "\n",
      "============================================================\n",
      "Step 3: Training traditional ML models...\n",
      "============================================================\n",
      "Training random_forest...\n",
      "Training svm...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Enhanced Feature Extraction (FIXED)\n",
    "# ============================================\n",
    "\n",
    "class RobustMFCCExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced MFCC feature extraction with multiple robust techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=40, n_fft=2048, hop_length=512):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_mfcc_features(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract comprehensive MFCC-based features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with robust loading\n",
    "            y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "            \n",
    "            # Zero-padding or truncation for consistent length\n",
    "            target_length = self.sr * 4  # 4 seconds\n",
    "            if len(y) < target_length:\n",
    "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:target_length]\n",
    "            \n",
    "            # Apply pre-emphasis filter\n",
    "            y = librosa.effects.preemphasis(y)\n",
    "            \n",
    "            # Extract base MFCCs\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Extract delta and delta-delta features\n",
    "            mfcc_delta = librosa.feature.delta(mfccs)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "            \n",
    "            # Extract other complementary features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Root Mean Square Energy\n",
    "            rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Spectral Centroid and Rolloff\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.vstack([\n",
    "                mfccs,\n",
    "                mfcc_delta,\n",
    "                mfcc_delta2,\n",
    "                chroma,\n",
    "                spectral_contrast,\n",
    "                rms,\n",
    "                zcr,\n",
    "                spectral_centroid,\n",
    "                spectral_rolloff\n",
    "            ])\n",
    "            \n",
    "            # Extract statistical features\n",
    "            feature_stats = self._extract_statistics(features)\n",
    "            \n",
    "            return feature_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            # Return zeros with appropriate dimension\n",
    "            # Calculate expected dimension: (40 + 40 + 40 + 12 + 7 + 1 + 1 + 1 + 1) * 9 = 1026\n",
    "            return np.zeros(1026)  # Fixed dimension\n",
    "    \n",
    "    def _extract_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Extract statistical features from feature matrix\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for feature in features:\n",
    "            stats.extend([\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.median(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 75),\n",
    "                np.mean(np.diff(feature)),  # Average change\n",
    "                np.std(np.diff(feature))    # Std of changes\n",
    "            ])\n",
    "        return np.array(stats)\n",
    "    \n",
    "    def extract_all_features(self, audio_paths, labels=None):\n",
    "        \"\"\"\n",
    "        Extract features from multiple audio files\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, path in enumerate(tqdm(audio_paths, desc=\"Extracting features\")):\n",
    "            feat = self.extract_mfcc_features(path)\n",
    "            if np.sum(feat) != 0 and not np.isnan(feat).any():  # Skip zero or NaN features\n",
    "                features.append(feat)\n",
    "                if labels is not None:\n",
    "                    valid_labels.append(labels[idx])\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"WARNING: No valid features extracted!\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Normalize features\n",
    "        features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Encode labels if provided\n",
    "        if labels is not None and len(valid_labels) > 0:\n",
    "            encoded_labels = self.label_encoder.fit_transform(valid_labels)\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        \n",
    "        return features, encoded_labels\n",
    "\n",
    "# ============================================\n",
    "# 2. UrbanSound8K Processor (FIXED)\n",
    "# ============================================\n",
    "\n",
    "class UrbanSound8KProcessor:\n",
    "    \"\"\"\n",
    "    Process UrbanSound8K dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            # Try alternative path structure\n",
    "            self.metadata_path = os.path.join(dataset_path, 'UrbanSound8K.csv')\n",
    "        \n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {self.metadata_path}\")\n",
    "        \n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        \n",
    "    def prepare_data(self, folds=None):\n",
    "        \"\"\"\n",
    "        Prepare data for specific folds or all folds\n",
    "        \"\"\"\n",
    "        if folds is None:\n",
    "            folds = list(range(1, 11))\n",
    "        elif isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        \n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        fold_numbers = []\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_data = self.metadata[self.metadata['fold'] == fold]\n",
    "            \n",
    "            for _, row in fold_data.iterrows():\n",
    "                # Try different possible paths\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.dataset_path, 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, 'audio', 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, str(row['fold']), row['slice_file_name'])\n",
    "                ]\n",
    "                \n",
    "                audio_file = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        audio_file = path\n",
    "                        break\n",
    "                \n",
    "                if audio_file:\n",
    "                    audio_paths.append(audio_file)\n",
    "                    labels.append(row['class'])\n",
    "                    fold_numbers.append(fold)\n",
    "                else:\n",
    "                    print(f\"WARNING: File not found: {row['slice_file_name']} in fold {fold}\")\n",
    "        \n",
    "        print(f\"Found {len(audio_paths)} valid audio files out of {len(fold_data) * len(folds)} expected\")\n",
    "        return audio_paths, labels, fold_numbers\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Get class distribution statistics\n",
    "        \"\"\"\n",
    "        return self.metadata['class'].value_counts()\n",
    "\n",
    "# ============================================\n",
    "# 3. Traditional ML Classifier\n",
    "# ============================================\n",
    "\n",
    "class TraditionalMLClassifier:\n",
    "    \"\"\"\n",
    "    Traditional machine learning classifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'svm': SVC(probability=True, random_state=42),\n",
    "            'xgboost': XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train all models\n",
    "        \"\"\"\n",
    "        if X_train.shape[0] == 0 or y_train is None:\n",
    "            raise ValueError(\"Training data is empty or labels are None!\")\n",
    "            \n",
    "        trained_models = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            trained_models[name] = model\n",
    "            \n",
    "        return trained_models\n",
    "    \n",
    "    def evaluate(self, models, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate all models\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            accuracy = model.score(X_test, y_test)\n",
    "            results[name] = accuracy\n",
    "            print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "        return results\n",
    "\n",
    "# ============================================\n",
    "# 4. Main Execution Pipeline (FIXED)\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    # Initialize paths\n",
    "    # UPDATE THIS PATH to your UrbanSound8K dataset location\n",
    "    DATASET_PATH = \"UrbanSound8K/\"  # Update this path\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Step 1: Process data\n",
    "    print(\"=\"*60)\n",
    "    print(\"Step 1: Processing UrbanSound8K dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        processor = UrbanSound8KProcessor(DATASET_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You have downloaded UrbanSound8K dataset\")\n",
    "        print(\"2. The dataset is extracted in the correct location\")\n",
    "        print(f\"3. Update DATASET_PATH variable (currently: {DATASET_PATH})\")\n",
    "        print(\"\\nDownload from: https://urbansounddataset.weebly.com/urbansound8k.html\")\n",
    "        return\n",
    "    \n",
    "    # Show class distribution\n",
    "    class_dist = processor.get_class_distribution()\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(class_dist)\n",
    "    print(f\"\\nTotal samples: {len(processor.metadata)}\")\n",
    "    \n",
    "    # Prepare data for folds 1-8 for training, 9-10 for testing\n",
    "    train_folds = list(range(1, 9))\n",
    "    test_folds = [9, 10]\n",
    "    \n",
    "    print(f\"\\nPreparing training data (folds {train_folds})...\")\n",
    "    train_paths, train_labels, _ = processor.prepare_data(train_folds)\n",
    "    \n",
    "    print(f\"\\nPreparing testing data (folds {test_folds})...\")\n",
    "    test_paths, test_labels, _ = processor.prepare_data(test_folds)\n",
    "    \n",
    "    print(f\"\\nTraining samples: {len(train_paths)}\")\n",
    "    print(f\"Testing samples: {len(test_paths)}\")\n",
    "    \n",
    "    if len(train_paths) == 0 or len(test_paths) == 0:\n",
    "        print(\"ERROR: No audio files found!\")\n",
    "        print(\"Check if audio files exist in the dataset directory\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Extract features\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 2: Extracting features...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    extractor = RobustMFCCExtractor()\n",
    "    \n",
    "    print(\"Extracting training features...\")\n",
    "    X_train, y_train = extractor.extract_all_features(train_paths, train_labels)\n",
    "    \n",
    "    print(\"\\nExtracting testing features...\")\n",
    "    X_test, y_test = extractor.extract_all_features(test_paths, test_labels)\n",
    "    \n",
    "    print(f\"\\nFeature shape - Train: {X_train.shape if len(X_train) > 0 else 'Empty'}\")\n",
    "    print(f\"Feature shape - Test: {X_test.shape if len(X_test) > 0 else 'Empty'}\")\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"ERROR: Feature extraction failed!\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"1. Audio files might be corrupted\")\n",
    "        print(\"2. Librosa might not be reading the files\")\n",
    "        print(\"3. Check file formats (should be .wav)\")\n",
    "        return\n",
    "    \n",
    "    # Check if labels are extracted\n",
    "    if y_train is None or y_test is None:\n",
    "        print(\"ERROR: Labels not extracted!\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Train traditional models\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 3: Training traditional ML models...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ml_classifier = TraditionalMLClassifier()\n",
    "    \n",
    "    try:\n",
    "        trained_models = ml_classifier.train(X_train, y_train)\n",
    "    except ValueError as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Evaluate models\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 4: Evaluating models...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = ml_classifier.evaluate(trained_models, X_test, y_test)\n",
    "    \n",
    "    # Step 5: Detailed evaluation for best model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 5: Detailed evaluation...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(results, key=results.get)\n",
    "    best_model = trained_models[best_model_name]\n",
    "    print(f\"\\nBest model: {best_model_name} (Accuracy: {results[best_model_name]:.4f})\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test) if hasattr(best_model, 'predict_proba') else None\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=extractor.label_encoder.classes_))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=extractor.label_encoder.classes_,\n",
    "                yticklabels=extractor.label_encoder.classes_)\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/confusion_matrix.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC-AUC if probabilities available\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')\n",
    "            print(f\"\\nROC-AUC Score (macro): {roc_auc:.4f}\")\n",
    "        except:\n",
    "            print(\"\\nROC-AUC calculation skipped\")\n",
    "    \n",
    "    # Step 6: Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'Accuracy': list(results.values())\n",
    "    })\n",
    "    \n",
    "    results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "    results_df.to_csv('results/model_results.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(f\"\\nResults saved to 'results/model_results.csv'\")\n",
    "    \n",
    "    # Save feature information\n",
    "    feature_info = {\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'feature_dim': X_train.shape[1] if len(X_train) > 0 else 0,\n",
    "        'classes': list(extractor.label_encoder.classes_)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('results/feature_info.json', 'w') as f:\n",
    "        json.dump(feature_info, f, indent=2)\n",
    "\n",
    "# ============================================\n",
    "# Alternative: Simple Debug Version\n",
    "# ============================================\n",
    "\n",
    "def debug_version():\n",
    "    \"\"\"\n",
    "    Simple debug version to test basic functionality\n",
    "    \"\"\"\n",
    "    print(\"Running debug version...\")\n",
    "    \n",
    "    # Test with a single file first\n",
    "    test_file = \"UrbanSound8K/fold1/7061-6-0-0.wav\"  # Try with a known file\n",
    "    \n",
    "    if not os.path.exists(test_file):\n",
    "        # Try to find any .wav file\n",
    "        import glob\n",
    "        wav_files = glob.glob(\"UrbanSound8K/**/*.wav\", recursive=True)\n",
    "        if wav_files:\n",
    "            test_file = wav_files[0]\n",
    "            print(f\"Using file: {test_file}\")\n",
    "        else:\n",
    "            print(\"No .wav files found!\")\n",
    "            return\n",
    "    \n",
    "    # Test feature extraction\n",
    "    extractor = RobustMFCCExtractor()\n",
    "    features = extractor.extract_mfcc_features(test_file)\n",
    "    print(f\"Feature extraction test - Shape: {features.shape}\")\n",
    "    print(f\"Features (first 10): {features[:10]}\")\n",
    "    \n",
    "    # Test loading metadata\n",
    "    if os.path.exists(\"UrbanSound8K/UrbanSound8K.csv\"):\n",
    "        metadata = pd.read_csv(\"UrbanSound8K/UrbanSound8K.csv\")\n",
    "        print(f\"\\nMetadata loaded: {len(metadata)} rows\")\n",
    "        print(f\"Columns: {list(metadata.columns)}\")\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        print(metadata['class'].value_counts())\n",
    "        \n",
    "        # Test with first 10 files\n",
    "        sample_files = metadata.head(10)\n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in sample_files.iterrows():\n",
    "            path = f\"UrbanSound8K/fold{row['fold']}/{row['slice_file_name']}\"\n",
    "            if os.path.exists(path):\n",
    "                audio_paths.append(path)\n",
    "                labels.append(row['class'])\n",
    "        \n",
    "        print(f\"\\nFound {len(audio_paths)} valid files out of 10\")\n",
    "        \n",
    "        if len(audio_paths) > 0:\n",
    "            X, y = extractor.extract_all_features(audio_paths, labels)\n",
    "            print(f\"\\nExtracted features shape: {X.shape}\")\n",
    "            print(f\"Labels shape: {y.shape if y is not None else 'None'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"UrbanSound8K Audio Classification System\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # First run debug to check basic functionality\n",
    "    debug_version()\n",
    "    \n",
    "    # Ask user if they want to run full pipeline\n",
    "    response = input(\"\\nDo you want to run the full pipeline? (yes/no): \")\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nDebug mode completed. Fix any issues before running full pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e058b-73c3-4696-a2b2-bc3feedef8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
