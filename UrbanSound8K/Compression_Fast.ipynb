{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23df62aa-6b37-46a5-8f79-fd3901c90300",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' from 'C:\\Users\\ch.sc.u4cse23160\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedShuffleSplit\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\fixes.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     22\u001b[39m     pd = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py:151\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    134\u001b[39m     concat,\n\u001b[32m    135\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     qcut,\n\u001b[32m    148\u001b[39m )\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[32m    156\u001b[39m     ExcelFile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m     read_spss,\n\u001b[32m    185\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\testing.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mPublic testing utility functions.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     assert_extension_array_equal,\n\u001b[32m      8\u001b[39m     assert_frame_equal,\n\u001b[32m      9\u001b[39m     assert_index_equal,\n\u001b[32m     10\u001b[39m     assert_series_equal,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_extension_array_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_frame_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_series_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_index_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\_testing\\__init__.py:405\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytest.raises(expected_exception, match=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m cython_table = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m.common._cython_table.items()\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    keys and expected result.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m \u001b[33;03m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'pandas' from 'C:\\Users\\ch.sc.u4cse23160\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class FixedCompressionFramework:\n",
    "    \"\"\"Fixed framework with proper stratified sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, wavelet='db4', dwt_level=3, sample_rate=16000, duration=2.0, random_seed=42):\n",
    "        self.wavelet = wavelet\n",
    "        self.dwt_level = dwt_level\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.random_seed = random_seed\n",
    "        self.scaler = StandardScaler()\n",
    "        self.failed_files = []\n",
    "        \n",
    "        # Create output directories\n",
    "        self.create_directories()\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        directories = ['results', 'visualizations']\n",
    "        for dir_name in directories:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    def load_dataset(self, n_files=100):\n",
    "        \"\"\"Load dataset with proper sampling\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        \n",
    "        base_path = \"UrbanSound8K/\"\n",
    "        metadata_path = os.path.join(base_path, \"metadata/UrbanSound8K.csv\")\n",
    "        \n",
    "        if os.path.exists(metadata_path):\n",
    "            metadata = pd.read_csv(metadata_path)\n",
    "            \n",
    "            # Ensure balanced sampling across classes\n",
    "            file_paths = []\n",
    "            labels = []\n",
    "            \n",
    "            # Get unique classes\n",
    "            unique_classes = metadata['class'].unique()\n",
    "            \n",
    "            # Sample n_files per class (or as many as available)\n",
    "            files_per_class = max(1, n_files // len(unique_classes))\n",
    "            \n",
    "            for class_name in unique_classes:\n",
    "                class_files = metadata[metadata['class'] == class_name]\n",
    "                \n",
    "                # Take min(files_per_class, available_files)\n",
    "                sample_size = min(files_per_class, len(class_files))\n",
    "                if sample_size > 0:\n",
    "                    sampled_files = class_files.sample(sample_size, random_state=self.random_seed)\n",
    "                    \n",
    "                    for _, row in sampled_files.iterrows():\n",
    "                        fold = row['fold']\n",
    "                        filename = row['slice_file_name']\n",
    "                        file_path = os.path.join(base_path, f\"audio/fold{fold}\", filename)\n",
    "                        file_path = file_path.replace('\\\\', '/')\n",
    "                        \n",
    "                        if os.path.exists(file_path):\n",
    "                            file_paths.append(file_path)\n",
    "                            labels.append(class_name)\n",
    "                        else:\n",
    "                            # Try alternative path\n",
    "                            alt_path = os.path.join(base_path, f\"fold{fold}\", filename)\n",
    "                            alt_path = alt_path.replace('\\\\', '/')\n",
    "                            if os.path.exists(alt_path):\n",
    "                                file_paths.append(alt_path)\n",
    "                                labels.append(class_name)\n",
    "            \n",
    "            print(f\"Loaded {len(file_paths)} files, {len(set(labels))} classes\")\n",
    "            return file_paths, labels\n",
    "        \n",
    "        else:\n",
    "            print(\"UrbanSound8K not found, creating synthetic dataset...\")\n",
    "            return self.create_synthetic_dataset(n_files)\n",
    "    \n",
    "    def create_synthetic_dataset(self, n_files=100):\n",
    "        \"\"\"Create synthetic audio files\"\"\"\n",
    "        print(f\"Creating {n_files} synthetic audio files...\")\n",
    "        \n",
    "        temp_dir = \"synthetic_audio\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        class_names = ['sine', 'noise', 'chirp', 'silence', 'mixed']\n",
    "        sr = self.sample_rate\n",
    "        duration = self.duration\n",
    "        \n",
    "        files_per_class = max(1, n_files // len(class_names))\n",
    "        \n",
    "        for class_idx, label in enumerate(class_names):\n",
    "            for i in range(files_per_class):\n",
    "                if label == 'sine':\n",
    "                    t = np.linspace(0, duration, int(sr * duration))\n",
    "                    freq = 440 + (class_idx * 100) + (i * 10)\n",
    "                    audio = 0.5 * np.sin(2 * np.pi * freq * t)\n",
    "                elif label == 'noise':\n",
    "                    audio = np.random.randn(int(sr * duration)) * 0.1\n",
    "                elif label == 'chirp':\n",
    "                    t = np.linspace(0, duration, int(sr * duration))\n",
    "                    freq = 100 + 500 * t + (class_idx * 100)\n",
    "                    audio = 0.3 * np.sin(2 * np.pi * freq * t)\n",
    "                elif label == 'silence':\n",
    "                    audio = np.zeros(int(sr * duration))\n",
    "                else:  # mixed\n",
    "                    t = np.linspace(0, duration, int(sr * duration))\n",
    "                    audio = 0.2 * np.sin(2 * np.pi * 440 * t) + \\\n",
    "                            0.1 * np.random.randn(int(sr * duration))\n",
    "                \n",
    "                filename = f\"{temp_dir}/audio_{label}_{class_idx}_{i}.wav\"\n",
    "                wavfile.write(filename, sr, (audio * 32767).astype(np.int16))\n",
    "                \n",
    "                file_paths.append(filename)\n",
    "                labels.append(label)\n",
    "        \n",
    "        print(f\"Created {len(file_paths)} synthetic files\")\n",
    "        return file_paths, labels\n",
    "    \n",
    "    def extract_features(self, audio, sr):\n",
    "        \"\"\"Extract robust features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. Basic statistics\n",
    "        features.append(np.mean(audio))\n",
    "        features.append(np.std(audio))\n",
    "        features.append(np.max(np.abs(audio)))\n",
    "        features.append(np.min(audio))\n",
    "        \n",
    "        # 2. Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio, frame_length=2048, hop_length=512)\n",
    "        features.append(np.mean(zcr))\n",
    "        features.append(np.std(zcr))\n",
    "        \n",
    "        # 3. Energy\n",
    "        energy = np.sum(audio ** 2)\n",
    "        features.append(energy)\n",
    "        \n",
    "        # 4. Simple spectral features\n",
    "        if len(audio) > 10 and np.any(audio != 0):\n",
    "            try:\n",
    "                # Simple FFT\n",
    "                fft = np.fft.fft(audio)\n",
    "                magnitude = np.abs(fft)\n",
    "                \n",
    "                # Take first half (positive frequencies)\n",
    "                half_len = len(magnitude) // 2\n",
    "                if half_len > 0:\n",
    "                    pos_mag = magnitude[:half_len]\n",
    "                    \n",
    "                    features.append(np.mean(pos_mag))\n",
    "                    features.append(np.std(pos_mag))\n",
    "                    features.append(np.max(pos_mag))\n",
    "                    \n",
    "                    # Simple spectral centroid\n",
    "                    if np.sum(pos_mag) > 0:\n",
    "                        freqs = np.fft.fftfreq(len(audio), 1/sr)[:half_len]\n",
    "                        centroid = np.sum(freqs * pos_mag) / np.sum(pos_mag)\n",
    "                        features.append(centroid)\n",
    "                    else:\n",
    "                        features.append(0.0)\n",
    "                else:\n",
    "                    features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "                    \n",
    "            except:\n",
    "                features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "        else:\n",
    "            features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "        \n",
    "        # Ensure fixed size\n",
    "        target_size = 13\n",
    "        if len(features) < target_size:\n",
    "            features.extend([0.0] * (target_size - len(features)))\n",
    "        \n",
    "        return np.array(features[:target_size])\n",
    "    \n",
    "    def create_measurement_matrix(self, M, N, matrix_type):\n",
    "        \"\"\"Create measurement matrix\"\"\"\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        if matrix_type == 'bernoulli':\n",
    "            return np.random.choice([-1, 1], size=(M, N)) / np.sqrt(M)\n",
    "        elif matrix_type == 'gaussian':\n",
    "            return np.random.randn(M, N) / np.sqrt(M)\n",
    "        else:  # logistic_henon\n",
    "            r = 3.99\n",
    "            a, b = 1.4, 0.3\n",
    "            x_log = np.random.rand()\n",
    "            x_hen, y_hen = np.random.rand(), np.random.rand()\n",
    "            \n",
    "            matrix = np.zeros((M, N))\n",
    "            for i in range(M):\n",
    "                for j in range(N):\n",
    "                    # Logistic map\n",
    "                    x_log = r * x_log * (1 - x_log)\n",
    "                    \n",
    "                    # Henon map\n",
    "                    x_hen_new = 1 - a * x_hen**2 + y_hen\n",
    "                    y_hen = b * x_hen\n",
    "                    x_hen = x_hen_new\n",
    "                    \n",
    "                    # Combine\n",
    "                    chaotic = (x_log + x_hen) / 2\n",
    "                    matrix[i, j] = 2 * chaotic - 1\n",
    "                \n",
    "                # Reset occasionally\n",
    "                if i % 100 == 0:\n",
    "                    x_log = (x_log + np.random.rand()) / 2\n",
    "                    x_hen = (x_hen + np.random.rand()) / 2\n",
    "            \n",
    "            return matrix / np.sqrt(M)\n",
    "    \n",
    "    def compress_audio(self, audio, compression_ratio=0.5, matrix_type='bernoulli'):\n",
    "        \"\"\"Compress audio using compressed sensing\"\"\"\n",
    "        # Apply DWT\n",
    "        try:\n",
    "            coeffs = pywt.wavedec(audio, self.wavelet, level=self.dwt_level)\n",
    "            coeffs_flat = np.concatenate(coeffs)\n",
    "        except:\n",
    "            # If DWT fails, use raw audio (truncated)\n",
    "            coeffs_flat = audio[:min(len(audio), 1000)]\n",
    "        \n",
    "        N = len(coeffs_flat)\n",
    "        M = max(10, int(N * compression_ratio))  # Minimum 10 measurements\n",
    "        \n",
    "        # Generate measurement matrix\n",
    "        Phi = self.create_measurement_matrix(M, N, matrix_type)\n",
    "        \n",
    "        # Compress\n",
    "        compressed = Phi @ coeffs_flat\n",
    "        \n",
    "        # Add statistics\n",
    "        features = np.concatenate([\n",
    "            compressed,\n",
    "            [\n",
    "                np.mean(compressed),\n",
    "                np.std(compressed),\n",
    "                np.max(np.abs(compressed)),\n",
    "                M / N  # Actual compression ratio\n",
    "            ]\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_all_features(self, file_paths, labels, compression_ratio=None, matrix_type=None):\n",
    "        \"\"\"Prepare features with proper error handling\"\"\"\n",
    "        print(\"Extracting features...\")\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for file_path, label in tqdm(zip(file_paths, labels), total=len(file_paths), desc=\"Processing\"):\n",
    "            try:\n",
    "                # Load audio\n",
    "                audio, sr = librosa.load(file_path, sr=self.sample_rate, \n",
    "                                        duration=self.duration, mono=True)\n",
    "                \n",
    "                # Extract features\n",
    "                if matrix_type is None:  # Original features\n",
    "                    features = self.extract_features(audio, sr)\n",
    "                else:  # Compressed features\n",
    "                    features = self.compress_audio(audio, compression_ratio, matrix_type)\n",
    "                \n",
    "                features_list.append(features)\n",
    "                labels_list.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.failed_files.append((file_path, str(e)))\n",
    "                # Use zero features as fallback\n",
    "                if matrix_type is None:\n",
    "                    features_list.append(np.zeros(13))\n",
    "                else:\n",
    "                    features_list.append(np.zeros(50 + 4))  # 50 compressed + 4 stats\n",
    "                labels_list.append(label)\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        print(f\"Extracted features: {X.shape}\")\n",
    "        return X, y\n",
    "    \n",
    "    def run_stratified_experiment(self):\n",
    "        \"\"\"Run experiment with proper stratified sampling\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"RUNNING STRATIFIED COMPRESSION EXPERIMENT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load balanced dataset\n",
    "        file_paths, labels = self.load_dataset(200)\n",
    "        \n",
    "        if len(file_paths) < 20:\n",
    "            print(\"Warning: Small dataset, results may vary\")\n",
    "        \n",
    "        # Get unique classes\n",
    "        unique_classes = np.unique(labels)\n",
    "        print(f\"Classes: {unique_classes}\")\n",
    "        print(f\"Total files: {len(file_paths)}\")\n",
    "        \n",
    "        # Experiment configurations\n",
    "        experiments = [\n",
    "            {'name': 'original', 'matrix_type': None, 'compression_ratio': None},\n",
    "            {'name': 'bernoulli_50', 'matrix_type': 'bernoulli', 'compression_ratio': 0.5},\n",
    "            {'name': 'bernoulli_60', 'matrix_type': 'bernoulli', 'compression_ratio': 0.6},\n",
    "            {'name': 'bernoulli_70', 'matrix_type': 'bernoulli', 'compression_ratio': 0.7},\n",
    "            {'name': 'gaussian_50', 'matrix_type': 'gaussian', 'compression_ratio': 0.5},\n",
    "            {'name': 'gaussian_60', 'matrix_type': 'gaussian', 'compression_ratio': 0.6},\n",
    "            {'name': 'gaussian_70', 'matrix_type': 'gaussian', 'compression_ratio': 0.7},\n",
    "            {'name': 'logistic_henon_50', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.5},\n",
    "            {'name': 'logistic_henon_60', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.6},\n",
    "            {'name': 'logistic_henon_70', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.7},\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for exp in experiments:\n",
    "            print(f\"\\nðŸ”¬ Experiment: {exp['name']}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract features\n",
    "                X, y = self.prepare_all_features(\n",
    "                    file_paths, labels,\n",
    "                    compression_ratio=exp['compression_ratio'],\n",
    "                    matrix_type=exp['matrix_type']\n",
    "                )\n",
    "                \n",
    "                # Encode labels once for this experiment\n",
    "                le = LabelEncoder()\n",
    "                y_encoded = le.fit_transform(y)\n",
    "                \n",
    "                # Stratified split\n",
    "                if len(np.unique(y_encoded)) > 1:\n",
    "                    # Use multiple splits for robustness\n",
    "                    accuracies = []\n",
    "                    feature_dims = []\n",
    "                    \n",
    "                    for split_seed in range(3):  # 3 different splits\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X, y_encoded, \n",
    "                            test_size=0.2,\n",
    "                            stratify=y_encoded,\n",
    "                            random_state=split_seed\n",
    "                        )\n",
    "                        \n",
    "                        # Scale features\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train_scaled = scaler.fit_transform(X_train)\n",
    "                        X_test_scaled = scaler.transform(X_test)\n",
    "                        \n",
    "                        # Train Random Forest\n",
    "                        clf = RandomForestClassifier(\n",
    "                            n_estimators=100,\n",
    "                            random_state=self.random_seed,\n",
    "                            n_jobs=-1\n",
    "                        )\n",
    "                        clf.fit(X_train_scaled, y_train)\n",
    "                        \n",
    "                        # Predict and evaluate\n",
    "                        y_pred = clf.predict(X_test_scaled)\n",
    "                        accuracy = accuracy_score(y_test, y_pred)\n",
    "                        \n",
    "                        accuracies.append(accuracy)\n",
    "                        feature_dims.append(X.shape[1])\n",
    "                    \n",
    "                    # Average results\n",
    "                    avg_accuracy = np.mean(accuracies)\n",
    "                    avg_features = np.mean(feature_dims)\n",
    "                    \n",
    "                    result = {\n",
    "                        'experiment': exp['name'],\n",
    "                        'matrix_type': exp['matrix_type'],\n",
    "                        'compression_ratio': exp['compression_ratio'],\n",
    "                        'accuracy': avg_accuracy,\n",
    "                        'accuracy_std': np.std(accuracies),\n",
    "                        'features': avg_features,\n",
    "                        'samples': len(X)\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"  âœ“ Accuracy: {avg_accuracy:.4f} (Â±{np.std(accuracies):.4f})\")\n",
    "                    print(f\"  âœ“ Features: {avg_features:.0f}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"  âœ— Only one class in dataset\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Save results\n",
    "        if results:\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "            \n",
    "            results_file = 'results/compression_results_final.csv'\n",
    "            results_df.to_csv(results_file, index=False)\n",
    "            \n",
    "            print(f\"\\nâœ… Results saved to: {results_file}\")\n",
    "            \n",
    "            # Create visualizations\n",
    "            self.create_visualizations(results_df)\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary(results_df)\n",
    "            \n",
    "            return results_df\n",
    "        else:\n",
    "            print(\"\\nâŒ No valid results obtained\")\n",
    "            return None\n",
    "    \n",
    "    def create_visualizations(self, results_df):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        print(\"\\nðŸ“Š Creating visualizations...\")\n",
    "        \n",
    "        # Filter out original for compressed comparisons\n",
    "        compressed_df = results_df[results_df['matrix_type'].notnull()].copy()\n",
    "        original_acc = results_df[results_df['experiment'] == 'original']['accuracy'].values\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # Plot 1: All experiments comparison\n",
    "        ax = axes[0, 0]\n",
    "        x_pos = np.arange(len(results_df))\n",
    "        bars = ax.bar(x_pos, results_df['accuracy'], \n",
    "                     yerr=results_df.get('accuracy_std', 0),\n",
    "                     alpha=0.7, capsize=5)\n",
    "        \n",
    "        # Color by matrix type\n",
    "        colors = {'bernoulli': 'blue', 'gaussian': 'green', \n",
    "                 'logistic_henon': 'red', None: 'black'}\n",
    "        for bar, exp in zip(bars, results_df['experiment']):\n",
    "            for matrix, color in colors.items():\n",
    "                if matrix and exp.startswith(matrix):\n",
    "                    bar.set_color(color)\n",
    "                    break\n",
    "            else:\n",
    "                bar.set_color('black')\n",
    "        \n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(results_df['experiment'], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('All Experiments Comparison')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Matrix type comparison\n",
    "        ax = axes[0, 1]\n",
    "        if len(compressed_df) > 0:\n",
    "            matrix_avg = compressed_df.groupby('matrix_type')['accuracy'].mean()\n",
    "            matrix_std = compressed_df.groupby('matrix_type')['accuracy'].std()\n",
    "            \n",
    "            x_pos = np.arange(len(matrix_avg))\n",
    "            bars = ax.bar(x_pos, matrix_avg, yerr=matrix_std,\n",
    "                         alpha=0.7, capsize=5,\n",
    "                         color=['blue', 'green', 'red'])\n",
    "            \n",
    "            if len(original_acc) > 0:\n",
    "                ax.axhline(y=original_acc[0], color='black', linestyle='--', \n",
    "                          label='Original', linewidth=2)\n",
    "            \n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(matrix_avg.index)\n",
    "            ax.set_ylabel('Average Accuracy')\n",
    "            ax.set_title('Matrix Type Comparison')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Compression ratio effect\n",
    "        ax = axes[0, 2]\n",
    "        if len(compressed_df) > 0:\n",
    "            for matrix_type in compressed_df['matrix_type'].unique():\n",
    "                matrix_data = compressed_df[compressed_df['matrix_type'] == matrix_type]\n",
    "                ax.plot(matrix_data['compression_ratio'], matrix_data['accuracy'],\n",
    "                       marker='o', linewidth=2, label=matrix_type,\n",
    "                       color=colors[matrix_type])\n",
    "            \n",
    "            if len(original_acc) > 0:\n",
    "                ax.axhline(y=original_acc[0], color='black', linestyle='--',\n",
    "                          label='Original', linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Compression Ratio')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Effect of Compression Ratio')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Feature dimension vs accuracy\n",
    "        ax = axes[1, 0]\n",
    "        scatter = ax.scatter(results_df['features'], results_df['accuracy'],\n",
    "                           c=results_df['compression_ratio'].fillna(1.0),\n",
    "                           s=100, alpha=0.7, cmap='viridis')\n",
    "        \n",
    "        # Add labels for compressed experiments\n",
    "        for _, row in compressed_df.iterrows():\n",
    "            ax.annotate(f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\",\n",
    "                       (row['features'], row['accuracy']),\n",
    "                       fontsize=8, ha='center')\n",
    "        \n",
    "        ax.set_xlabel('Feature Dimension')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Feature Dimension vs Accuracy')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax, label='Compression Ratio')\n",
    "        \n",
    "        # Plot 5: Best configurations\n",
    "        ax = axes[1, 1]\n",
    "        if len(compressed_df) > 0:\n",
    "            # Get top 5 compressed configurations\n",
    "            top5 = compressed_df.nlargest(5, 'accuracy')\n",
    "            \n",
    "            bars = ax.bar(range(len(top5)), top5['accuracy'],\n",
    "                         yerr=top5.get('accuracy_std', 0),\n",
    "                         alpha=0.7, capsize=5)\n",
    "            \n",
    "            # Color bars\n",
    "            for i, (_, row) in enumerate(top5.iterrows()):\n",
    "                bars[i].set_color(colors[row['matrix_type']])\n",
    "            \n",
    "            ax.set_xticks(range(len(top5)))\n",
    "            ax.set_xticklabels([f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\"\n",
    "                              for _, row in top5.iterrows()], rotation=45, ha='right')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Top 5 Compressed Configurations')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, bar in enumerate(bars):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{bar.get_height():.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 6: Speed vs Accuracy trade-off (estimated)\n",
    "        ax = axes[1, 2]\n",
    "        if len(results_df) > 0:\n",
    "            # Estimate computational complexity\n",
    "            # Original features are fastest, compressed take longer\n",
    "            estimated_time = []\n",
    "            for _, row in results_df.iterrows():\n",
    "                if row['experiment'] == 'original':\n",
    "                    estimated_time.append(1.0)  # baseline\n",
    "                else:\n",
    "                    # More features = longer time\n",
    "                    time_factor = row['features'] / 13  # relative to original\n",
    "                    # Compressed sensing adds overhead\n",
    "                    if row['matrix_type'] == 'logistic_henon':\n",
    "                        time_factor *= 1.5  # chaotic maps are slower\n",
    "                    estimated_time.append(time_factor)\n",
    "            \n",
    "            results_df['estimated_time'] = estimated_time\n",
    "            \n",
    "            scatter = ax.scatter(results_df['estimated_time'], results_df['accuracy'],\n",
    "                               c=results_df['compression_ratio'].fillna(1.0),\n",
    "                               s=100, alpha=0.7, cmap='viridis')\n",
    "            \n",
    "            ax.set_xlabel('Estimated Relative Time')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Speed vs Accuracy Trade-off')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/comprehensive_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Visualizations saved to: visualizations/comprehensive_results.png\")\n",
    "    \n",
    "    def print_summary(self, results_df):\n",
    "        \"\"\"Print comprehensive summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Original performance\n",
    "        original_row = results_df[results_df['experiment'] == 'original']\n",
    "        if len(original_row) > 0:\n",
    "            print(f\"\\nðŸ“ˆ ORIGINAL FEATURES:\")\n",
    "            print(f\"  Accuracy: {original_row['accuracy'].values[0]:.4f}\")\n",
    "            print(f\"  Features: {original_row['features'].values[0]:.0f}\")\n",
    "        \n",
    "        # Best compressed\n",
    "        compressed_df = results_df[results_df['matrix_type'].notnull()]\n",
    "        if len(compressed_df) > 0:\n",
    "            best_compressed = compressed_df.loc[compressed_df['accuracy'].idxmax()]\n",
    "            \n",
    "            print(f\"\\nðŸ† BEST COMPRESSED CONFIGURATION:\")\n",
    "            print(f\"  Matrix: {best_compressed['matrix_type'].capitalize()}\")\n",
    "            print(f\"  Compression: {best_compressed['compression_ratio']*100:.0f}%\")\n",
    "            print(f\"  Accuracy: {best_compressed['accuracy']:.4f}\")\n",
    "            print(f\"  Features: {best_compressed['features']:.0f}\")\n",
    "            \n",
    "            if len(original_row) > 0:\n",
    "                orig_acc = original_row['accuracy'].values[0]\n",
    "                comp_acc = best_compressed['accuracy']\n",
    "                accuracy_drop = orig_acc - comp_acc\n",
    "                feature_reduction = 1 - (best_compressed['features'] / original_row['features'].values[0])\n",
    "                \n",
    "                print(f\"  Accuracy Drop: {accuracy_drop:.4f}\")\n",
    "                print(f\"  Feature Reduction: {feature_reduction*100:.1f}%\")\n",
    "        \n",
    "        # Best by matrix type\n",
    "        print(f\"\\nðŸ“Š BEST BY MATRIX TYPE:\")\n",
    "        for matrix_type in compressed_df['matrix_type'].unique():\n",
    "            matrix_data = compressed_df[compressed_df['matrix_type'] == matrix_type]\n",
    "            best_matrix = matrix_data.loc[matrix_data['accuracy'].idxmax()]\n",
    "            print(f\"  {matrix_type.capitalize()}: {best_matrix['compression_ratio']*100:.0f}% \"\n",
    "                  f\"(Acc: {best_matrix['accuracy']:.4f})\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "        if len(compressed_df) > 0:\n",
    "            # Find configurations with < 10% accuracy drop\n",
    "            if len(original_row) > 0:\n",
    "                orig_acc = original_row['accuracy'].values[0]\n",
    "                good_configs = compressed_df[compressed_df['accuracy'] >= orig_acc * 0.9]\n",
    "                \n",
    "                if len(good_configs) > 0:\n",
    "                    print(\"  Configurations with <10% accuracy drop:\")\n",
    "                    for _, row in good_configs.nlargest(3, 'accuracy').iterrows():\n",
    "                        print(f\"    â€¢ {row['matrix_type']} {row['compression_ratio']*100:.0f}%: \"\n",
    "                              f\"Acc={row['accuracy']:.4f}, Features={row['features']:.0f}\")\n",
    "                else:\n",
    "                    print(\"  No configurations with <10% accuracy drop found.\")\n",
    "            \n",
    "            # Fastest configuration\n",
    "            fastest = compressed_df.loc[compressed_df['features'].idxmin()]\n",
    "            print(f\"  Fastest (fewest features): {fastest['matrix_type']} \"\n",
    "                  f\"{fastest['compression_ratio']*100:.0f}%\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ Output files:\")\n",
    "        print(f\"  - results/compression_results_final.csv\")\n",
    "        print(f\"  - visualizations/comprehensive_results.png\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    print(\"ðŸŽ¯ COMPRESSED SENSING AUDIO CLASSIFICATION EXPERIMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize framework\n",
    "    framework = FixedCompressionFramework(\n",
    "        wavelet='db4',\n",
    "        dwt_level=3,\n",
    "        sample_rate=22050,\n",
    "        duration=3.0,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸš€ Starting experiment...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the complete experiment\n",
    "    results = framework.run_stratified_experiment()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"âœ… EXPERIMENT COMPLETED IN {total_time/60:.1f} MINUTES!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show quick results\n",
    "        print(\"\\nðŸŽ¯ QUICK RESULTS:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        original_acc = results[results['experiment'] == 'original']['accuracy'].values\n",
    "        if len(original_acc) > 0:\n",
    "            print(f\"Original: {original_acc[0]:.4f}\")\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            best_compressed = results[results['experiment'] != 'original'].nlargest(1, 'accuracy')\n",
    "            if len(best_compressed) > 0:\n",
    "                row = best_compressed.iloc[0]\n",
    "                print(f\"Best Compressed: {row['experiment']} = {row['accuracy']:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nâŒ Experiment failed to produce results\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ‰ All done! Check the 'results' and 'visualizations' folders.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc15f508-e227-4070-bb7d-7ec47d4f28b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' from 'C:\\Users\\ch.sc.u4cse23160\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m     13\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\utils\\fixes.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     22\u001b[39m     pd = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py:151\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    134\u001b[39m     concat,\n\u001b[32m    135\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     qcut,\n\u001b[32m    148\u001b[39m )\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# excel\u001b[39;00m\n\u001b[32m    156\u001b[39m     ExcelFile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m     read_spss,\n\u001b[32m    185\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\testing.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mPublic testing utility functions.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_testing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     assert_extension_array_equal,\n\u001b[32m      8\u001b[39m     assert_frame_equal,\n\u001b[32m      9\u001b[39m     assert_index_equal,\n\u001b[32m     10\u001b[39m     assert_series_equal,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_extension_array_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_frame_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_series_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert_index_equal\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\_testing\\__init__.py:405\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytest\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytest.raises(expected_exception, match=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m cython_table = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m.common._cython_table.items()\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cython_table_params\u001b[39m(ndframe, func_names_and_expected):\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    Combine frame, functions from com._cython_table\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    keys and expected result.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    423\u001b[39m \u001b[33;03m        List of three items (DataFrame, function, expected result)\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'pandas' from 'C:\\Users\\ch.sc.u4cse23160\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\__init__.py' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import signal\n",
    "import sys\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class CheckpointCompressionFramework:\n",
    "    \"\"\"Compression framework with checkpointing and resume capability\"\"\"\n",
    "    \n",
    "    def __init__(self, wavelet='db4', dwt_level=3, sample_rate=22050, duration=3.0, random_seed=42):\n",
    "        self.wavelet = wavelet\n",
    "        self.dwt_level = dwt_level\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # Setup checkpoint directory\n",
    "        self.checkpoint_dir = 'checkpoints'\n",
    "        self.results_dir = 'results'\n",
    "        self.vis_dir = 'visualizations'\n",
    "        \n",
    "        self.create_directories()\n",
    "        \n",
    "        # State tracking\n",
    "        self.state_file = os.path.join(self.checkpoint_dir, 'experiment_state.pkl')\n",
    "        self.current_experiment = None\n",
    "        self.completed_experiments = set()\n",
    "        self.results = []\n",
    "        self.failed_files = []\n",
    "        \n",
    "        # Load previous state if exists\n",
    "        self.load_state()\n",
    "        \n",
    "        # Setup interrupt handler\n",
    "        self.setup_interrupt_handler()\n",
    "        \n",
    "        print(f\"ðŸ“Š Loaded state: {len(self.completed_experiments)} experiments completed\")\n",
    "    \n",
    "    def setup_interrupt_handler(self):\n",
    "        \"\"\"Setup handler for keyboard interrupts\"\"\"\n",
    "        def signal_handler(sig, frame):\n",
    "            print(\"\\n\\nâš ï¸  Keyboard interrupt detected!\")\n",
    "            print(\"ðŸ’¾ Saving checkpoint before exit...\")\n",
    "            self.save_state()\n",
    "            print(\"âœ… Checkpoint saved. You can resume later.\")\n",
    "            sys.exit(0)\n",
    "        \n",
    "        signal.signal(signal.SIGINT, signal_handler)\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        directories = [self.checkpoint_dir, self.results_dir, self.vis_dir]\n",
    "        for dir_name in directories:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    def save_state(self):\n",
    "        \"\"\"Save current experiment state\"\"\"\n",
    "        state = {\n",
    "            'current_experiment': self.current_experiment,\n",
    "            'completed_experiments': list(self.completed_experiments),\n",
    "            'results': self.results,\n",
    "            'failed_files': self.failed_files,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(self.state_file, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        \n",
    "        # Also save results incrementally\n",
    "        if self.results:\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "            results_df.to_csv(os.path.join(self.results_dir, 'partial_results.csv'), index=False)\n",
    "        \n",
    "        print(f\"ðŸ’¾ Checkpoint saved: {len(self.completed_experiments)} experiments completed\")\n",
    "    \n",
    "    def load_state(self):\n",
    "        \"\"\"Load previous experiment state\"\"\"\n",
    "        if os.path.exists(self.state_file):\n",
    "            try:\n",
    "                with open(self.state_file, 'rb') as f:\n",
    "                    state = pickle.load(f)\n",
    "                \n",
    "                self.current_experiment = state.get('current_experiment')\n",
    "                self.completed_experiments = set(state.get('completed_experiments', []))\n",
    "                self.results = state.get('results', [])\n",
    "                self.failed_files = state.get('failed_files', [])\n",
    "                \n",
    "                print(f\"âœ… Loaded checkpoint from {state.get('timestamp', 'unknown time')}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Could not load checkpoint: {e}\")\n",
    "                return False\n",
    "        return False\n",
    "    \n",
    "    def clear_state(self):\n",
    "        \"\"\"Clear checkpoint state (start fresh)\"\"\"\n",
    "        if os.path.exists(self.state_file):\n",
    "            os.remove(self.state_file)\n",
    "        self.completed_experiments = set()\n",
    "        self.results = []\n",
    "        self.current_experiment = None\n",
    "        print(\"ðŸ§¹ Checkpoint cleared\")\n",
    "    \n",
    "    def load_dataset(self, n_files=200):\n",
    "        \"\"\"Load dataset with checkpointing\"\"\"\n",
    "        dataset_file = os.path.join(self.checkpoint_dir, 'dataset.pkl')\n",
    "        \n",
    "        # Try to load cached dataset\n",
    "        if os.path.exists(dataset_file):\n",
    "            print(\"ðŸ“‚ Loading cached dataset...\")\n",
    "            with open(dataset_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                return data['file_paths'], data['labels']\n",
    "        \n",
    "        print(\"ðŸ“‚ Loading fresh dataset...\")\n",
    "        base_path = \"UrbanSound8K/\"\n",
    "        metadata_path = os.path.join(base_path, \"metadata/UrbanSound8K.csv\")\n",
    "        \n",
    "        if os.path.exists(metadata_path):\n",
    "            metadata = pd.read_csv(metadata_path)\n",
    "            \n",
    "            file_paths = []\n",
    "            labels = []\n",
    "            \n",
    "            # Get unique classes\n",
    "            unique_classes = metadata['class'].unique()\n",
    "            \n",
    "            # Sample files per class\n",
    "            files_per_class = max(1, n_files // len(unique_classes))\n",
    "            \n",
    "            for class_name in unique_classes:\n",
    "                class_files = metadata[metadata['class'] == class_name]\n",
    "                sample_size = min(files_per_class, len(class_files))\n",
    "                \n",
    "                if sample_size > 0:\n",
    "                    sampled_files = class_files.sample(sample_size, random_state=self.random_seed)\n",
    "                    \n",
    "                    for _, row in sampled_files.iterrows():\n",
    "                        fold = row['fold']\n",
    "                        filename = row['slice_file_name']\n",
    "                        file_path = os.path.join(base_path, f\"audio/fold{fold}\", filename)\n",
    "                        file_path = file_path.replace('\\\\', '/')\n",
    "                        \n",
    "                        if os.path.exists(file_path):\n",
    "                            file_paths.append(file_path)\n",
    "                            labels.append(class_name)\n",
    "            \n",
    "            # Cache dataset\n",
    "            dataset_data = {\n",
    "                'file_paths': file_paths,\n",
    "                'labels': labels,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            with open(dataset_file, 'wb') as f:\n",
    "                pickle.dump(dataset_data, f)\n",
    "            \n",
    "            print(f\"âœ… Loaded {len(file_paths)} files, {len(set(labels))} classes\")\n",
    "            return file_paths, labels\n",
    "        \n",
    "        else:\n",
    "            print(\"âŒ UrbanSound8K not found\")\n",
    "            return [], []\n",
    "    \n",
    "    def extract_features(self, audio, sr, feature_cache=None):\n",
    "        \"\"\"Extract features with caching\"\"\"\n",
    "        if feature_cache is None:\n",
    "            feature_cache = {}\n",
    "        \n",
    "        # Simple hash for caching\n",
    "        audio_hash = hash(audio.tobytes())\n",
    "        \n",
    "        if audio_hash in feature_cache:\n",
    "            return feature_cache[audio_hash]\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Basic statistics\n",
    "        features.append(np.mean(audio))\n",
    "        features.append(np.std(audio))\n",
    "        features.append(np.max(np.abs(audio)))\n",
    "        \n",
    "        # Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio, frame_length=2048, hop_length=512)\n",
    "        features.append(np.mean(zcr))\n",
    "        \n",
    "        # Energy\n",
    "        features.append(np.sum(audio ** 2))\n",
    "        \n",
    "        # Simple spectral features\n",
    "        if len(audio) > 10 and np.any(audio != 0):\n",
    "            try:\n",
    "                fft = np.fft.fft(audio)\n",
    "                magnitude = np.abs(fft)\n",
    "                half_len = len(magnitude) // 2\n",
    "                \n",
    "                if half_len > 0:\n",
    "                    pos_mag = magnitude[:half_len]\n",
    "                    features.append(np.mean(pos_mag))\n",
    "                    features.append(np.std(pos_mag))\n",
    "                    \n",
    "                    # Spectral centroid approximation\n",
    "                    if np.sum(pos_mag) > 0:\n",
    "                        freqs = np.fft.fftfreq(len(audio), 1/sr)[:half_len]\n",
    "                        centroid = np.sum(freqs * pos_mag) / np.sum(pos_mag)\n",
    "                        features.append(centroid)\n",
    "                    else:\n",
    "                        features.append(0.0)\n",
    "                else:\n",
    "                    features.extend([0.0, 0.0, 0.0])\n",
    "            except:\n",
    "                features.extend([0.0, 0.0, 0.0])\n",
    "        else:\n",
    "            features.extend([0.0, 0.0, 0.0])\n",
    "        \n",
    "        # Ensure fixed size\n",
    "        target_size = 8\n",
    "        if len(features) < target_size:\n",
    "            features.extend([0.0] * (target_size - len(features)))\n",
    "        \n",
    "        result = np.array(features[:target_size])\n",
    "        feature_cache[audio_hash] = result\n",
    "        return result\n",
    "    \n",
    "    def compress_audio(self, audio, compression_ratio=0.5, matrix_type='bernoulli', matrix_cache=None):\n",
    "        \"\"\"Compress audio with matrix caching\"\"\"\n",
    "        if matrix_cache is None:\n",
    "            matrix_cache = {}\n",
    "        \n",
    "        # Apply DWT\n",
    "        try:\n",
    "            coeffs = pywt.wavedec(audio, self.wavelet, level=self.dwt_level)\n",
    "            coeffs_flat = np.concatenate(coeffs)\n",
    "        except:\n",
    "            coeffs_flat = audio[:min(len(audio), 1000)]\n",
    "        \n",
    "        N = len(coeffs_flat)\n",
    "        M = max(10, int(N * compression_ratio))\n",
    "        \n",
    "        # Generate or load matrix\n",
    "        matrix_key = f\"{matrix_type}_{M}_{N}_{self.random_seed}\"\n",
    "        if matrix_key in matrix_cache:\n",
    "            Phi = matrix_cache[matrix_key]\n",
    "        else:\n",
    "            np.random.seed(self.random_seed)\n",
    "            \n",
    "            if matrix_type == 'bernoulli':\n",
    "                Phi = np.random.choice([-1, 1], size=(M, N)) / np.sqrt(M)\n",
    "            elif matrix_type == 'gaussian':\n",
    "                Phi = np.random.randn(M, N) / np.sqrt(M)\n",
    "            else:  # logistic_henon\n",
    "                r = 3.99\n",
    "                a, b = 1.4, 0.3\n",
    "                x_log = np.random.rand()\n",
    "                x_hen, y_hen = np.random.rand(), np.random.rand()\n",
    "                \n",
    "                Phi = np.zeros((M, N))\n",
    "                for i in range(M):\n",
    "                    for j in range(N):\n",
    "                        x_log = r * x_log * (1 - x_log)\n",
    "                        x_hen_new = 1 - a * x_hen**2 + y_hen\n",
    "                        y_hen = b * x_hen\n",
    "                        x_hen = x_hen_new\n",
    "                        \n",
    "                        chaotic = (x_log + x_hen) / 2\n",
    "                        Phi[i, j] = 2 * chaotic - 1\n",
    "                \n",
    "                Phi = Phi / np.sqrt(M)\n",
    "            \n",
    "            matrix_cache[matrix_key] = Phi\n",
    "        \n",
    "        # Compress\n",
    "        compressed = Phi @ coeffs_flat\n",
    "        \n",
    "        # Add statistics\n",
    "        features = np.concatenate([\n",
    "            compressed,\n",
    "            [\n",
    "                np.mean(compressed),\n",
    "                np.std(compressed),\n",
    "                np.max(np.abs(compressed)),\n",
    "                M / N\n",
    "            ]\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_features_with_checkpoint(self, file_paths, labels, experiment_name, \n",
    "                                       compression_ratio=None, matrix_type=None):\n",
    "        \"\"\"Prepare features with checkpointing\"\"\"\n",
    "        features_file = os.path.join(self.checkpoint_dir, f'features_{experiment_name}.npy')\n",
    "        labels_file = os.path.join(self.checkpoint_dir, f'labels_{experiment_name}.npy')\n",
    "        \n",
    "        # Check if features already exist\n",
    "        if os.path.exists(features_file) and os.path.exists(labels_file):\n",
    "            print(f\"  Loading cached features for {experiment_name}...\")\n",
    "            X = np.load(features_file)\n",
    "            y = np.load(labels_file)\n",
    "            return X, y\n",
    "        \n",
    "        print(f\"  Extracting features for {experiment_name}...\")\n",
    "        \n",
    "        feature_cache = {}\n",
    "        matrix_cache = {}\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        progress_file = os.path.join(self.checkpoint_dir, f'progress_{experiment_name}.pkl')\n",
    "        \n",
    "        # Load progress if exists\n",
    "        if os.path.exists(progress_file):\n",
    "            with open(progress_file, 'rb') as f:\n",
    "                progress = pickle.load(f)\n",
    "                start_idx = progress['last_index'] + 1\n",
    "                features_list = progress['features_list']\n",
    "                labels_list = progress['labels_list']\n",
    "                feature_cache = progress.get('feature_cache', {})\n",
    "                matrix_cache = progress.get('matrix_cache', {})\n",
    "            print(f\"  Resuming from file {start_idx}/{len(file_paths)}\")\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        \n",
    "        # Process files\n",
    "        for i in tqdm(range(start_idx, len(file_paths)), desc=f\"Processing {experiment_name}\"):\n",
    "            file_path = file_paths[i]\n",
    "            label = labels[i]\n",
    "            \n",
    "            try:\n",
    "                # Load audio\n",
    "                audio, sr = librosa.load(file_path, sr=self.sample_rate, \n",
    "                                       duration=self.duration, mono=True)\n",
    "                \n",
    "                # Extract features\n",
    "                if matrix_type is None:  # Original features\n",
    "                    features = self.extract_features(audio, sr, feature_cache)\n",
    "                else:  # Compressed features\n",
    "                    features = self.compress_audio(audio, compression_ratio, matrix_type, matrix_cache)\n",
    "                \n",
    "                features_list.append(features)\n",
    "                labels_list.append(label)\n",
    "                \n",
    "                # Save progress every 10 files\n",
    "                if i % 10 == 0:\n",
    "                    progress = {\n",
    "                        'last_index': i,\n",
    "                        'features_list': features_list,\n",
    "                        'labels_list': labels_list,\n",
    "                        'feature_cache': feature_cache,\n",
    "                        'matrix_cache': matrix_cache\n",
    "                    }\n",
    "                    with open(progress_file, 'wb') as f:\n",
    "                        pickle.dump(progress, f)\n",
    "                    \n",
    "                    # Also save state\n",
    "                    self.save_state()\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.failed_files.append((file_path, str(e)))\n",
    "                # Use zero features as fallback\n",
    "                if matrix_type is None:\n",
    "                    features_list.append(np.zeros(8))\n",
    "                else:\n",
    "                    features_list.append(np.zeros(50 + 4))\n",
    "                labels_list.append(label)\n",
    "                continue\n",
    "        \n",
    "        # Convert to arrays\n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        # Save features\n",
    "        np.save(features_file, X)\n",
    "        np.save(labels_file, y)\n",
    "        \n",
    "        # Clean up progress file\n",
    "        if os.path.exists(progress_file):\n",
    "            os.remove(progress_file)\n",
    "        \n",
    "        print(f\"  Extracted features: {X.shape}\")\n",
    "        return X, y\n",
    "    \n",
    "    def run_experiment_with_checkpoint(self, exp_config, file_paths, labels):\n",
    "        \"\"\"Run a single experiment with checkpointing\"\"\"\n",
    "        exp_name = exp_config['name']\n",
    "        \n",
    "        # Skip if already completed\n",
    "        if exp_name in self.completed_experiments:\n",
    "            print(f\"â­ï¸  Skipping {exp_name} (already completed)\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ”¬ Experiment: {exp_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        self.current_experiment = exp_name\n",
    "        \n",
    "        try:\n",
    "            # Prepare features\n",
    "            X, y = self.prepare_features_with_checkpoint(\n",
    "                file_paths, labels,\n",
    "                exp_name,\n",
    "                compression_ratio=exp_config.get('compression_ratio'),\n",
    "                matrix_type=exp_config.get('matrix_type')\n",
    "            )\n",
    "            \n",
    "            if len(X) < 10:\n",
    "                print(f\"  âœ— Not enough samples: {len(X)}\")\n",
    "                return None\n",
    "            \n",
    "            # Encode labels\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "            \n",
    "            # Run multiple stratified splits\n",
    "            accuracies = []\n",
    "            feature_dims = []\n",
    "            \n",
    "            for split_seed in range(3):  # 3 different splits\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y_encoded,\n",
    "                    test_size=0.2,\n",
    "                    stratify=y_encoded,\n",
    "                    random_state=split_seed\n",
    "                )\n",
    "                \n",
    "                # Scale features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Train classifier\n",
    "                clf = RandomForestClassifier(\n",
    "                    n_estimators=100,\n",
    "                    random_state=self.random_seed,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                clf.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                # Predict and evaluate\n",
    "                y_pred = clf.predict(X_test_scaled)\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                \n",
    "                accuracies.append(accuracy)\n",
    "                feature_dims.append(X.shape[1])\n",
    "                \n",
    "                # Save partial results\n",
    "                partial_result = {\n",
    "                    'experiment': exp_name,\n",
    "                    'matrix_type': exp_config.get('matrix_type'),\n",
    "                    'compression_ratio': exp_config.get('compression_ratio'),\n",
    "                    'accuracy': accuracy,\n",
    "                    'features': X.shape[1],\n",
    "                    'split': split_seed,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                self.results.append(partial_result)\n",
    "                self.save_state()\n",
    "            \n",
    "            # Calculate average results\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            avg_features = np.mean(feature_dims)\n",
    "            \n",
    "            # Create final result entry\n",
    "            result = {\n",
    "                'experiment': exp_name,\n",
    "                'matrix_type': exp_config.get('matrix_type'),\n",
    "                'compression_ratio': exp_config.get('compression_ratio'),\n",
    "                'accuracy': avg_accuracy,\n",
    "                'accuracy_std': std_accuracy,\n",
    "                'features': avg_features,\n",
    "                'samples': len(X)\n",
    "            }\n",
    "            \n",
    "            # Mark as completed\n",
    "            self.completed_experiments.add(exp_name)\n",
    "            self.current_experiment = None\n",
    "            \n",
    "            print(f\"  âœ… Accuracy: {avg_accuracy:.4f} (Â±{std_accuracy:.4f})\")\n",
    "            print(f\"  âœ… Features: {avg_features:.0f}\")\n",
    "            print(f\"  âœ… Samples: {len(X)}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error in {exp_name}: {str(e)[:100]}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        \"\"\"Run all experiments with checkpointing\"\"\"\n",
    "        print(\"ðŸš€ STARTING CHECKPOINTED COMPRESSION EXPERIMENTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load dataset\n",
    "        file_paths, labels = self.load_dataset(200)\n",
    "        \n",
    "        if len(file_paths) < 20:\n",
    "            print(\"âŒ Not enough files to run experiments\")\n",
    "            return None\n",
    "        \n",
    "        # Define all experiments\n",
    "        experiments = [\n",
    "            {'name': 'original', 'matrix_type': None, 'compression_ratio': None},\n",
    "            {'name': 'bernoulli_50', 'matrix_type': 'bernoulli', 'compression_ratio': 0.5},\n",
    "            {'name': 'bernoulli_60', 'matrix_type': 'bernoulli', 'compression_ratio': 0.6},\n",
    "            {'name': 'bernoulli_70', 'matrix_type': 'bernoulli', 'compression_ratio': 0.7},\n",
    "            {'name': 'gaussian_50', 'matrix_type': 'gaussian', 'compression_ratio': 0.5},\n",
    "            {'name': 'gaussian_60', 'matrix_type': 'gaussian', 'compression_ratio': 0.6},\n",
    "            {'name': 'gaussian_70', 'matrix_type': 'gaussian', 'compression_ratio': 0.7},\n",
    "            {'name': 'logistic_henon_50', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.5},\n",
    "            {'name': 'logistic_henon_60', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.6},\n",
    "            {'name': 'logistic_henon_70', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.7},\n",
    "        ]\n",
    "        \n",
    "        final_results = []\n",
    "        \n",
    "        # Check for existing final results\n",
    "        results_file = os.path.join(self.results_dir, 'final_results.csv')\n",
    "        if os.path.exists(results_file):\n",
    "            print(\"ðŸ“‚ Loading existing final results...\")\n",
    "            existing_results = pd.read_csv(results_file)\n",
    "            for _, row in existing_results.iterrows():\n",
    "                self.completed_experiments.add(row['experiment'])\n",
    "                final_results.append(row.to_dict())\n",
    "        \n",
    "        # Run remaining experiments\n",
    "        remaining = [exp for exp in experiments if exp['name'] not in self.completed_experiments]\n",
    "        \n",
    "        if remaining:\n",
    "            print(f\"ðŸ“‹ {len(remaining)} experiments remaining\")\n",
    "            print(f\"â­ï¸  {len(self.completed_experiments)} already completed\")\n",
    "            \n",
    "            for exp_config in remaining:\n",
    "                result = self.run_experiment_with_checkpoint(exp_config, file_paths, labels)\n",
    "                if result:\n",
    "                    final_results.append(result)\n",
    "                    # Save intermediate results\n",
    "                    results_df = pd.DataFrame(final_results)\n",
    "                    results_df.to_csv(results_file, index=False)\n",
    "        else:\n",
    "            print(\"âœ… All experiments already completed!\")\n",
    "            if not final_results and os.path.exists(results_file):\n",
    "                final_results = pd.read_csv(results_file).to_dict('records')\n",
    "        \n",
    "        if final_results:\n",
    "            # Save final results\n",
    "            results_df = pd.DataFrame(final_results)\n",
    "            results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "            results_df.to_csv(results_file, index=False)\n",
    "            \n",
    "            # Create visualizations\n",
    "            self.create_visualizations(results_df)\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary(results_df)\n",
    "            \n",
    "            # Clear checkpoint files (optional)\n",
    "            if input(\"\\nðŸ§¹ Clear checkpoint files? (y/n): \").lower() == 'y':\n",
    "                self.clear_state()\n",
    "                # Clear feature cache files\n",
    "                for file in os.listdir(self.checkpoint_dir):\n",
    "                    if file.endswith('.npy') or file.endswith('.pkl'):\n",
    "                        os.remove(os.path.join(self.checkpoint_dir, file))\n",
    "            \n",
    "            return results_df\n",
    "        else:\n",
    "            print(\"âŒ No results obtained\")\n",
    "            return None\n",
    "    \n",
    "    def create_visualizations(self, results_df):\n",
    "        \"\"\"Create visualizations\"\"\"\n",
    "        print(\"\\nðŸŽ¨ Creating visualizations...\")\n",
    "        \n",
    "        # Filter out original for compressed comparisons\n",
    "        compressed_df = results_df[results_df['matrix_type'].notnull()].copy()\n",
    "        original_acc = results_df[results_df['experiment'] == 'original']['accuracy'].values\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: All experiments\n",
    "        ax = axes[0, 0]\n",
    "        x_pos = np.arange(len(results_df))\n",
    "        bars = ax.bar(x_pos, results_df['accuracy'], \n",
    "                     yerr=results_df.get('accuracy_std', 0),\n",
    "                     alpha=0.7, capsize=5)\n",
    "        \n",
    "        colors = {'bernoulli': 'blue', 'gaussian': 'green', \n",
    "                 'logistic_henon': 'red', None: 'black'}\n",
    "        \n",
    "        for bar, exp in zip(bars, results_df['experiment']):\n",
    "            for matrix, color in colors.items():\n",
    "                if matrix and exp.startswith(matrix):\n",
    "                    bar.set_color(color)\n",
    "                    break\n",
    "            else:\n",
    "                bar.set_color('black')\n",
    "        \n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(results_df['experiment'], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('All Experiments Comparison')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Matrix type comparison\n",
    "        ax = axes[0, 1]\n",
    "        if len(compressed_df) > 0:\n",
    "            matrix_avg = compressed_df.groupby('matrix_type')['accuracy'].mean()\n",
    "            matrix_std = compressed_df.groupby('matrix_type')['accuracy'].std()\n",
    "            \n",
    "            x_pos = np.arange(len(matrix_avg))\n",
    "            bars = ax.bar(x_pos, matrix_avg, yerr=matrix_std,\n",
    "                         alpha=0.7, capsize=5,\n",
    "                         color=['blue', 'green', 'red'])\n",
    "            \n",
    "            if len(original_acc) > 0:\n",
    "                ax.axhline(y=original_acc[0], color='black', linestyle='--', \n",
    "                          label='Original', linewidth=2)\n",
    "            \n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(matrix_avg.index)\n",
    "            ax.set_ylabel('Average Accuracy')\n",
    "            ax.set_title('Matrix Type Comparison')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Compression ratio effect\n",
    "        ax = axes[1, 0]\n",
    "        if len(compressed_df) > 0:\n",
    "            for matrix_type in compressed_df['matrix_type'].unique():\n",
    "                matrix_data = compressed_df[compressed_df['matrix_type'] == matrix_type]\n",
    "                ax.plot(matrix_data['compression_ratio'], matrix_data['accuracy'],\n",
    "                       marker='o', linewidth=2, label=matrix_type,\n",
    "                       color=colors[matrix_type])\n",
    "            \n",
    "            if len(original_acc) > 0:\n",
    "                ax.axhline(y=original_acc[0], color='black', linestyle='--',\n",
    "                          label='Original', linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Compression Ratio')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Effect of Compression Ratio')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Best configurations\n",
    "        ax = axes[1, 1]\n",
    "        if len(compressed_df) > 0:\n",
    "            top5 = compressed_df.nlargest(5, 'accuracy')\n",
    "            \n",
    "            bars = ax.bar(range(len(top5)), top5['accuracy'],\n",
    "                         yerr=top5.get('accuracy_std', 0),\n",
    "                         alpha=0.7, capsize=5)\n",
    "            \n",
    "            for i, (_, row) in enumerate(top5.iterrows()):\n",
    "                bars[i].set_color(colors[row['matrix_type']])\n",
    "            \n",
    "            ax.set_xticks(range(len(top5)))\n",
    "            ax.set_xticklabels([f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\"\n",
    "                              for _, row in top5.iterrows()], rotation=45, ha='right')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Top 5 Compressed Configurations')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        vis_file = os.path.join(self.vis_dir, 'compression_results.png')\n",
    "        plt.savefig(vis_file, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"âœ… Visualizations saved to: {vis_file}\")\n",
    "    \n",
    "    def print_summary(self, results_df):\n",
    "        \"\"\"Print summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Original performance\n",
    "        original_row = results_df[results_df['experiment'] == 'original']\n",
    "        if len(original_row) > 0:\n",
    "            print(f\"\\nðŸ“ˆ ORIGINAL FEATURES:\")\n",
    "            print(f\"  Accuracy: {original_row['accuracy'].values[0]:.4f}\")\n",
    "            print(f\"  Features: {original_row['features'].values[0]:.0f}\")\n",
    "        \n",
    "        # Best compressed\n",
    "        compressed_df = results_df[results_df['matrix_type'].notnull()]\n",
    "        if len(compressed_df) > 0:\n",
    "            best_compressed = compressed_df.loc[compressed_df['accuracy'].idxmax()]\n",
    "            \n",
    "            print(f\"\\nðŸ† BEST COMPRESSED CONFIGURATION:\")\n",
    "            print(f\"  Matrix: {best_compressed['matrix_type'].capitalize()}\")\n",
    "            print(f\"  Compression: {best_compressed['compression_ratio']*100:.0f}%\")\n",
    "            print(f\"  Accuracy: {best_compressed['accuracy']:.4f}\")\n",
    "            print(f\"  Features: {best_compressed['features']:.0f}\")\n",
    "            \n",
    "            if len(original_row) > 0:\n",
    "                orig_acc = original_row['accuracy'].values[0]\n",
    "                comp_acc = best_compressed['accuracy']\n",
    "                accuracy_drop = orig_acc - comp_acc\n",
    "                feature_reduction = 1 - (best_compressed['features'] / original_row['features'].values[0])\n",
    "                \n",
    "                print(f\"  Accuracy Drop: {accuracy_drop:.4f}\")\n",
    "                print(f\"  Feature Reduction: {feature_reduction*100:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nðŸ“ Output files:\")\n",
    "        print(f\"  - {self.results_dir}/final_results.csv\")\n",
    "        print(f\"  - {self.vis_dir}/compression_results.png\")\n",
    "        print(f\"  - Checkpoints in: {self.checkpoint_dir}/\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    print(\"ðŸŽ¯ CHECKPOINTED COMPRESSED SENSING EXPERIMENT\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"âš ï¸  Press Ctrl+C at any time to save checkpoint and exit\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if user wants to resume or start fresh\n",
    "    if os.path.exists('checkpoints/experiment_state.pkl'):\n",
    "        choice = input(\"\\nðŸ“‚ Found previous checkpoint. Resume from checkpoint? (y/n): \").lower()\n",
    "        if choice == 'n':\n",
    "            # Clear state and start fresh\n",
    "            framework = CheckpointCompressionFramework()\n",
    "            framework.clear_state()\n",
    "        else:\n",
    "            framework = CheckpointCompressionFramework()\n",
    "    else:\n",
    "        framework = CheckpointCompressionFramework()\n",
    "    \n",
    "    print(\"\\nðŸš€ Starting experiments...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = framework.run_all_experiments()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Fatal error: {e}\")\n",
    "        print(\"ðŸ’¾ Saving checkpoint before exit...\")\n",
    "        framework.save_state()\n",
    "        raise\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"âœ… EXPERIMENT COMPLETED IN {total_time/60:.1f} MINUTES!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show quick results\n",
    "        print(\"\\nðŸŽ¯ QUICK RESULTS:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        original_acc = results[results['experiment'] == 'original']['accuracy'].values\n",
    "        if len(original_acc) > 0:\n",
    "            print(f\"Original: {original_acc[0]:.4f}\")\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            best_compressed = results[results['experiment'] != 'original'].nlargest(1, 'accuracy')\n",
    "            if len(best_compressed) > 0:\n",
    "                row = best_compressed.iloc[0]\n",
    "                print(f\"Best Compressed: {row['experiment']} = {row['accuracy']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ‰ All done! Check the output folders for results.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca27a6a4-2f69-451f-b40b-dbb7eb518f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\ch.sc.u4cse23160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ch.sc.u4cse23160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ch.sc.u4cse23160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ch.sc.u4cse23160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ch.sc.u4cse23160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ch.sc.u4cse23160\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba6098-f96f-4e23-8b33-0253d56d40bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
