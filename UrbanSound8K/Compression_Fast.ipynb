{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1e774-17e2-4823-841b-7814faccc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from scipy import stats\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e7e4d-869b-41f6-a03d-aafd226a2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a994c48-a17e-466d-a152-979dbb0ea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientMeasurementMatrices:\n",
    "    \"\"\"Memory-efficient measurement matrices with block generation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def bernoulli_matrix_block(rows, cols, start_row=0, seed=42):\n",
    "        \"\"\"Generate Bernoulli matrix block without storing full matrix\"\"\"\n",
    "        np.random.seed(seed + start_row)\n",
    "        matrix = np.random.choice([-1, 1], size=(rows, cols))\n",
    "        return matrix / np.sqrt(rows)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian_matrix_block(rows, cols, start_row=0, seed=42):\n",
    "        \"\"\"Generate Gaussian matrix block\"\"\"\n",
    "        np.random.seed(seed + start_row)\n",
    "        matrix = np.random.randn(rows, cols)\n",
    "        return matrix / np.sqrt(rows)\n",
    "    \n",
    "    @staticmethod\n",
    "    def logistic_henon_matrix_block(rows, cols, start_row=0, seed=42):\n",
    "        \"\"\"Generate Logistic-Henon chaotic matrix block\"\"\"\n",
    "        np.random.seed(seed + start_row)\n",
    "        matrix = np.zeros((rows, cols))\n",
    "        \n",
    "        # Initialize chaotic maps\n",
    "        r = 3.99\n",
    "        x = np.random.rand()\n",
    "        a, b = 1.4, 0.3\n",
    "        x_h, y_h = np.random.rand(), np.random.rand()\n",
    "        \n",
    "        # Generate each element\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                # Logistic map\n",
    "                x = r * x * (1 - x)\n",
    "                \n",
    "                # Henon map\n",
    "                x_h_new = 1 - a * x_h**2 + y_h\n",
    "                y_h = b * x_h\n",
    "                x_h = x_h_new\n",
    "                \n",
    "                # Combine\n",
    "                chaotic_value = (x + x_h) / 2\n",
    "                matrix[i, j] = 2 * chaotic_value - 1\n",
    "            \n",
    "            # Reset periodically to avoid correlation\n",
    "            if (start_row + i) % 100 == 0:\n",
    "                x = (x + np.random.rand()) / 2\n",
    "                x_h = (x_h + np.random.rand()) / 2\n",
    "        \n",
    "        return matrix / np.sqrt(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0836b0a-fa20-4b05-9845-d1f71c2e1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCompressionFramework:\n",
    "    \"\"\"Complete framework for audio compression experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=\"UrbanSound8K/\", wavelet='db4', dwt_level=4, \n",
    "                 sample_rate=22050, duration=4.0, random_seed=42):\n",
    "        self.base_path = base_path\n",
    "        self.wavelet = wavelet\n",
    "        self.dwt_level = dwt_level\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.random_seed = random_seed\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "        self.experiment_data = {}\n",
    "        \n",
    "        # Create output directories\n",
    "        self.create_directories()\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        directories = [\n",
    "            'results',\n",
    "            'models', \n",
    "            'features',\n",
    "            'visualizations',\n",
    "            'checkpoints'\n",
    "        ]\n",
    "        for dir_name in directories:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    def load_urbansound_dataset(self, max_files_per_class=None):\n",
    "        \"\"\"Load UrbanSound8K dataset with optional limit\"\"\"\n",
    "        print(\"Loading UrbanSound8K dataset...\")\n",
    "        \n",
    "        # Read metadata\n",
    "        metadata_path = os.path.join(self.base_path, \"metadata/UrbanSound8K.csv\")\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Group by class and limit files if specified\n",
    "        if max_files_per_class:\n",
    "            sampled_metadata = metadata.groupby('class').apply(\n",
    "                lambda x: x.sample(min(len(x), max_files_per_class), random_state=self.random_seed)\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            sampled_metadata = metadata\n",
    "        \n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in tqdm(sampled_metadata.iterrows(), total=len(sampled_metadata), desc=\"Building file list\"):\n",
    "            fold = row['fold']\n",
    "            filename = row['slice_file_name']\n",
    "            file_path = os.path.join(self.base_path, f\"audio/fold{fold}\", filename)\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                file_paths.append(file_path)\n",
    "                labels.append(row['class'])\n",
    "        \n",
    "        print(f\"Loaded {len(file_paths)} files, {len(set(labels))} classes\")\n",
    "        return file_paths, labels\n",
    "    \n",
    "    def extract_original_features(self, audio):\n",
    "        \"\"\"Extract features from original audio (baseline)\"\"\"\n",
    "        # MFCC features (standard for audio classification)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=20)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        # Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=self.sample_rate)\n",
    "        chroma_scaled = np.mean(chroma.T, axis=0)\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate)\n",
    "        \n",
    "        # Combine features\n",
    "        features = np.concatenate([\n",
    "            mfccs_scaled,\n",
    "            chroma_scaled,\n",
    "            [np.mean(spectral_centroid), np.std(spectral_centroid)],\n",
    "            [np.mean(spectral_bandwidth), np.std(spectral_bandwidth)],\n",
    "            librosa.feature.zero_crossing_rate(audio)[0].mean(),\n",
    "            np.mean(librosa.feature.rms(y=audio))\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def compress_audio(self, audio, compression_ratio, matrix_type='bernoulli'):\n",
    "        \"\"\"Memory-efficient audio compression\"\"\"\n",
    "        # Apply DWT\n",
    "        coeffs = pywt.wavedec(audio, self.wavelet, level=self.dwt_level)\n",
    "        coeffs_flat = np.concatenate(coeffs)\n",
    "        N = len(coeffs_flat)\n",
    "        M = int(N * compression_ratio)\n",
    "        \n",
    "        # Block processing to avoid large matrices\n",
    "        block_size = 1000  # Process 1000 measurements at a time\n",
    "        compressed = np.zeros(M)\n",
    "        \n",
    "        # Get matrix generator based on type\n",
    "        if matrix_type == 'bernoulli':\n",
    "            matrix_func = MemoryEfficientMeasurementMatrices.bernoulli_matrix_block\n",
    "        elif matrix_type == 'gaussian':\n",
    "            matrix_func = MemoryEfficientMeasurementMatrices.gaussian_matrix_block\n",
    "        elif matrix_type == 'logistic_henon':\n",
    "            matrix_func = MemoryEfficientMeasurementMatrices.logistic_henon_matrix_block\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown matrix type: {matrix_type}\")\n",
    "        \n",
    "        # Process in blocks\n",
    "        for block_start in range(0, M, block_size):\n",
    "            block_end = min(block_start + block_size, M)\n",
    "            block_rows = block_end - block_start\n",
    "            \n",
    "            # Generate matrix block\n",
    "            Phi_block = matrix_func(block_rows, N, start_row=block_start, seed=self.random_seed)\n",
    "            \n",
    "            # Compute measurements for this block\n",
    "            compressed[block_start:block_end] = Phi_block @ coeffs_flat\n",
    "        \n",
    "        # Add statistics as features\n",
    "        features = np.concatenate([\n",
    "            compressed,\n",
    "            [\n",
    "                np.mean(compressed),\n",
    "                np.std(compressed),\n",
    "                np.max(np.abs(compressed)),\n",
    "                np.min(compressed),\n",
    "                np.percentile(np.abs(compressed), 90),\n",
    "                M / N  # Actual compression ratio\n",
    "            ]\n",
    "        ])\n",
    "        \n",
    "        return features, N, M\n",
    "    \n",
    "    def prepare_features(self, file_paths, labels, feature_type='original', \n",
    "                        compression_ratio=None, matrix_type=None):\n",
    "        \"\"\"Prepare features for training\"\"\"\n",
    "        print(f\"\\nPreparing {feature_type} features...\")\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        compression_info = []\n",
    "        \n",
    "        for i, (file_path, label) in tqdm(enumerate(zip(file_paths, labels)), \n",
    "                                         total=len(file_paths),\n",
    "                                         desc=f\"Processing {feature_type}\"):\n",
    "            try:\n",
    "                # Load and preprocess audio\n",
    "                audio, sr = librosa.load(file_path, sr=self.sample_rate, duration=self.duration)\n",
    "                \n",
    "                # Ensure fixed length\n",
    "                target_length = int(self.duration * self.sample_rate)\n",
    "                if len(audio) < target_length:\n",
    "                    audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target_length]\n",
    "                \n",
    "                # Normalize\n",
    "                audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
    "                \n",
    "                # Extract features\n",
    "                if feature_type == 'original':\n",
    "                    features = self.extract_original_features(audio)\n",
    "                    compression_info.append({'N': len(audio), 'M': len(features), 'ratio': 1.0})\n",
    "                else:\n",
    "                    features, N, M = self.compress_audio(audio, compression_ratio, matrix_type)\n",
    "                    compression_info.append({'N': N, 'M': M, 'ratio': M/N})\n",
    "                \n",
    "                features_list.append(features)\n",
    "                labels_list.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Save feature info\n",
    "        feature_info = {\n",
    "            'feature_type': feature_type,\n",
    "            'matrix_type': matrix_type,\n",
    "            'compression_ratio': compression_ratio,\n",
    "            'X_shape': X.shape,\n",
    "            'y_shape': y.shape,\n",
    "            'compression_stats': pd.DataFrame(compression_info).mean().to_dict()\n",
    "        }\n",
    "        \n",
    "        return X, y_encoded, feature_info\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, X_test, y_train, y_test, feature_info, \n",
    "                          classifier_names=['SVM', 'RandomForest', 'MLP']):\n",
    "        \"\"\"Train and evaluate classifiers\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        trained_models = {}\n",
    "        \n",
    "        for clf_name in classifier_names:\n",
    "            print(f\"  Training {clf_name}...\")\n",
    "            \n",
    "            # Initialize classifier\n",
    "            if clf_name == 'SVM':\n",
    "                clf = SVC(kernel='rbf', C=1.0, gamma='scale', \n",
    "                         random_state=self.random_seed, probability=True)\n",
    "            elif clf_name == 'RandomForest':\n",
    "                clf = RandomForestClassifier(n_estimators=100, \n",
    "                                           random_state=self.random_seed,\n",
    "                                           n_jobs=-1)  # Use all cores\n",
    "            elif clf_name == 'MLP':\n",
    "                clf = MLPClassifier(hidden_layer_sizes=(128, 64), \n",
    "                                   max_iter=500, random_state=self.random_seed,\n",
    "                                   early_stopping=True)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train\n",
    "            start_time = time.time()\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Predict\n",
    "            start_time = time.time()\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            y_pred_proba = clf.predict_proba(X_test_scaled) if hasattr(clf, 'predict_proba') else None\n",
    "            test_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                **feature_info,\n",
    "                'classifier': clf_name,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_score': f1,\n",
    "                'train_time': train_time,\n",
    "                'test_time': test_time,\n",
    "                'model_size': self._estimate_model_size(clf),\n",
    "                'train_samples': len(X_train),\n",
    "                'test_samples': len(X_test),\n",
    "                'feature_dim': X_train.shape[1]\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Save trained model\n",
    "            model_key = f\"{feature_info['feature_type']}_{feature_info.get('matrix_type', 'original')}_{clf_name}\"\n",
    "            trained_models[model_key] = {\n",
    "                'model': clf,\n",
    "                'scaler': scaler,\n",
    "                'feature_info': feature_info\n",
    "            }\n",
    "            \n",
    "            print(f\"    Accuracy: {accuracy:.4f}, F1: {f1:.4f}, \"\n",
    "                  f\"Train: {train_time:.2f}s, Test: {test_time:.4f}s\")\n",
    "        \n",
    "        return results, trained_models\n",
    "    \n",
    "    def _estimate_model_size(self, model):\n",
    "        \"\"\"Estimate model size in MB\"\"\"\n",
    "        import sys\n",
    "        \n",
    "        # Save to temporary file and check size\n",
    "        temp_file = 'temp_model.pkl'\n",
    "        joblib.dump(model, temp_file)\n",
    "        size_mb = os.path.getsize(temp_file) / (1024 * 1024)\n",
    "        os.remove(temp_file)\n",
    "        \n",
    "        return size_mb\n",
    "    \n",
    "    def run_experiment_pipeline(self, file_paths, labels, test_size=0.2, \n",
    "                               max_files_per_class=100):\n",
    "        \"\"\"Complete experiment pipeline\"\"\"\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"STARTING COMPREHENSIVE AUDIO COMPRESSION EXPERIMENTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Limit files if specified\n",
    "        if max_files_per_class:\n",
    "            print(f\"Limiting to {max_files_per_class} files per class\")\n",
    "            file_paths, labels = self._limit_files_per_class(file_paths, labels, max_files_per_class)\n",
    "        \n",
    "        # Split data once for consistency\n",
    "        X_full = list(zip(file_paths, labels))\n",
    "        y_full = labels\n",
    "        \n",
    "        # Encode labels for stratification\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y_full)\n",
    "        \n",
    "        train_idx, test_idx = train_test_split(\n",
    "            np.arange(len(X_full)), test_size=test_size, \n",
    "            random_state=self.random_seed, stratify=y_encoded\n",
    "        )\n",
    "        \n",
    "        train_files = [file_paths[i] for i in train_idx]\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        test_files = [file_paths[i] for i in test_idx]\n",
    "        test_labels = [labels[i] for i in test_idx]\n",
    "        \n",
    "        print(f\"Train set: {len(train_files)} files\")\n",
    "        print(f\"Test set: {len(test_files)} files\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Experiment 1: Original features (baseline)\n",
    "        print(\"\\n[EXPERIMENT 1] Original Audio Features (Baseline)\")\n",
    "        X_train_orig, y_train_orig, orig_info = self.prepare_features(\n",
    "            train_files, train_labels, feature_type='original'\n",
    "        )\n",
    "        X_test_orig, y_test_orig, _ = self.prepare_features(\n",
    "            test_files, test_labels, feature_type='original'\n",
    "        )\n",
    "        \n",
    "        orig_results, orig_models = self.train_and_evaluate(\n",
    "            X_train_orig, X_test_orig, y_train_orig, y_test_orig, orig_info\n",
    "        )\n",
    "        \n",
    "        self.results['original'] = orig_results\n",
    "        self.experiment_data['original'] = {\n",
    "            'X_train': X_train_orig, 'X_test': X_test_orig,\n",
    "            'y_train': y_train_orig, 'y_test': y_test_orig,\n",
    "            'info': orig_info\n",
    "        }\n",
    "        \n",
    "        # Experiments 2-10: Compressed sensing with different matrices and ratios\n",
    "        matrix_types = ['bernoulli', 'gaussian', 'logistic_henon']\n",
    "        compression_ratios = [0.5, 0.6, 0.7]  # 50%, 60%, 70%\n",
    "        \n",
    "        for matrix_type in matrix_types:\n",
    "            for comp_ratio in compression_ratios:\n",
    "                exp_name = f\"{matrix_type}_{int(comp_ratio*100)}\"\n",
    "                print(f\"\\n[EXPERIMENT {exp_name.upper()}] {matrix_type.capitalize()} Matrix, {comp_ratio*100:.0f}% Compression\")\n",
    "                \n",
    "                # Prepare compressed features\n",
    "                X_train_comp, y_train_comp, comp_info = self.prepare_features(\n",
    "                    train_files, train_labels, \n",
    "                    feature_type='compressed',\n",
    "                    compression_ratio=comp_ratio,\n",
    "                    matrix_type=matrix_type\n",
    "                )\n",
    "                \n",
    "                X_test_comp, y_test_comp, _ = self.prepare_features(\n",
    "                    test_files, test_labels,\n",
    "                    feature_type='compressed',\n",
    "                    compression_ratio=comp_ratio,\n",
    "                    matrix_type=matrix_type\n",
    "                )\n",
    "                \n",
    "                # Train and evaluate\n",
    "                comp_results, comp_models = self.train_and_evaluate(\n",
    "                    X_train_comp, X_test_comp, y_train_comp, y_test_comp, comp_info\n",
    "                )\n",
    "                \n",
    "                self.results[exp_name] = comp_results\n",
    "                self.experiment_data[exp_name] = {\n",
    "                    'X_train': X_train_comp, 'X_test': X_test_comp,\n",
    "                    'y_train': y_train_comp, 'y_test': y_test_comp,\n",
    "                    'info': comp_info\n",
    "                }\n",
    "                \n",
    "                # Save checkpoint\n",
    "                self.save_checkpoint(exp_name)\n",
    "        \n",
    "        # Combine all results\n",
    "        all_results = []\n",
    "        for exp_name, exp_results in self.results.items():\n",
    "            all_results.extend(exp_results)\n",
    "        \n",
    "        self.complete_results = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Save everything\n",
    "        self.save_results()\n",
    "        \n",
    "        return self.complete_results\n",
    "    \n",
    "    def _limit_files_per_class(self, file_paths, labels, max_per_class):\n",
    "        \"\"\"Limit number of files per class\"\"\"\n",
    "        df = pd.DataFrame({'path': file_paths, 'label': labels})\n",
    "        sampled_df = df.groupby('label').apply(\n",
    "            lambda x: x.sample(min(len(x), max_per_class), random_state=self.random_seed)\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        return sampled_df['path'].tolist(), sampled_df['label'].tolist()\n",
    "    \n",
    "    def save_checkpoint(self, exp_name):\n",
    "        \"\"\"Save checkpoint after each experiment\"\"\"\n",
    "        checkpoint = {\n",
    "            'results': self.results,\n",
    "            'experiment_data': {k: v for k, v in self.experiment_data.items() if 'X_train' not in k},  # Don't save data\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        checkpoint_file = f\"checkpoints/checkpoint_{exp_name}.pkl\"\n",
    "        joblib.dump(checkpoint, checkpoint_file)\n",
    "        print(f\"  Checkpoint saved: {checkpoint_file}\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save all results to files\"\"\"\n",
    "        print(\"\\nSaving results...\")\n",
    "        \n",
    "        # Save results DataFrame\n",
    "        results_file = \"results/compression_experiments_results.csv\"\n",
    "        self.complete_results.to_csv(results_file, index=False)\n",
    "        print(f\"✓ Results saved to: {results_file}\")\n",
    "        \n",
    "        # Save detailed results JSON\n",
    "        json_file = \"results/experiment_details.json\"\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'experiment_summary': self.complete_results.to_dict('records'),\n",
    "                'statistics': self._calculate_statistics(),\n",
    "                'config': {\n",
    "                    'wavelet': self.wavelet,\n",
    "                    'dwt_level': self.dwt_level,\n",
    "                    'sample_rate': self.sample_rate,\n",
    "                    'duration': self.duration,\n",
    "                    'random_seed': self.random_seed\n",
    "                }\n",
    "            }, f, indent=2)\n",
    "        print(f\"✓ Detailed results saved to: {json_file}\")\n",
    "        \n",
    "        # Save feature dimensions comparison\n",
    "        dims_df = self.complete_results[['feature_type', 'matrix_type', 'compression_ratio', 'feature_dim']].drop_duplicates()\n",
    "        dims_df.to_csv(\"results/feature_dimensions.csv\", index=False)\n",
    "        print(f\"✓ Feature dimensions saved\")\n",
    "    \n",
    "    def _calculate_statistics(self):\n",
    "        \"\"\"Calculate experiment statistics\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # Group by experiment type\n",
    "        for exp_name, exp_results in self.results.items():\n",
    "            exp_df = pd.DataFrame(exp_results)\n",
    "            stats[exp_name] = {\n",
    "                'mean_accuracy': exp_df['accuracy'].mean(),\n",
    "                'std_accuracy': exp_df['accuracy'].std(),\n",
    "                'mean_f1': exp_df['f1_score'].mean(),\n",
    "                'mean_train_time': exp_df['train_time'].mean(),\n",
    "                'mean_test_time': exp_df['test_time'].mean(),\n",
    "                'mean_model_size': exp_df['model_size'].mean()\n",
    "            }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd0e67-4029-475d-873b-988cf85ac035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveVisualizer:\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self, results_df):\n",
    "        self.results_df = results_df\n",
    "    \n",
    "    def create_all_visualizations(self):\n",
    "        \"\"\"Create all comparison visualizations\"\"\"\n",
    "        print(\"\\nCreating visualizations...\")\n",
    "        \n",
    "        self.plot_accuracy_comparison()\n",
    "        self.plot_compression_tradeoff()\n",
    "        self.plot_matrix_comparison()\n",
    "        self.plot_feature_dimension_impact()\n",
    "        self.plot_training_time_comparison()\n",
    "        self.plot_confusion_matrix_heatmaps()\n",
    "        self.plot_statistical_significance()\n",
    "        \n",
    "        print(\"✓ All visualizations saved to 'visualizations/' folder\")\n",
    "    \n",
    "    def plot_accuracy_comparison(self):\n",
    "        \"\"\"Plot accuracy comparison across all experiments\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Accuracy Comparison Across Compression Methods and Ratios', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Accuracy by matrix type (grouped bars)\n",
    "        ax = axes[0, 0]\n",
    "        matrix_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        matrix_pivot = matrix_data.pivot_table(\n",
    "            index=['matrix_type', 'compression_ratio'],\n",
    "            columns='classifier',\n",
    "            values='accuracy',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        matrix_pivot.plot(kind='bar', ax=ax, width=0.8, colormap='tab20c')\n",
    "        ax.set_title('Accuracy by Matrix Type and Compression Ratio')\n",
    "        ax.set_xlabel('Matrix Type × Compression Ratio')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend(title='Classifier')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 2: Original vs Best Compression\n",
    "        ax = axes[0, 1]\n",
    "        original_acc = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        compressed_acc = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # Find best compressed configuration for each classifier\n",
    "        best_compressed = []\n",
    "        for classifier in self.results_df['classifier'].unique():\n",
    "            classifier_data = compressed_acc[compressed_acc['classifier'] == classifier]\n",
    "            best_idx = classifier_data['accuracy'].idxmax()\n",
    "            best_compressed.append(classifier_data.loc[best_idx])\n",
    "        \n",
    "        best_compressed_df = pd.DataFrame(best_compressed)\n",
    "        \n",
    "        x = np.arange(len(self.results_df['classifier'].unique()))\n",
    "        width = 0.35\n",
    "        \n",
    "        for i, classifier in enumerate(self.results_df['classifier'].unique()):\n",
    "            orig_acc = original_acc[original_acc['classifier'] == classifier]['accuracy'].values[0]\n",
    "            best_comp_acc = best_compressed_df[best_compressed_df['classifier'] == classifier]['accuracy'].values[0]\n",
    "            \n",
    "            ax.bar(i - width/2, orig_acc, width, label='Original' if i == 0 else '', color='blue', alpha=0.7)\n",
    "            ax.bar(i + width/2, best_comp_acc, width, label='Best Compressed' if i == 0 else '', color='red', alpha=0.7)\n",
    "            \n",
    "            # Add text labels\n",
    "            ax.text(i - width/2, orig_acc + 0.01, f'{orig_acc:.3f}', ha='center', va='bottom')\n",
    "            ax.text(i + width/2, best_comp_acc + 0.01, f'{best_comp_acc:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        ax.set_xlabel('Classifier')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Original vs Best Compressed Accuracy')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.results_df['classifier'].unique())\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Compression Ratio Impact\n",
    "        ax = axes[1, 0]\n",
    "        for matrix_type in matrix_data['matrix_type'].unique():\n",
    "            matrix_subset = matrix_data[matrix_data['matrix_type'] == matrix_type]\n",
    "            avg_by_ratio = matrix_subset.groupby('compression_ratio')['accuracy'].mean()\n",
    "            ax.plot(avg_by_ratio.index, avg_by_ratio.values, \n",
    "                   marker='o', linewidth=2, label=matrix_type.capitalize())\n",
    "        \n",
    "        ax.set_xlabel('Compression Ratio')\n",
    "        ax.set_ylabel('Average Accuracy')\n",
    "        ax.set_title('Impact of Compression Ratio on Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Feature Dimension vs Accuracy\n",
    "        ax = axes[1, 1]\n",
    "        scatter_data = self.results_df.groupby(['feature_type', 'matrix_type', 'compression_ratio']).agg({\n",
    "            'feature_dim': 'mean',\n",
    "            'accuracy': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        colors = {'original': 'black', 'bernoulli': 'blue', 'gaussian': 'green', 'logistic_henon': 'red'}\n",
    "        sizes = {'original': 200, 'bernoulli': 100, 'gaussian': 100, 'logistic_henon': 100}\n",
    "        \n",
    "        for _, row in scatter_data.iterrows():\n",
    "            color = colors.get(row['matrix_type'] if row['feature_type'] == 'compressed' else 'original', 'gray')\n",
    "            size = sizes.get(row['matrix_type'] if row['feature_type'] == 'compressed' else 'original', 100)\n",
    "            marker = 'o' if row['feature_type'] == 'original' else 's'\n",
    "            \n",
    "            ax.scatter(row['feature_dim'], row['accuracy'], \n",
    "                      color=color, s=size, alpha=0.7, marker=marker)\n",
    "            \n",
    "            if row['feature_type'] == 'compressed':\n",
    "                label = f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\"\n",
    "                ax.text(row['feature_dim'], row['accuracy'] + 0.005, \n",
    "                       label, fontsize=8, ha='center')\n",
    "        \n",
    "        ax.set_xlabel('Feature Dimension')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Feature Dimension vs Accuracy')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.lines import Line2D\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='w', label='Original',\n",
    "                  markerfacecolor='black', markersize=10),\n",
    "            Line2D([0], [0], marker='s', color='w', label='Bernoulli',\n",
    "                  markerfacecolor='blue', markersize=10),\n",
    "            Line2D([0], [0], marker='s', color='w', label='Gaussian',\n",
    "                  markerfacecolor='green', markersize=10),\n",
    "            Line2D([0], [0], marker='s', color='w', label='Logistic-Henon',\n",
    "                  markerfacecolor='red', markersize=10)\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, title='Matrix Type')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_compression_tradeoff(self):\n",
    "        \"\"\"Plot compression trade-off analysis\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle('Compression Trade-off Analysis', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Filter compressed data\n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # Plot 1: Accuracy vs Compression Ratio\n",
    "        ax = axes[0]\n",
    "        for classifier in compressed_data['classifier'].unique():\n",
    "            classifier_data = compressed_data[compressed_data['classifier'] == classifier]\n",
    "            avg_by_ratio = classifier_data.groupby('compression_ratio')['accuracy'].mean()\n",
    "            ax.plot(avg_by_ratio.index, avg_by_ratio.values, marker='o', label=classifier)\n",
    "        \n",
    "        ax.set_xlabel('Compression Ratio')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Accuracy vs Compression Ratio')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Training Time vs Feature Dimension\n",
    "        ax = axes[1]\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            matrix_subset = compressed_data[compressed_data['matrix_type'] == matrix_type]\n",
    "            ax.scatter(matrix_subset['feature_dim'], matrix_subset['train_time'], \n",
    "                      alpha=0.6, label=matrix_type, s=50)\n",
    "        \n",
    "        ax.set_xlabel('Feature Dimension')\n",
    "        ax.set_ylabel('Training Time (s)')\n",
    "        ax.set_title('Training Time vs Feature Dimension')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Model Size vs Accuracy\n",
    "        ax = axes[2]\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            for classifier in compressed_data['classifier'].unique():\n",
    "                subset = compressed_data[(compressed_data['matrix_type'] == matrix_type) & \n",
    "                                        (compressed_data['classifier'] == classifier)]\n",
    "                if len(subset) > 0:\n",
    "                    ax.scatter(subset['model_size'], subset['accuracy'], \n",
    "                              alpha=0.6, s=50, \n",
    "                              label=f\"{matrix_type}_{classifier}\" if matrix_type == 'bernoulli' else '')\n",
    "        \n",
    "        ax.set_xlabel('Model Size (MB)')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Model Size vs Accuracy')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/compression_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_matrix_comparison(self):\n",
    "        \"\"\"Detailed matrix type comparison\"\"\"\n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: Matrix type performance by classifier\n",
    "        ax = axes[0, 0]\n",
    "        matrix_perf = compressed_data.groupby(['matrix_type', 'classifier'])['accuracy'].mean().unstack()\n",
    "        matrix_perf.plot(kind='bar', ax=ax, width=0.8, colormap='Set2')\n",
    "        ax.set_title('Matrix Type Performance by Classifier')\n",
    "        ax.set_xlabel('Matrix Type')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.legend(title='Classifier')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, patch in enumerate(ax.patches):\n",
    "            ax.text(patch.get_x() + patch.get_width()/2, patch.get_height() + 0.01,\n",
    "                   f'{patch.get_height():.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Plot 2: Speed comparison\n",
    "        ax = axes[0, 1]\n",
    "        speed_data = compressed_data.groupby(['matrix_type', 'classifier'])[['train_time', 'test_time']].mean()\n",
    "        speed_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "        ax.set_title('Training and Testing Time by Matrix Type')\n",
    "        ax.set_xlabel('Matrix Type × Classifier')\n",
    "        ax.set_ylabel('Time (seconds)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 3: Statistical distribution\n",
    "        ax = axes[1, 0]\n",
    "        accuracy_data = []\n",
    "        labels = []\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            matrix_subset = compressed_data[compressed_data['matrix_type'] == matrix_type]\n",
    "            accuracy_data.append(matrix_subset['accuracy'].values)\n",
    "            labels.append(matrix_type)\n",
    "        \n",
    "        ax.boxplot(accuracy_data, labels=labels)\n",
    "        ax.set_title('Accuracy Distribution by Matrix Type')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Best performing configuration\n",
    "        ax = axes[1, 1]\n",
    "        best_configs = []\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            for classifier in compressed_data['classifier'].unique():\n",
    "                subset = compressed_data[(compressed_data['matrix_type'] == matrix_type) & \n",
    "                                        (compressed_data['classifier'] == classifier)]\n",
    "                if len(subset) > 0:\n",
    "                    best_idx = subset['accuracy'].idxmax()\n",
    "                    best_configs.append(subset.loc[best_idx])\n",
    "        \n",
    "        best_df = pd.DataFrame(best_configs)\n",
    "        best_df['config'] = best_df['matrix_type'] + '_' + best_df['compression_ratio'].astype(str)\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(best_df)))\n",
    "        bars = ax.bar(range(len(best_df)), best_df['accuracy'], color=colors)\n",
    "        ax.set_title('Best Configuration for Each Matrix+Classifier')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_xticks(range(len(best_df)))\n",
    "        ax.set_xticklabels([f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\\n{row['classifier']}\" \n",
    "                           for _, row in best_df.iterrows()], rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, acc in zip(bars, best_df['accuracy']):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/matrix_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_dimension_impact(self):\n",
    "        \"\"\"Analyze impact of feature dimension\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Get compressed data\n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # Plot 1: Dimension reduction percentage\n",
    "        ax = axes[0]\n",
    "        original_dims = self.results_df[self.results_df['feature_type'] == 'original']['feature_dim'].mean()\n",
    "        \n",
    "        dim_reduction = []\n",
    "        labels = []\n",
    "        for _, row in compressed_data.iterrows():\n",
    "            reduction = (1 - row['feature_dim'] / original_dims) * 100\n",
    "            dim_reduction.append(reduction)\n",
    "            labels.append(f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\")\n",
    "        \n",
    "        ax.bar(range(len(dim_reduction)), dim_reduction, alpha=0.7)\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Dimension Reduction (%)')\n",
    "        ax.set_title('Feature Dimension Reduction')\n",
    "        ax.set_xticks(range(len(dim_reduction)))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 2: Accuracy retention vs dimension reduction\n",
    "        ax = axes[1]\n",
    "        original_acc = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        \n",
    "        for classifier in compressed_data['classifier'].unique():\n",
    "            classifier_orig = original_acc[original_acc['classifier'] == classifier]['accuracy'].values[0]\n",
    "            classifier_comp = compressed_data[compressed_data['classifier'] == classifier]\n",
    "            \n",
    "            retention = []\n",
    "            reduction = []\n",
    "            for _, row in classifier_comp.iterrows():\n",
    "                acc_retention = (row['accuracy'] / classifier_orig) * 100\n",
    "                dim_reduction = (1 - row['feature_dim'] / original_dims) * 100\n",
    "                retention.append(acc_retention)\n",
    "                reduction.append(dim_reduction)\n",
    "            \n",
    "            ax.scatter(reduction, retention, alpha=0.6, label=classifier, s=80)\n",
    "        \n",
    "        ax.set_xlabel('Dimension Reduction (%)')\n",
    "        ax.set_ylabel('Accuracy Retention (%)')\n",
    "        ax.set_title('Accuracy Retention vs Dimension Reduction')\n",
    "        ax.axhline(y=100, color='r', linestyle='--', alpha=0.5, label='Original Accuracy')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/feature_dimension_impact.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_time_comparison(self):\n",
    "        \"\"\"Compare training times\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Plot 1: Training time by configuration\n",
    "        ax = axes[0]\n",
    "        time_data = self.results_df.pivot_table(\n",
    "            index=['feature_type', 'matrix_type', 'compression_ratio'],\n",
    "            columns='classifier',\n",
    "            values='train_time',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        time_data.plot(kind='bar', ax=ax, width=0.8, colormap='tab20c')\n",
    "        ax.set_title('Training Time by Configuration')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Training Time (seconds)')\n",
    "        ax.legend(title='Classifier')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 2: Speedup factor\n",
    "        ax = axes[1]\n",
    "        original_times = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        compressed_times = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        speedup_data = []\n",
    "        labels = []\n",
    "        for classifier in self.results_df['classifier'].unique():\n",
    "            orig_time = original_times[original_times['classifier'] == classifier]['train_time'].mean()\n",
    "            \n",
    "            for matrix_type in compressed_times['matrix_type'].unique():\n",
    "                for ratio in compressed_times['compression_ratio'].unique():\n",
    "                    comp_time = compressed_times[(compressed_times['classifier'] == classifier) &\n",
    "                                                (compressed_times['matrix_type'] == matrix_type) &\n",
    "                                                (compressed_times['compression_ratio'] == ratio)]['train_time'].mean()\n",
    "                    \n",
    "                    if not np.isnan(comp_time) and comp_time > 0:\n",
    "                        speedup = orig_time / comp_time\n",
    "                        speedup_data.append(speedup)\n",
    "                        labels.append(f\"{classifier[:3]}_{matrix_type[:3]}_{int(ratio*100)}%\")\n",
    "        \n",
    "        colors = ['green' if x >= 1 else 'red' for x in speedup_data]\n",
    "        bars = ax.bar(range(len(speedup_data)), speedup_data, color=colors, alpha=0.7)\n",
    "        ax.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='No Speedup')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Speedup Factor (Original/Compressed)')\n",
    "        ax.set_title('Training Speedup from Compression')\n",
    "        ax.set_xticks(range(len(speedup_data)))\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, speedup in zip(bars, speedup_data):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                   f'{speedup:.2f}x', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/training_time_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix_heatmaps(self):\n",
    "        \"\"\"Create confusion matrix heatmaps for best configurations\"\"\"\n",
    "        # This is a placeholder - you'll need to implement based on your actual predictions\n",
    "        print(\"Confusion matrix visualization would require prediction data storage\")\n",
    "        \n",
    "        # Example of what you could implement:\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "        \n",
    "        best_configs = self._get_best_configurations()\n",
    "        \n",
    "        for idx, (exp_name, config) in enumerate(best_configs.items()):\n",
    "            row, col = divmod(idx, 3)\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Get predictions (you need to store these during training)\n",
    "            y_true = config['y_test']\n",
    "            y_pred = config['predictions']\n",
    "            \n",
    "            cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "            sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', ax=ax)\n",
    "            ax.set_title(f\"{exp_name}\\nAccuracy: {config['accuracy']:.3f}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "    \n",
    "    def plot_statistical_significance(self):\n",
    "        \"\"\"Plot statistical significance tests\"\"\"\n",
    "        # Perform ANOVA or t-tests\n",
    "        print(\"Statistical significance analysis would require multiple runs\")\n",
    "        \n",
    "        # Example implementation:\n",
    "        \"\"\"\n",
    "        from scipy import stats\n",
    "        import itertools\n",
    "        \n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # Compare matrix types\n",
    "        matrix_accuracies = {}\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            matrix_accuracies[matrix_type] = compressed_data[\n",
    "                compressed_data['matrix_type'] == matrix_type\n",
    "            ]['accuracy'].values\n",
    "        \n",
    "        # Create significance matrix\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        matrix_types = list(matrix_accuracies.keys())\n",
    "        significance_matrix = np.zeros((len(matrix_types), len(matrix_types)))\n",
    "        \n",
    "        for i, j in itertools.combinations(range(len(matrix_types)), 2):\n",
    "            t_stat, p_value = stats.ttest_ind(\n",
    "                matrix_accuracies[matrix_types[i]],\n",
    "                matrix_accuracies[matrix_types[j]]\n",
    "            )\n",
    "            significance_matrix[i, j] = p_value\n",
    "            significance_matrix[j, i] = p_value\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(significance_matrix, annot=True, fmt='.4f', \n",
    "                   cmap='RdYlGn_r', vmin=0, vmax=0.05,\n",
    "                   xticklabels=matrix_types, yticklabels=matrix_types,\n",
    "                   ax=ax)\n",
    "        ax.set_title('Statistical Significance (p-values)\\nMatrix Type Comparisons')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/statistical_significance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33cf18-039f-404d-82f6-0518d702b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryReportGenerator:\n",
    "    \"\"\"Generate comprehensive summary reports\"\"\"\n",
    "    \n",
    "    def __init__(self, results_df):\n",
    "        self.results_df = results_df\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate complete summary report\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT SUMMARY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.print_overall_summary()\n",
    "        self.print_best_configurations()\n",
    "        self.print_compression_analysis()\n",
    "        self.print_recommendations()\n",
    "        \n",
    "        # Save report\n",
    "        self.save_detailed_report()\n",
    "    \n",
    "    def print_overall_summary(self):\n",
    "        \"\"\"Print overall experiment summary\"\"\"\n",
    "        print(\"\\n📊 OVERALL SUMMARY\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        # Original performance\n",
    "        original_data = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        print(\"Original Audio Features (Baseline):\")\n",
    "        for classifier in original_data['classifier'].unique():\n",
    "            classifier_data = original_data[original_data['classifier'] == classifier]\n",
    "            print(f\"  {classifier}: Accuracy = {classifier_data['accuracy'].mean():.4f}, \"\n",
    "                  f\"Features = {int(classifier_data['feature_dim'].mean())}\")\n",
    "        \n",
    "        # Compressed performance summary\n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        print(f\"\\nCompressed Sensing Average (All configurations):\")\n",
    "        print(f\"  Accuracy: {compressed_data['accuracy'].mean():.4f}\")\n",
    "        print(f\"  Feature Dimension Reduction: \"\n",
    "              f\"{(1 - compressed_data['feature_dim'].mean() / original_data['feature_dim'].mean()) * 100:.1f}%\")\n",
    "        print(f\"  Training Time Reduction: \"\n",
    "              f\"{(1 - compressed_data['train_time'].mean() / original_data['train_time'].mean()) * 100:.1f}%\")\n",
    "    \n",
    "    def print_best_configurations(self):\n",
    "        \"\"\"Print best performing configurations\"\"\"\n",
    "        print(\"\\n🏆 BEST CONFIGURATIONS\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        original_data = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        \n",
    "        # Best overall\n",
    "        best_overall = compressed_data.loc[compressed_data['accuracy'].idxmax()]\n",
    "        print(f\"Best Overall Compressed Configuration:\")\n",
    "        print(f\"  Matrix: {best_overall['matrix_type'].capitalize()}\")\n",
    "        print(f\"  Compression: {best_overall['compression_ratio']*100:.0f}%\")\n",
    "        print(f\"  Classifier: {best_overall['classifier']}\")\n",
    "        print(f\"  Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "        print(f\"  Feature Dim: {int(best_overall['feature_dim'])}\")\n",
    "        \n",
    "        # Best by matrix type\n",
    "        print(f\"\\nBest by Matrix Type:\")\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            matrix_data = compressed_data[compressed_data['matrix_type'] == matrix_type]\n",
    "            best_matrix = matrix_data.loc[matrix_data['accuracy'].idxmax()]\n",
    "            print(f\"  {matrix_type.capitalize()}: \"\n",
    "                  f\"{best_matrix['classifier']} at {best_matrix['compression_ratio']*100:.0f}% \"\n",
    "                  f\"(Acc: {best_matrix['accuracy']:.4f})\")\n",
    "        \n",
    "        # Best by classifier\n",
    "        print(f\"\\nBest by Classifier:\")\n",
    "        for classifier in compressed_data['classifier'].unique():\n",
    "            classifier_data = compressed_data[compressed_data['classifier'] == classifier]\n",
    "            best_classifier = classifier_data.loc[classifier_data['accuracy'].idxmax()]\n",
    "            \n",
    "            # Compare with original\n",
    "            orig_acc = original_data[original_data['classifier'] == classifier]['accuracy'].values[0]\n",
    "            accuracy_drop = orig_acc - best_classifier['accuracy']\n",
    "            \n",
    "            print(f\"  {classifier}: {best_classifier['matrix_type']} \"\n",
    "                  f\"{best_classifier['compression_ratio']*100:.0f}% \"\n",
    "                  f\"(Acc: {best_classifier['accuracy']:.4f}, \"\n",
    "                  f\"Drop: {accuracy_drop:.4f})\")\n",
    "    \n",
    "    def print_compression_analysis(self):\n",
    "        \"\"\"Print compression trade-off analysis\"\"\"\n",
    "        print(\"\\n📉 COMPRESSION TRADE-OFF ANALYSIS\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        original_data = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        \n",
    "        # Analyze by compression ratio\n",
    "        print(\"Performance by Compression Ratio:\")\n",
    "        for ratio in sorted(compressed_data['compression_ratio'].unique()):\n",
    "            ratio_data = compressed_data[compressed_data['compression_ratio'] == ratio]\n",
    "            avg_acc = ratio_data['accuracy'].mean()\n",
    "            avg_features = ratio_data['feature_dim'].mean()\n",
    "            orig_features = original_data['feature_dim'].mean()\n",
    "            \n",
    "            feature_reduction = (1 - avg_features / orig_features) * 100\n",
    "            \n",
    "            print(f\"  {ratio*100:.0f}% compression: \"\n",
    "                  f\"Accuracy = {avg_acc:.4f}, \"\n",
    "                  f\"Features reduced by {feature_reduction:.1f}%\")\n",
    "    \n",
    "    def print_recommendations(self):\n",
    "        \"\"\"Print practical recommendations\"\"\"\n",
    "        print(\"\\n💡 PRACTICAL RECOMMENDATIONS\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # Find configurations with < 5% accuracy drop\n",
    "        original_data = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        good_configs = []\n",
    "        \n",
    "        for classifier in compressed_data['classifier'].unique():\n",
    "            orig_acc = original_data[original_data['classifier'] == classifier]['accuracy'].values[0]\n",
    "            classifier_data = compressed_data[compressed_data['classifier'] == classifier]\n",
    "            \n",
    "            for _, row in classifier_data.iterrows():\n",
    "                accuracy_drop = orig_acc - row['accuracy']\n",
    "                if accuracy_drop <= 0.05:  # 5% or less drop\n",
    "                    good_configs.append({\n",
    "                        'classifier': classifier,\n",
    "                        'matrix': row['matrix_type'],\n",
    "                        'ratio': row['compression_ratio'],\n",
    "                        'accuracy': row['accuracy'],\n",
    "                        'drop': accuracy_drop,\n",
    "                        'features': row['feature_dim'],\n",
    "                        'train_time': row['train_time']\n",
    "                    })\n",
    "        \n",
    "        if good_configs:\n",
    "            print(\"Recommended configurations (accuracy drop ≤ 5%):\")\n",
    "            for config in sorted(good_configs, key=lambda x: x['accuracy'], reverse=True)[:5]:\n",
    "                print(f\"  • {config['classifier']} with {config['matrix']} \"\n",
    "                      f\"{config['ratio']*100:.0f}%: \"\n",
    "                      f\"Acc = {config['accuracy']:.4f}, \"\n",
    "                      f\"Drop = {config['drop']:.4f}, \"\n",
    "                      f\"Features = {int(config['features'])}\")\n",
    "        else:\n",
    "            print(\"No configurations with ≤5% accuracy drop found.\")\n",
    "        \n",
    "        # Speed-focused recommendations\n",
    "        print(\"\\nSpeed-focused recommendations:\")\n",
    "        fast_configs = compressed_data.nsmallest(5, 'train_time')\n",
    "        for _, row in fast_configs.iterrows():\n",
    "            print(f\"  • {row['classifier']} with {row['matrix_type']} \"\n",
    "                  f\"{row['compression_ratio']*100:.0f}%: \"\n",
    "                  f\"Train time = {row['train_time']:.2f}s, \"\n",
    "                  f\"Acc = {row['accuracy']:.4f}\")\n",
    "    \n",
    "    def save_detailed_report(self):\n",
    "        \"\"\"Save detailed report to file\"\"\"\n",
    "        report_content = []\n",
    "        report_content.append(\"=\"*80)\n",
    "        report_content.append(\"COMPREHENSIVE AUDIO COMPRESSION EXPERIMENT REPORT\")\n",
    "        report_content.append(\"=\"*80)\n",
    "        report_content.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        report_content.append(f\"Total Experiments: {len(self.results_df)}\")\n",
    "        report_content.append(\"\")\n",
    "        \n",
    "        # Add summary sections\n",
    "        sections = [\n",
    "            self._get_overall_summary_text(),\n",
    "            self._get_best_configurations_text(),\n",
    "            self._get_compression_analysis_text(),\n",
    "            self._get_recommendations_text()\n",
    "        ]\n",
    "        \n",
    "        for section in sections:\n",
    "            report_content.extend(section)\n",
    "            report_content.append(\"\")\n",
    "        \n",
    "        # Save to file\n",
    "        with open('results/experiment_report.txt', 'w') as f:\n",
    "            f.write('\\n'.join(report_content))\n",
    "        \n",
    "        print(f\"\\n✓ Detailed report saved to: results/experiment_report.txt\")\n",
    "    \n",
    "    def _get_overall_summary_text(self):\n",
    "        \"\"\"Get overall summary as text\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"OVERALL SUMMARY\")\n",
    "        lines.append(\"-\"*40)\n",
    "        \n",
    "        original_data = self.results_df[self.results_df['feature_type'] == 'original']\n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        lines.append(\"Original Audio Features (Baseline):\")\n",
    "        for classifier in original_data['classifier'].unique():\n",
    "            classifier_data = original_data[original_data['classifier'] == classifier]\n",
    "            lines.append(f\"  {classifier}: Accuracy = {classifier_data['accuracy'].mean():.4f}, \"\n",
    "                        f\"Features = {int(classifier_data['feature_dim'].mean())}\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Compressed Sensing Summary:\")\n",
    "        lines.append(f\"  Average Accuracy: {compressed_data['accuracy'].mean():.4f}\")\n",
    "        lines.append(f\"  Average Feature Reduction: \"\n",
    "                     f\"{(1 - compressed_data['feature_dim'].mean() / original_data['feature_dim'].mean()) * 100:.1f}%\")\n",
    "        lines.append(f\"  Average Training Time Reduction: \"\n",
    "                     f\"{(1 - compressed_data['train_time'].mean() / original_data['train_time'].mean()) * 100:.1f}%\")\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def _get_best_configurations_text(self):\n",
    "        \"\"\"Get best configurations as text\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"BEST CONFIGURATIONS\")\n",
    "        lines.append(\"-\"*40)\n",
    "        \n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # Best overall\n",
    "        best_overall = compressed_data.loc[compressed_data['accuracy'].idxmax()]\n",
    "        lines.append(f\"Best Overall:\")\n",
    "        lines.append(f\"  Matrix: {best_overall['matrix_type'].capitalize()}\")\n",
    "        lines.append(f\"  Compression: {best_overall['compression_ratio']*100:.0f}%\")\n",
    "        lines.append(f\"  Classifier: {best_overall['classifier']}\")\n",
    "        lines.append(f\"  Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "        lines.append(f\"  Features: {int(best_overall['feature_dim'])}\")\n",
    "        lines.append(f\"  Training Time: {best_overall['train_time']:.2f}s\")\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def _get_compression_analysis_text(self):\n",
    "        \"\"\"Get compression analysis as text\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"COMPRESSION TRADE-OFF ANALYSIS\")\n",
    "        lines.append(\"-\"*40)\n",
    "        \n",
    "        compressed_data = self.results_df[self.results_df['feature_type'] == 'compressed']\n",
    "        \n",
    "        # By matrix type\n",
    "        lines.append(\"Performance by Matrix Type:\")\n",
    "        for matrix_type in compressed_data['matrix_type'].unique():\n",
    "            matrix_data = compressed_data[compressed_data['matrix_type'] == matrix_type]\n",
    "            lines.append(f\"  {matrix_type.capitalize()}: \"\n",
    "                        f\"Accuracy = {matrix_data['accuracy'].mean():.4f}, \"\n",
    "                        f\"Train Time = {matrix_data['train_time'].mean():.2f}s\")\n",
    "        \n",
    "        # By compression ratio\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Performance by Compression Ratio:\")\n",
    "        for ratio in sorted(compressed_data['compression_ratio'].unique()):\n",
    "            ratio_data = compressed_data[compressed_data['compression_ratio'] == ratio]\n",
    "            lines.append(f\"  {ratio*100:.0f}%: \"\n",
    "                        f\"Accuracy = {ratio_data['accuracy'].mean():.4f}, \"\n",
    "                        f\"Features = {ratio_data['feature_dim'].mean():.0f}\")\n",
    "        \n",
    "        return lines\n",
    "    \n",
    "    def _get_recommendations_text(self):\n",
    "        \"\"\"Get recommendations as text\"\"\"\n",
    "        lines = []\n",
    "        lines.append(\"RECOMMENDATIONS\")\n",
    "        lines.append(\"-\"*40)\n",
    "        lines.append(\"Based on the experiments, here are practical recommendations:\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"1. For Maximum Accuracy:\")\n",
    "        lines.append(\"   - Use the best overall configuration identified above\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"2. For Speed/Practicality:\")\n",
    "        lines.append(\"   - Bernoulli matrix is fastest to generate\")\n",
    "        lines.append(\"   - 70% compression gives good speedup with reasonable accuracy\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"3. For Memory Efficiency:\")\n",
    "        lines.append(\"   - Higher compression ratios use less memory\")\n",
    "        lines.append(\"   - Logistic-Henon may have security benefits but is slower\")\n",
    "        \n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91435b-c3c2-49e4-b419-9ff799bf0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting Comprehensive Audio Compression Experiments\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize framework\n",
    "    framework = AudioCompressionFramework(\n",
    "        base_path=\"UrbanSound8K/\",\n",
    "        wavelet='db4',\n",
    "        dwt_level=4,\n",
    "        sample_rate=22050,\n",
    "        duration=4.0,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Load dataset (limit to 50 files per class for initial testing)\n",
    "    print(\"\\n📂 Loading dataset...\")\n",
    "    file_paths, labels = framework.load_urbansound_dataset(max_files_per_class=50)\n",
    "    \n",
    "    print(f\"\\n📊 Dataset Statistics:\")\n",
    "    print(f\"  Total files: {len(file_paths)}\")\n",
    "    print(f\"  Classes: {len(set(labels))}\")\n",
    "    print(f\"  Sample rate: {framework.sample_rate} Hz\")\n",
    "    print(f\"  Duration: {framework.duration} seconds\")\n",
    "    print(f\"  Expected audio length: {int(framework.duration * framework.sample_rate)} samples\")\n",
    "    \n",
    "    # Run experiments\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🏃 Running Experiments...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    results_df = framework.run_experiment_pipeline(\n",
    "        file_paths, \n",
    "        labels,\n",
    "        test_size=0.2,\n",
    "        max_files_per_class=None  # Use all loaded files\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n✅ All experiments completed in {total_time/3600:.2f} hours\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\n🎨 Generating visualizations...\")\n",
    "    visualizer = ComprehensiveVisualizer(results_df)\n",
    "    visualizer.create_all_visualizations()\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\n📋 Generating summary report...\")\n",
    "    report_generator = SummaryReportGenerator(results_df)\n",
    "    report_generator.generate_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 EXPERIMENT COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n📁 Output files:\")\n",
    "    print(\"  - results/compression_experiments_results.csv\")\n",
    "    print(\"  - results/experiment_details.json\")\n",
    "    print(\"  - results/experiment_report.txt\")\n",
    "    print(\"  - results/feature_dimensions.csv\")\n",
    "    print(\"  - visualizations/*.png\")\n",
    "    print(\"  - checkpoints/*.pkl\")\n",
    "    print(\"\\n📊 Next steps:\")\n",
    "    print(\"  1. Review the visualizations in the 'visualizations/' folder\")\n",
    "    print(\"  2. Read the detailed report in 'results/experiment_report.txt'\")\n",
    "    print(\"  3. Analyze trade-offs for your specific application\")\n",
    "    print(\"  4. Run with full dataset if results are promising\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff2fb5-f94d-4903-a5d5-95a2d6c0959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test function (for debugging)\n",
    "def quick_test():\n",
    "    \"\"\"Quick test with minimal data\"\"\"\n",
    "    print(\"Running quick test with 10 files per class...\")\n",
    "    \n",
    "    framework = AudioCompressionFramework()\n",
    "    file_paths, labels = framework.load_urbansound_dataset(max_files_per_class=10)\n",
    "    \n",
    "    # Just test one configuration\n",
    "    print(\"\\nTesting Bernoulli 50% compression...\")\n",
    "    X_train, y_train, info = framework.prepare_features(\n",
    "        file_paths[:20], labels[:20],\n",
    "        feature_type='compressed',\n",
    "        compression_ratio=0.5,\n",
    "        matrix_type='bernoulli'\n",
    "    )\n",
    "    \n",
    "    X_test, y_test, _ = framework.prepare_features(\n",
    "        file_paths[20:30], labels[20:30],\n",
    "        feature_type='compressed',\n",
    "        compression_ratio=0.5,\n",
    "        matrix_type='bernoulli'\n",
    "    )\n",
    "    \n",
    "    results, models = framework.train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, info,\n",
    "        classifier_names=['SVM']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuick test results: Accuracy = {results[0]['accuracy']:.4f}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment for quick testing\n",
    "    # results = quick_test()\n",
    "    \n",
    "    # Run full experiments\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
