{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4fba2-4bd1-49df-91cf-c9211316b4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ COMPRESSED SENSING AUDIO CLASSIFICATION EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "üöÄ Starting experiment...\n",
      "================================================================================\n",
      "RUNNING STRATIFIED COMPRESSION EXPERIMENT\n",
      "================================================================================\n",
      "Loading dataset...\n",
      "Loaded 200 files, 10 classes\n",
      "Classes: ['air_conditioner' 'car_horn' 'children_playing' 'dog_bark' 'drilling'\n",
      " 'engine_idling' 'gun_shot' 'jackhammer' 'siren' 'street_music']\n",
      "Total files: 200\n",
      "\n",
      "üî¨ Experiment: original\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:14<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features: (200, 13)\n",
      "  ‚úì Accuracy: 0.3250 (¬±0.0736)\n",
      "  ‚úì Features: 13\n",
      "\n",
      "üî¨ Experiment: bernoulli_50\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [1:57:58<00:00, 35.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úó Error: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dim...\n",
      "\n",
      "üî¨ Experiment: bernoulli_60\n",
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                             | 60/200 [1:09:13<2:06:59, 54.42s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class FixedCompressionFramework:\n",
    "    \"\"\"Fixed framework with proper stratified sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, wavelet='db4', dwt_level=3, sample_rate=16000, duration=2.0, random_seed=42):\n",
    "        self.wavelet = wavelet\n",
    "        self.dwt_level = dwt_level\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.random_seed = random_seed\n",
    "        self.scaler = StandardScaler()\n",
    "        self.failed_files = []\n",
    "        \n",
    "        # Create output directories\n",
    "        self.create_directories()\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        directories = ['results', 'visualizations']\n",
    "        for dir_name in directories:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "    \n",
    "    def load_dataset(self, n_files=100):\n",
    "        \"\"\"Load dataset with proper sampling\"\"\"\n",
    "        print(\"Loading dataset...\")\n",
    "        \n",
    "        base_path = \"UrbanSound8K/\"\n",
    "        metadata_path = os.path.join(base_path, \"metadata/UrbanSound8K.csv\")\n",
    "        \n",
    "        if os.path.exists(metadata_path):\n",
    "            metadata = pd.read_csv(metadata_path)\n",
    "            \n",
    "            # Ensure balanced sampling across classes\n",
    "            file_paths = []\n",
    "            labels = []\n",
    "            \n",
    "            # Get unique classes\n",
    "            unique_classes = metadata['class'].unique()\n",
    "            \n",
    "            # Sample n_files per class (or as many as available)\n",
    "            files_per_class = max(1, n_files // len(unique_classes))\n",
    "            \n",
    "            for class_name in unique_classes:\n",
    "                class_files = metadata[metadata['class'] == class_name]\n",
    "                \n",
    "                # Take min(files_per_class, available_files)\n",
    "                sample_size = min(files_per_class, len(class_files))\n",
    "                if sample_size > 0:\n",
    "                    sampled_files = class_files.sample(sample_size, random_state=self.random_seed)\n",
    "                    \n",
    "                    for _, row in sampled_files.iterrows():\n",
    "                        fold = row['fold']\n",
    "                        filename = row['slice_file_name']\n",
    "                        file_path = os.path.join(base_path, f\"audio/fold{fold}\", filename)\n",
    "                        file_path = file_path.replace('\\\\', '/')\n",
    "                        \n",
    "                        if os.path.exists(file_path):\n",
    "                            file_paths.append(file_path)\n",
    "                            labels.append(class_name)\n",
    "                        else:\n",
    "                            # Try alternative path\n",
    "                            alt_path = os.path.join(base_path, f\"fold{fold}\", filename)\n",
    "                            alt_path = alt_path.replace('\\\\', '/')\n",
    "                            if os.path.exists(alt_path):\n",
    "                                file_paths.append(alt_path)\n",
    "                                labels.append(class_name)\n",
    "            \n",
    "            print(f\"Loaded {len(file_paths)} files, {len(set(labels))} classes\")\n",
    "            return file_paths, labels\n",
    "        \n",
    "        else:\n",
    "            print(\"UrbanSound8K not found, creating synthetic dataset...\")\n",
    "            return self.create_synthetic_dataset(n_files)\n",
    "    \n",
    "    def create_synthetic_dataset(self, n_files=100):\n",
    "        \"\"\"Create synthetic audio files\"\"\"\n",
    "        print(f\"Creating {n_files} synthetic audio files...\")\n",
    "        \n",
    "        temp_dir = \"synthetic_audio\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        class_names = ['sine', 'noise', 'chirp', 'silence', 'mixed']\n",
    "        sr = self.sample_rate\n",
    "        duration = self.duration\n",
    "        \n",
    "        files_per_class = max(1, n_files // len(class_names))\n",
    "        \n",
    "        for class_idx, label in enumerate(class_names):\n",
    "            for i in range(files_per_class):\n",
    "                if label == 'sine':\n",
    "                    t = np.linspace(0, duration, int(sr * duration))\n",
    "                    freq = 440 + (class_idx * 100) + (i * 10)\n",
    "                    audio = 0.5 * np.sin(2 * np.pi * freq * t)\n",
    "                elif label == 'noise':\n",
    "                    audio = np.random.randn(int(sr * duration)) * 0.1\n",
    "                elif label == 'chirp':\n",
    "                    t = np.linspace(0, duration, int(sr * duration))\n",
    "                    freq = 100 + 500 * t + (class_idx * 100)\n",
    "                    audio = 0.3 * np.sin(2 * np.pi * freq * t)\n",
    "                elif label == 'silence':\n",
    "                    audio = np.zeros(int(sr * duration))\n",
    "                else:  # mixed\n",
    "                    t = np.linspace(0, duration, int(sr * duration))\n",
    "                    audio = 0.2 * np.sin(2 * np.pi * 440 * t) + \\\n",
    "                            0.1 * np.random.randn(int(sr * duration))\n",
    "                \n",
    "                filename = f\"{temp_dir}/audio_{label}_{class_idx}_{i}.wav\"\n",
    "                wavfile.write(filename, sr, (audio * 32767).astype(np.int16))\n",
    "                \n",
    "                file_paths.append(filename)\n",
    "                labels.append(label)\n",
    "        \n",
    "        print(f\"Created {len(file_paths)} synthetic files\")\n",
    "        return file_paths, labels\n",
    "    \n",
    "    def extract_features(self, audio, sr):\n",
    "        \"\"\"Extract robust features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. Basic statistics\n",
    "        features.append(np.mean(audio))\n",
    "        features.append(np.std(audio))\n",
    "        features.append(np.max(np.abs(audio)))\n",
    "        features.append(np.min(audio))\n",
    "        \n",
    "        # 2. Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio, frame_length=2048, hop_length=512)\n",
    "        features.append(np.mean(zcr))\n",
    "        features.append(np.std(zcr))\n",
    "        \n",
    "        # 3. Energy\n",
    "        energy = np.sum(audio ** 2)\n",
    "        features.append(energy)\n",
    "        \n",
    "        # 4. Simple spectral features\n",
    "        if len(audio) > 10 and np.any(audio != 0):\n",
    "            try:\n",
    "                # Simple FFT\n",
    "                fft = np.fft.fft(audio)\n",
    "                magnitude = np.abs(fft)\n",
    "                \n",
    "                # Take first half (positive frequencies)\n",
    "                half_len = len(magnitude) // 2\n",
    "                if half_len > 0:\n",
    "                    pos_mag = magnitude[:half_len]\n",
    "                    \n",
    "                    features.append(np.mean(pos_mag))\n",
    "                    features.append(np.std(pos_mag))\n",
    "                    features.append(np.max(pos_mag))\n",
    "                    \n",
    "                    # Simple spectral centroid\n",
    "                    if np.sum(pos_mag) > 0:\n",
    "                        freqs = np.fft.fftfreq(len(audio), 1/sr)[:half_len]\n",
    "                        centroid = np.sum(freqs * pos_mag) / np.sum(pos_mag)\n",
    "                        features.append(centroid)\n",
    "                    else:\n",
    "                        features.append(0.0)\n",
    "                else:\n",
    "                    features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "                    \n",
    "            except:\n",
    "                features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "        else:\n",
    "            features.extend([0.0, 0.0, 0.0, 0.0])\n",
    "        \n",
    "        # Ensure fixed size\n",
    "        target_size = 13\n",
    "        if len(features) < target_size:\n",
    "            features.extend([0.0] * (target_size - len(features)))\n",
    "        \n",
    "        return np.array(features[:target_size])\n",
    "    \n",
    "    def create_measurement_matrix(self, M, N, matrix_type):\n",
    "        \"\"\"Create measurement matrix\"\"\"\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        if matrix_type == 'bernoulli':\n",
    "            return np.random.choice([-1, 1], size=(M, N)) / np.sqrt(M)\n",
    "        elif matrix_type == 'gaussian':\n",
    "            return np.random.randn(M, N) / np.sqrt(M)\n",
    "        else:  # logistic_henon\n",
    "            r = 3.99\n",
    "            a, b = 1.4, 0.3\n",
    "            x_log = np.random.rand()\n",
    "            x_hen, y_hen = np.random.rand(), np.random.rand()\n",
    "            \n",
    "            matrix = np.zeros((M, N))\n",
    "            for i in range(M):\n",
    "                for j in range(N):\n",
    "                    # Logistic map\n",
    "                    x_log = r * x_log * (1 - x_log)\n",
    "                    \n",
    "                    # Henon map\n",
    "                    x_hen_new = 1 - a * x_hen**2 + y_hen\n",
    "                    y_hen = b * x_hen\n",
    "                    x_hen = x_hen_new\n",
    "                    \n",
    "                    # Combine\n",
    "                    chaotic = (x_log + x_hen) / 2\n",
    "                    matrix[i, j] = 2 * chaotic - 1\n",
    "                \n",
    "                # Reset occasionally\n",
    "                if i % 100 == 0:\n",
    "                    x_log = (x_log + np.random.rand()) / 2\n",
    "                    x_hen = (x_hen + np.random.rand()) / 2\n",
    "            \n",
    "            return matrix / np.sqrt(M)\n",
    "    \n",
    "    def compress_audio(self, audio, compression_ratio=0.5, matrix_type='bernoulli'):\n",
    "        \"\"\"Compress audio using compressed sensing\"\"\"\n",
    "        # Apply DWT\n",
    "        try:\n",
    "            coeffs = pywt.wavedec(audio, self.wavelet, level=self.dwt_level)\n",
    "            coeffs_flat = np.concatenate(coeffs)\n",
    "        except:\n",
    "            # If DWT fails, use raw audio (truncated)\n",
    "            coeffs_flat = audio[:min(len(audio), 1000)]\n",
    "        \n",
    "        N = len(coeffs_flat)\n",
    "        M = max(10, int(N * compression_ratio))  # Minimum 10 measurements\n",
    "        \n",
    "        # Generate measurement matrix\n",
    "        Phi = self.create_measurement_matrix(M, N, matrix_type)\n",
    "        \n",
    "        # Compress\n",
    "        compressed = Phi @ coeffs_flat\n",
    "        \n",
    "        # Add statistics\n",
    "        features = np.concatenate([\n",
    "            compressed,\n",
    "            [\n",
    "                np.mean(compressed),\n",
    "                np.std(compressed),\n",
    "                np.max(np.abs(compressed)),\n",
    "                M / N  # Actual compression ratio\n",
    "            ]\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_all_features(self, file_paths, labels, compression_ratio=None, matrix_type=None):\n",
    "        \"\"\"Prepare features with proper error handling\"\"\"\n",
    "        print(\"Extracting features...\")\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for file_path, label in tqdm(zip(file_paths, labels), total=len(file_paths), desc=\"Processing\"):\n",
    "            try:\n",
    "                # Load audio\n",
    "                audio, sr = librosa.load(file_path, sr=self.sample_rate, \n",
    "                                        duration=self.duration, mono=True)\n",
    "                \n",
    "                # Extract features\n",
    "                if matrix_type is None:  # Original features\n",
    "                    features = self.extract_features(audio, sr)\n",
    "                else:  # Compressed features\n",
    "                    features = self.compress_audio(audio, compression_ratio, matrix_type)\n",
    "                \n",
    "                features_list.append(features)\n",
    "                labels_list.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.failed_files.append((file_path, str(e)))\n",
    "                # Use zero features as fallback\n",
    "                if matrix_type is None:\n",
    "                    features_list.append(np.zeros(13))\n",
    "                else:\n",
    "                    features_list.append(np.zeros(50 + 4))  # 50 compressed + 4 stats\n",
    "                labels_list.append(label)\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        print(f\"Extracted features: {X.shape}\")\n",
    "        return X, y\n",
    "    \n",
    "    def run_stratified_experiment(self):\n",
    "        \"\"\"Run experiment with proper stratified sampling\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"RUNNING STRATIFIED COMPRESSION EXPERIMENT\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load balanced dataset\n",
    "        file_paths, labels = self.load_dataset(200)\n",
    "        \n",
    "        if len(file_paths) < 20:\n",
    "            print(\"Warning: Small dataset, results may vary\")\n",
    "        \n",
    "        # Get unique classes\n",
    "        unique_classes = np.unique(labels)\n",
    "        print(f\"Classes: {unique_classes}\")\n",
    "        print(f\"Total files: {len(file_paths)}\")\n",
    "        \n",
    "        # Experiment configurations\n",
    "        experiments = [\n",
    "            {'name': 'original', 'matrix_type': None, 'compression_ratio': None},\n",
    "            {'name': 'bernoulli_50', 'matrix_type': 'bernoulli', 'compression_ratio': 0.5},\n",
    "            {'name': 'bernoulli_60', 'matrix_type': 'bernoulli', 'compression_ratio': 0.6},\n",
    "            {'name': 'bernoulli_70', 'matrix_type': 'bernoulli', 'compression_ratio': 0.7},\n",
    "            {'name': 'gaussian_50', 'matrix_type': 'gaussian', 'compression_ratio': 0.5},\n",
    "            {'name': 'gaussian_60', 'matrix_type': 'gaussian', 'compression_ratio': 0.6},\n",
    "            {'name': 'gaussian_70', 'matrix_type': 'gaussian', 'compression_ratio': 0.7},\n",
    "            {'name': 'logistic_henon_50', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.5},\n",
    "            {'name': 'logistic_henon_60', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.6},\n",
    "            {'name': 'logistic_henon_70', 'matrix_type': 'logistic_henon', 'compression_ratio': 0.7},\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for exp in experiments:\n",
    "            print(f\"\\nüî¨ Experiment: {exp['name']}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract features\n",
    "                X, y = self.prepare_all_features(\n",
    "                    file_paths, labels,\n",
    "                    compression_ratio=exp['compression_ratio'],\n",
    "                    matrix_type=exp['matrix_type']\n",
    "                )\n",
    "                \n",
    "                # Encode labels once for this experiment\n",
    "                le = LabelEncoder()\n",
    "                y_encoded = le.fit_transform(y)\n",
    "                \n",
    "                # Stratified split\n",
    "                if len(np.unique(y_encoded)) > 1:\n",
    "                    # Use multiple splits for robustness\n",
    "                    accuracies = []\n",
    "                    feature_dims = []\n",
    "                    \n",
    "                    for split_seed in range(3):  # 3 different splits\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X, y_encoded, \n",
    "                            test_size=0.2,\n",
    "                            stratify=y_encoded,\n",
    "                            random_state=split_seed\n",
    "                        )\n",
    "                        \n",
    "                        # Scale features\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train_scaled = scaler.fit_transform(X_train)\n",
    "                        X_test_scaled = scaler.transform(X_test)\n",
    "                        \n",
    "                        # Train Random Forest\n",
    "                        clf = RandomForestClassifier(\n",
    "                            n_estimators=100,\n",
    "                            random_state=self.random_seed,\n",
    "                            n_jobs=-1\n",
    "                        )\n",
    "                        clf.fit(X_train_scaled, y_train)\n",
    "                        \n",
    "                        # Predict and evaluate\n",
    "                        y_pred = clf.predict(X_test_scaled)\n",
    "                        accuracy = accuracy_score(y_test, y_pred)\n",
    "                        \n",
    "                        accuracies.append(accuracy)\n",
    "                        feature_dims.append(X.shape[1])\n",
    "                    \n",
    "                    # Average results\n",
    "                    avg_accuracy = np.mean(accuracies)\n",
    "                    avg_features = np.mean(feature_dims)\n",
    "                    \n",
    "                    result = {\n",
    "                        'experiment': exp['name'],\n",
    "                        'matrix_type': exp['matrix_type'],\n",
    "                        'compression_ratio': exp['compression_ratio'],\n",
    "                        'accuracy': avg_accuracy,\n",
    "                        'accuracy_std': np.std(accuracies),\n",
    "                        'features': avg_features,\n",
    "                        'samples': len(X)\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"  ‚úì Accuracy: {avg_accuracy:.4f} (¬±{np.std(accuracies):.4f})\")\n",
    "                    print(f\"  ‚úì Features: {avg_features:.0f}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"  ‚úó Only one class in dataset\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        # Save results\n",
    "        if results:\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "            \n",
    "            results_file = 'results/compression_results_final.csv'\n",
    "            results_df.to_csv(results_file, index=False)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Results saved to: {results_file}\")\n",
    "            \n",
    "            # Create visualizations\n",
    "            self.create_visualizations(results_df)\n",
    "            \n",
    "            # Print summary\n",
    "            self.print_summary(results_df)\n",
    "            \n",
    "            return results_df\n",
    "        else:\n",
    "            print(\"\\n‚ùå No valid results obtained\")\n",
    "            return None\n",
    "    \n",
    "    def create_visualizations(self, results_df):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        print(\"\\nüìä Creating visualizations...\")\n",
    "        \n",
    "        # Filter out original for compressed comparisons\n",
    "        compressed_df = results_df[results_df['matrix_type'].notnull()].copy()\n",
    "        original_acc = results_df[results_df['experiment'] == 'original']['accuracy'].values\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # Plot 1: All experiments comparison\n",
    "        ax = axes[0, 0]\n",
    "        x_pos = np.arange(len(results_df))\n",
    "        bars = ax.bar(x_pos, results_df['accuracy'], \n",
    "                     yerr=results_df.get('accuracy_std', 0),\n",
    "                     alpha=0.7, capsize=5)\n",
    "        \n",
    "        # Color by matrix type\n",
    "        colors = {'bernoulli': 'blue', 'gaussian': 'green', \n",
    "                 'logistic_henon': 'red', None: 'black'}\n",
    "        for bar, exp in zip(bars, results_df['experiment']):\n",
    "            for matrix, color in colors.items():\n",
    "                if matrix and exp.startswith(matrix):\n",
    "                    bar.set_color(color)\n",
    "                    break\n",
    "            else:\n",
    "                bar.set_color('black')\n",
    "        \n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(results_df['experiment'], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('All Experiments Comparison')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Matrix type comparison\n",
    "        ax = axes[0, 1]\n",
    "        if len(compressed_df) > 0:\n",
    "            matrix_avg = compressed_df.groupby('matrix_type')['accuracy'].mean()\n",
    "            matrix_std = compressed_df.groupby('matrix_type')['accuracy'].std()\n",
    "            \n",
    "            x_pos = np.arange(len(matrix_avg))\n",
    "            bars = ax.bar(x_pos, matrix_avg, yerr=matrix_std,\n",
    "                         alpha=0.7, capsize=5,\n",
    "                         color=['blue', 'green', 'red'])\n",
    "            \n",
    "            if len(original_acc) > 0:\n",
    "                ax.axhline(y=original_acc[0], color='black', linestyle='--', \n",
    "                          label='Original', linewidth=2)\n",
    "            \n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(matrix_avg.index)\n",
    "            ax.set_ylabel('Average Accuracy')\n",
    "            ax.set_title('Matrix Type Comparison')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Compression ratio effect\n",
    "        ax = axes[0, 2]\n",
    "        if len(compressed_df) > 0:\n",
    "            for matrix_type in compressed_df['matrix_type'].unique():\n",
    "                matrix_data = compressed_df[compressed_df['matrix_type'] == matrix_type]\n",
    "                ax.plot(matrix_data['compression_ratio'], matrix_data['accuracy'],\n",
    "                       marker='o', linewidth=2, label=matrix_type,\n",
    "                       color=colors[matrix_type])\n",
    "            \n",
    "            if len(original_acc) > 0:\n",
    "                ax.axhline(y=original_acc[0], color='black', linestyle='--',\n",
    "                          label='Original', linewidth=2)\n",
    "            \n",
    "            ax.set_xlabel('Compression Ratio')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Effect of Compression Ratio')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Feature dimension vs accuracy\n",
    "        ax = axes[1, 0]\n",
    "        scatter = ax.scatter(results_df['features'], results_df['accuracy'],\n",
    "                           c=results_df['compression_ratio'].fillna(1.0),\n",
    "                           s=100, alpha=0.7, cmap='viridis')\n",
    "        \n",
    "        # Add labels for compressed experiments\n",
    "        for _, row in compressed_df.iterrows():\n",
    "            ax.annotate(f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\",\n",
    "                       (row['features'], row['accuracy']),\n",
    "                       fontsize=8, ha='center')\n",
    "        \n",
    "        ax.set_xlabel('Feature Dimension')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Feature Dimension vs Accuracy')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax, label='Compression Ratio')\n",
    "        \n",
    "        # Plot 5: Best configurations\n",
    "        ax = axes[1, 1]\n",
    "        if len(compressed_df) > 0:\n",
    "            # Get top 5 compressed configurations\n",
    "            top5 = compressed_df.nlargest(5, 'accuracy')\n",
    "            \n",
    "            bars = ax.bar(range(len(top5)), top5['accuracy'],\n",
    "                         yerr=top5.get('accuracy_std', 0),\n",
    "                         alpha=0.7, capsize=5)\n",
    "            \n",
    "            # Color bars\n",
    "            for i, (_, row) in enumerate(top5.iterrows()):\n",
    "                bars[i].set_color(colors[row['matrix_type']])\n",
    "            \n",
    "            ax.set_xticks(range(len(top5)))\n",
    "            ax.set_xticklabels([f\"{row['matrix_type'][:3]}_{int(row['compression_ratio']*100)}%\"\n",
    "                              for _, row in top5.iterrows()], rotation=45, ha='right')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Top 5 Compressed Configurations')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, bar in enumerate(bars):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{bar.get_height():.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 6: Speed vs Accuracy trade-off (estimated)\n",
    "        ax = axes[1, 2]\n",
    "        if len(results_df) > 0:\n",
    "            # Estimate computational complexity\n",
    "            # Original features are fastest, compressed take longer\n",
    "            estimated_time = []\n",
    "            for _, row in results_df.iterrows():\n",
    "                if row['experiment'] == 'original':\n",
    "                    estimated_time.append(1.0)  # baseline\n",
    "                else:\n",
    "                    # More features = longer time\n",
    "                    time_factor = row['features'] / 13  # relative to original\n",
    "                    # Compressed sensing adds overhead\n",
    "                    if row['matrix_type'] == 'logistic_henon':\n",
    "                        time_factor *= 1.5  # chaotic maps are slower\n",
    "                    estimated_time.append(time_factor)\n",
    "            \n",
    "            results_df['estimated_time'] = estimated_time\n",
    "            \n",
    "            scatter = ax.scatter(results_df['estimated_time'], results_df['accuracy'],\n",
    "                               c=results_df['compression_ratio'].fillna(1.0),\n",
    "                               s=100, alpha=0.7, cmap='viridis')\n",
    "            \n",
    "            ax.set_xlabel('Estimated Relative Time')\n",
    "            ax.set_ylabel('Accuracy')\n",
    "            ax.set_title('Speed vs Accuracy Trade-off')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('visualizations/comprehensive_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úì Visualizations saved to: visualizations/comprehensive_results.png\")\n",
    "    \n",
    "    def print_summary(self, results_df):\n",
    "        \"\"\"Print comprehensive summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Original performance\n",
    "        original_row = results_df[results_df['experiment'] == 'original']\n",
    "        if len(original_row) > 0:\n",
    "            print(f\"\\nüìà ORIGINAL FEATURES:\")\n",
    "            print(f\"  Accuracy: {original_row['accuracy'].values[0]:.4f}\")\n",
    "            print(f\"  Features: {original_row['features'].values[0]:.0f}\")\n",
    "        \n",
    "        # Best compressed\n",
    "        compressed_df = results_df[results_df['matrix_type'].notnull()]\n",
    "        if len(compressed_df) > 0:\n",
    "            best_compressed = compressed_df.loc[compressed_df['accuracy'].idxmax()]\n",
    "            \n",
    "            print(f\"\\nüèÜ BEST COMPRESSED CONFIGURATION:\")\n",
    "            print(f\"  Matrix: {best_compressed['matrix_type'].capitalize()}\")\n",
    "            print(f\"  Compression: {best_compressed['compression_ratio']*100:.0f}%\")\n",
    "            print(f\"  Accuracy: {best_compressed['accuracy']:.4f}\")\n",
    "            print(f\"  Features: {best_compressed['features']:.0f}\")\n",
    "            \n",
    "            if len(original_row) > 0:\n",
    "                orig_acc = original_row['accuracy'].values[0]\n",
    "                comp_acc = best_compressed['accuracy']\n",
    "                accuracy_drop = orig_acc - comp_acc\n",
    "                feature_reduction = 1 - (best_compressed['features'] / original_row['features'].values[0])\n",
    "                \n",
    "                print(f\"  Accuracy Drop: {accuracy_drop:.4f}\")\n",
    "                print(f\"  Feature Reduction: {feature_reduction*100:.1f}%\")\n",
    "        \n",
    "        # Best by matrix type\n",
    "        print(f\"\\nüìä BEST BY MATRIX TYPE:\")\n",
    "        for matrix_type in compressed_df['matrix_type'].unique():\n",
    "            matrix_data = compressed_df[compressed_df['matrix_type'] == matrix_type]\n",
    "            best_matrix = matrix_data.loc[matrix_data['accuracy'].idxmax()]\n",
    "            print(f\"  {matrix_type.capitalize()}: {best_matrix['compression_ratio']*100:.0f}% \"\n",
    "                  f\"(Acc: {best_matrix['accuracy']:.4f})\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "        if len(compressed_df) > 0:\n",
    "            # Find configurations with < 10% accuracy drop\n",
    "            if len(original_row) > 0:\n",
    "                orig_acc = original_row['accuracy'].values[0]\n",
    "                good_configs = compressed_df[compressed_df['accuracy'] >= orig_acc * 0.9]\n",
    "                \n",
    "                if len(good_configs) > 0:\n",
    "                    print(\"  Configurations with <10% accuracy drop:\")\n",
    "                    for _, row in good_configs.nlargest(3, 'accuracy').iterrows():\n",
    "                        print(f\"    ‚Ä¢ {row['matrix_type']} {row['compression_ratio']*100:.0f}%: \"\n",
    "                              f\"Acc={row['accuracy']:.4f}, Features={row['features']:.0f}\")\n",
    "                else:\n",
    "                    print(\"  No configurations with <10% accuracy drop found.\")\n",
    "            \n",
    "            # Fastest configuration\n",
    "            fastest = compressed_df.loc[compressed_df['features'].idxmin()]\n",
    "            print(f\"  Fastest (fewest features): {fastest['matrix_type']} \"\n",
    "                  f\"{fastest['compression_ratio']*100:.0f}%\")\n",
    "        \n",
    "        print(f\"\\nüìÅ Output files:\")\n",
    "        print(f\"  - results/compression_results_final.csv\")\n",
    "        print(f\"  - visualizations/comprehensive_results.png\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    print(\"üéØ COMPRESSED SENSING AUDIO CLASSIFICATION EXPERIMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize framework\n",
    "    framework = FixedCompressionFramework(\n",
    "        wavelet='db4',\n",
    "        dwt_level=3,\n",
    "        sample_rate=22050,\n",
    "        duration=3.0,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüöÄ Starting experiment...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run the complete experiment\n",
    "    results = framework.run_stratified_experiment()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"‚úÖ EXPERIMENT COMPLETED IN {total_time/60:.1f} MINUTES!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show quick results\n",
    "        print(\"\\nüéØ QUICK RESULTS:\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        original_acc = results[results['experiment'] == 'original']['accuracy'].values\n",
    "        if len(original_acc) > 0:\n",
    "            print(f\"Original: {original_acc[0]:.4f}\")\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            best_compressed = results[results['experiment'] != 'original'].nlargest(1, 'accuracy')\n",
    "            if len(best_compressed) > 0:\n",
    "                row = best_compressed.iloc[0]\n",
    "                print(f\"Best Compressed: {row['experiment']} = {row['accuracy']:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ùå Experiment failed to produce results\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ All done! Check the 'results' and 'visualizations' folders.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19a6de-dbf8-42b3-88f4-76536caf459b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
