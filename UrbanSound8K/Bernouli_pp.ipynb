{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2cd1e-9134-46d5-9325-33bcf4b5295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Compression-Classification Pipeline\n",
      "======================================================================\n",
      "Running quick compression test...\n",
      "Original signal shape: (1024,)\n",
      "\n",
      "1. Bernoulli Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "2. DWT (Haar) Compression (50%):\n",
      "   Compressed coefficients: 512\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "3. Hybrid Compression (50%):\n",
      "   Compressed shape: (512,)\n",
      "   Compression: 50.0% of original\n",
      "\n",
      "Quick test completed successfully!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want to run full compression-classification experiments? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\n",
      "======================================================================\n",
      "======================================================================\n",
      "COMPRESSION-CLASSIFICATION EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "Preparing data...\n",
      "Found 7079 valid audio files out of 6448 expected\n",
      "Found 1653 valid audio files out of 1674 expected\n",
      "\n",
      "============================================================\n",
      "Compression Method: BERNOULLI\n",
      "============================================================\n",
      "\n",
      "Compression Ratio: 25%\n",
      "----------------------------------------\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train bernoulli 0.25:   5%|██▊                                                      | 345/7079 [00:50<13:13,  8.48it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "from scipy.fft import dct, idct\n",
    "from scipy.sparse.linalg import lsqr\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           roc_auc_score, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score)\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Enhanced Feature Extraction (Base Class)\n",
    "# ============================================\n",
    "\n",
    "class RobustMFCCExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced MFCC feature extraction with multiple robust techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=40, n_fft=2048, hop_length=512):\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def extract_mfcc_features(self, audio_path):\n",
    "        \"\"\"\n",
    "        Extract comprehensive MFCC-based features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with robust loading\n",
    "            y, sr = librosa.load(audio_path, sr=self.sr, duration=4.0)\n",
    "            \n",
    "            # Zero-padding or truncation for consistent length\n",
    "            target_length = self.sr * 4  # 4 seconds\n",
    "            if len(y) < target_length:\n",
    "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:target_length]\n",
    "            \n",
    "            # Apply pre-emphasis filter\n",
    "            y = librosa.effects.preemphasis(y)\n",
    "            \n",
    "            # Extract base MFCCs\n",
    "            mfccs = librosa.feature.mfcc(\n",
    "                y=y, \n",
    "                sr=sr, \n",
    "                n_mfcc=self.n_mfcc,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length\n",
    "            )\n",
    "            \n",
    "            # Extract delta and delta-delta features\n",
    "            mfcc_delta = librosa.feature.delta(mfccs)\n",
    "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "            \n",
    "            # Extract other complementary features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Root Mean Square Energy\n",
    "            rms = librosa.feature.rms(y=y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Zero Crossing Rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y, hop_length=self.hop_length)\n",
    "            \n",
    "            # Spectral Centroid and Rolloff\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.vstack([\n",
    "                mfccs,\n",
    "                mfcc_delta,\n",
    "                mfcc_delta2,\n",
    "                chroma,\n",
    "                spectral_contrast,\n",
    "                rms,\n",
    "                zcr,\n",
    "                spectral_centroid,\n",
    "                spectral_rolloff\n",
    "            ])\n",
    "            \n",
    "            # Extract statistical features\n",
    "            feature_stats = self._extract_statistics(features)\n",
    "            \n",
    "            return feature_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            # Return zeros with appropriate dimension\n",
    "            return np.zeros(1026)  # Fixed dimension\n",
    "    \n",
    "    def _extract_statistics(self, features):\n",
    "        \"\"\"\n",
    "        Extract statistical features from feature matrix\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        for feature in features:\n",
    "            stats.extend([\n",
    "                np.mean(feature),\n",
    "                np.std(feature),\n",
    "                np.median(feature),\n",
    "                np.min(feature),\n",
    "                np.max(feature),\n",
    "                np.percentile(feature, 25),\n",
    "                np.percentile(feature, 75),\n",
    "                np.mean(np.diff(feature)),  # Average change\n",
    "                np.std(np.diff(feature))    # Std of changes\n",
    "            ])\n",
    "        return np.array(stats)\n",
    "    \n",
    "    def extract_all_features(self, audio_paths, labels=None):\n",
    "        \"\"\"\n",
    "        Extract features from multiple audio files\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        valid_labels = []\n",
    "        \n",
    "        for idx, path in enumerate(tqdm(audio_paths, desc=\"Extracting features\")):\n",
    "            feat = self.extract_mfcc_features(path)\n",
    "            if np.sum(feat) != 0 and not np.isnan(feat).any():  # Skip zero or NaN features\n",
    "                features.append(feat)\n",
    "                if labels is not None:\n",
    "                    valid_labels.append(labels[idx])\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"WARNING: No valid features extracted!\")\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        features = np.array(features)\n",
    "        \n",
    "        # Normalize features\n",
    "        features = self.scaler.fit_transform(features)\n",
    "        \n",
    "        # Encode labels if provided\n",
    "        if labels is not None and len(valid_labels) > 0:\n",
    "            encoded_labels = self.label_encoder.fit_transform(valid_labels)\n",
    "        else:\n",
    "            encoded_labels = None\n",
    "        \n",
    "        return features, encoded_labels\n",
    "\n",
    "# ============================================\n",
    "# 2. UrbanSound8K Processor\n",
    "# ============================================\n",
    "\n",
    "class UrbanSound8KProcessor:\n",
    "    \"\"\"\n",
    "    Process UrbanSound8K dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.metadata_path = os.path.join(dataset_path, 'metadata', 'UrbanSound8K.csv')\n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            # Try alternative path structure\n",
    "            self.metadata_path = os.path.join(dataset_path, 'UrbanSound8K.csv')\n",
    "        \n",
    "        if not os.path.exists(self.metadata_path):\n",
    "            raise FileNotFoundError(f\"Metadata file not found at {self.metadata_path}\")\n",
    "        \n",
    "        self.metadata = pd.read_csv(self.metadata_path)\n",
    "        \n",
    "    def prepare_data(self, folds=None):\n",
    "        \"\"\"\n",
    "        Prepare data for specific folds or all folds\n",
    "        \"\"\"\n",
    "        if folds is None:\n",
    "            folds = list(range(1, 11))\n",
    "        elif isinstance(folds, int):\n",
    "            folds = [folds]\n",
    "        \n",
    "        audio_paths = []\n",
    "        labels = []\n",
    "        fold_numbers = []\n",
    "        \n",
    "        for fold in folds:\n",
    "            fold_data = self.metadata[self.metadata['fold'] == fold]\n",
    "            \n",
    "            for _, row in fold_data.iterrows():\n",
    "                # Try different possible paths\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.dataset_path, 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, 'audio', 'fold' + str(row['fold']), row['slice_file_name']),\n",
    "                    os.path.join(self.dataset_path, str(row['fold']), row['slice_file_name'])\n",
    "                ]\n",
    "                \n",
    "                audio_file = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        audio_file = path\n",
    "                        break\n",
    "                \n",
    "                if audio_file:\n",
    "                    audio_paths.append(audio_file)\n",
    "                    labels.append(row['class'])\n",
    "                    fold_numbers.append(fold)\n",
    "                else:\n",
    "                    print(f\"WARNING: File not found: {row['slice_file_name']} in fold {fold}\")\n",
    "        \n",
    "        print(f\"Found {len(audio_paths)} valid audio files out of {len(fold_data) * len(folds)} expected\")\n",
    "        return audio_paths, labels, fold_numbers\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"\n",
    "        Get class distribution statistics\n",
    "        \"\"\"\n",
    "        return self.metadata['class'].value_counts()\n",
    "\n",
    "# ============================================\n",
    "# 3. Compression Sensing Modules\n",
    "# ============================================\n",
    "\n",
    "class BernoulliCompressor:\n",
    "    \"\"\"Bernoulli Random Matrix Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize Bernoulli compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        seed : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.measurement_matrix = None\n",
    "        self.reconstruction_matrix = None\n",
    "        \n",
    "    def create_bernoulli_matrix(self, n_original, n_compressed):\n",
    "        \"\"\"\n",
    "        Create Bernoulli random measurement matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_original : int\n",
    "            Original signal dimension\n",
    "        n_compressed : int\n",
    "            Compressed dimension\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        measurement_matrix : np.array\n",
    "            Bernoulli random matrix\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Bernoulli matrix with entries +1/sqrt(n_compressed) and -1/sqrt(n_compressed)\n",
    "        bernoulli_values = np.random.choice([1, -1], size=(n_compressed, n_original))\n",
    "        bernoulli_matrix = bernoulli_values / np.sqrt(n_compressed)\n",
    "        return bernoulli_matrix\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using Bernoulli random matrix\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (1D array)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        \"\"\"\n",
    "        n_original = len(signal)\n",
    "        n_compressed = int(n_original * self.compression_ratio)\n",
    "        \n",
    "        # Create measurement matrix\n",
    "        self.measurement_matrix = self.create_bernoulli_matrix(n_original, n_compressed)\n",
    "        \n",
    "        # Compress signal\n",
    "        compressed_signal = np.dot(self.measurement_matrix, signal)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct_l1(self, compressed_signal, max_iter=100):\n",
    "        \"\"\"\n",
    "        Reconstruct original signal using L1 minimization (Basis Pursuit)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal\n",
    "        max_iter : int\n",
    "            Maximum iterations for reconstruction\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        if self.measurement_matrix is None:\n",
    "            raise ValueError(\"Measurement matrix not created. Run compress() first.\")\n",
    "        \n",
    "        n_original = self.measurement_matrix.shape[1]\n",
    "        \n",
    "        # Use least squares with L1 regularization (simplified)\n",
    "        reconstructed_signal = lsqr(self.measurement_matrix, compressed_signal, iter_lim=max_iter)[0]\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class DWTCompressor:\n",
    "    \"\"\"Discrete Wavelet Transform (Haar) Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, wavelet='haar'):\n",
    "        \"\"\"\n",
    "        Initialize DWT compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Ratio of compressed dimension to original dimension\n",
    "        wavelet : str\n",
    "            Wavelet type (default: 'haar')\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.wavelet = wavelet\n",
    "        \n",
    "    def haar_transform(self, signal):\n",
    "        \"\"\"\n",
    "        Apply Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal (length must be power of 2)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "        \"\"\"\n",
    "        n = len(signal)\n",
    "        coeffs = np.zeros_like(signal, dtype=float)\n",
    "        \n",
    "        # Simple Haar transform implementation\n",
    "        temp = signal.copy()\n",
    "        length = n\n",
    "        \n",
    "        while length > 1:\n",
    "            for i in range(length // 2):\n",
    "                avg = (temp[2*i] + temp[2*i+1]) / np.sqrt(2)\n",
    "                diff = (temp[2*i] - temp[2*i+1]) / np.sqrt(2)\n",
    "                coeffs[i] = avg\n",
    "                coeffs[length // 2 + i] = diff\n",
    "            temp[:length] = coeffs[:length]\n",
    "            length //= 2\n",
    "            \n",
    "        return coeffs\n",
    "    \n",
    "    def inverse_haar_transform(self, coeffs):\n",
    "        \"\"\"\n",
    "        Apply inverse Haar wavelet transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        coeffs : np.array\n",
    "            Wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        n = len(coeffs)\n",
    "        signal = coeffs.copy()\n",
    "        length = 2\n",
    "        \n",
    "        while length <= n:\n",
    "            temp = signal.copy()\n",
    "            for i in range(length // 2):\n",
    "                signal[2*i] = (temp[i] + temp[length // 2 + i]) / np.sqrt(2)\n",
    "                signal[2*i+1] = (temp[i] - temp[length // 2 + i]) / np.sqrt(2)\n",
    "            length *= 2\n",
    "            \n",
    "        return signal\n",
    "    \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Compress signal using DWT and thresholding\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Compressed signal (thresholded coefficients)\n",
    "        compression_mask : np.array\n",
    "            Mask indicating which coefficients were kept\n",
    "        \"\"\"\n",
    "        # Apply wavelet transform\n",
    "        coeffs = self.haar_transform(signal)\n",
    "        \n",
    "        # Keep only largest coefficients based on compression ratio\n",
    "        n_coeffs = len(coeffs)\n",
    "        n_keep = int(n_coeffs * self.compression_ratio)\n",
    "        \n",
    "        # Get indices of largest absolute coefficients\n",
    "        indices = np.argsort(np.abs(coeffs))[-n_keep:]\n",
    "        compression_mask = np.zeros(n_coeffs, dtype=bool)\n",
    "        compression_mask[indices] = True\n",
    "        \n",
    "        # Create compressed signal (only keep selected coefficients)\n",
    "        compressed_coeffs = np.zeros_like(coeffs)\n",
    "        compressed_coeffs[indices] = coeffs[indices]\n",
    "        \n",
    "        return compressed_coeffs, compression_mask\n",
    "    \n",
    "    def reconstruct(self, compressed_coeffs):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from compressed coefficients\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_coeffs : np.array\n",
    "            Compressed wavelet coefficients\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Apply inverse transform\n",
    "        reconstructed_signal = self.inverse_haar_transform(compressed_coeffs)\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "class HybridCompressor:\n",
    "    \"\"\"Hybrid Bernoulli + DWT Compression\"\"\"\n",
    "    \n",
    "    def __init__(self, compression_ratio=0.5, seed=42):\n",
    "        \"\"\"\n",
    "        Initialize hybrid compressor\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compression_ratio : float (0-1)\n",
    "            Overall compression ratio\n",
    "        seed : int\n",
    "            Random seed for Bernoulli matrix\n",
    "        \"\"\"\n",
    "        self.compression_ratio = compression_ratio\n",
    "        self.seed = seed\n",
    "        self.bernoulli_compressor = BernoulliCompressor(compression_ratio, seed)\n",
    "        self.dwt_compressor = DWTCompressor(1.0)  # No compression in DWT stage\n",
    "        \n",
    "    def compress(self, signal):\n",
    "        \"\"\"\n",
    "        Apply DWT then Bernoulli compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : np.array\n",
    "            Original signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Apply DWT\n",
    "        dwt_coeffs = self.dwt_compressor.haar_transform(signal)\n",
    "        \n",
    "        # Step 2: Apply Bernoulli compression on DWT coefficients\n",
    "        compressed_signal = self.bernoulli_compressor.compress(dwt_coeffs)\n",
    "        \n",
    "        return compressed_signal\n",
    "    \n",
    "    def reconstruct(self, compressed_signal):\n",
    "        \"\"\"\n",
    "        Reconstruct signal from hybrid compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        compressed_signal : np.array\n",
    "            Hybrid compressed signal\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed_signal : np.array\n",
    "            Reconstructed signal\n",
    "        \"\"\"\n",
    "        # Step 1: Reconstruct DWT coefficients\n",
    "        dwt_coeffs = self.bernoulli_compressor.reconstruct_l1(compressed_signal)\n",
    "        \n",
    "        # Step 2: Apply inverse DWT\n",
    "        reconstructed_signal = self.dwt_compressor.inverse_haar_transform(dwt_coeffs)\n",
    "        \n",
    "        return reconstructed_signal\n",
    "\n",
    "# ============================================\n",
    "# 4. Enhanced Feature Extraction with Compression\n",
    "# ============================================\n",
    "\n",
    "class CompressedFeatureExtractor(RobustMFCCExtractor):\n",
    "    \"\"\"\n",
    "    Feature extractor with compression capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sr=22050, n_mfcc=40, n_fft=2048, hop_length=512):\n",
    "        super().__init__(sr, n_mfcc, n_fft, hop_length)\n",
    "        \n",
    "    def extract_features_with_compression(self, audio_path, compression_method='bernoulli', \n",
    "                                          compression_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Extract features with optional compression\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        audio_path : str\n",
    "            Path to audio file\n",
    "        compression_method : str\n",
    "            'bernoulli', 'dwt', 'hybrid', or 'none'\n",
    "        compression_ratio : float\n",
    "            Compression ratio (0-1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        features : np.array\n",
    "            Extracted features (compressed or original)\n",
    "        original_features : np.array\n",
    "            Original features before compression\n",
    "        compression_info : dict\n",
    "            Compression metadata\n",
    "        \"\"\"\n",
    "        # Extract original features\n",
    "        original_features = self.extract_mfcc_features(audio_path)\n",
    "        \n",
    "        if compression_method == 'none' or compression_ratio >= 1.0:\n",
    "            return original_features, original_features, {'method': 'none', 'ratio': 1.0}\n",
    "        \n",
    "        # Ensure features are 1D and have appropriate length\n",
    "        features_1d = original_features.flatten()\n",
    "        n_original = len(features_1d)\n",
    "        \n",
    "        # Make length a power of 2 for DWT\n",
    "        if compression_method in ['dwt', 'hybrid']:\n",
    "            n_padded = 2 ** int(np.ceil(np.log2(n_original)))\n",
    "            features_1d = np.pad(features_1d, (0, n_padded - n_original), 'constant')\n",
    "        \n",
    "        compression_info = {\n",
    "            'method': compression_method,\n",
    "            'ratio': compression_ratio,\n",
    "            'original_length': len(features_1d),\n",
    "            'compressed_length': int(len(features_1d) * compression_ratio)\n",
    "        }\n",
    "        \n",
    "        # Apply compression\n",
    "        if compression_method == 'bernoulli':\n",
    "            compressor = BernoulliCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_1d)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        elif compression_method == 'dwt':\n",
    "            compressor = DWTCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_coeffs, mask = compressor.compress(features_1d)\n",
    "            compressed_features = compressed_coeffs[mask]\n",
    "            compression_info['compressor'] = compressor\n",
    "            compression_info['mask'] = mask\n",
    "            \n",
    "        elif compression_method == 'hybrid':\n",
    "            compressor = HybridCompressor(compression_ratio=compression_ratio)\n",
    "            compressed_features = compressor.compress(features_1d)\n",
    "            compression_info['compressor'] = compressor\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown compression method: {compression_method}\")\n",
    "        \n",
    "        return compressed_features, original_features, compression_info\n",
    "\n",
    "# ============================================\n",
    "# 5. Compression Experiment Pipeline\n",
    "# ============================================\n",
    "\n",
    "class CompressionExperiment:\n",
    "    \"\"\"\n",
    "    Pipeline for compression and classification experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path, output_dir='compression_results'):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Experiment configurations\n",
    "        self.compression_methods = ['bernoulli', 'dwt', 'hybrid', 'none']\n",
    "        self.compression_ratios = [0.25, 0.50, 0.75, 1.0]  # 1.0 = no compression\n",
    "        self.classifiers = {\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "            'svm': SVC(probability=True, random_state=42),\n",
    "            'xgboost': XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "        }\n",
    "        \n",
    "    def run_experiments(self, train_folds=list(range(1, 9)), test_folds=[9, 10]):\n",
    "        \"\"\"\n",
    "        Run full compression-classification experiments\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_folds : list\n",
    "            Folds to use for training\n",
    "        test_folds : list\n",
    "            Folds to use for testing\n",
    "        \"\"\"\n",
    "        print(\"=\"*70)\n",
    "        print(\"COMPRESSION-CLASSIFICATION EXPERIMENTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Initialize processor and extractor\n",
    "        processor = UrbanSound8KProcessor(self.dataset_path)\n",
    "        extractor = CompressedFeatureExtractor()\n",
    "        \n",
    "        # Prepare data\n",
    "        print(\"\\nPreparing data...\")\n",
    "        train_paths, train_labels, _ = processor.prepare_data(train_folds)\n",
    "        test_paths, test_labels, _ = processor.prepare_data(test_folds)\n",
    "        \n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_full = label_encoder.fit_transform(train_labels)\n",
    "        y_test_full = label_encoder.transform(test_labels)\n",
    "        \n",
    "        # Store all results\n",
    "        all_results = []\n",
    "        reconstruction_errors = []\n",
    "        \n",
    "        # Run experiments for each compression configuration\n",
    "        for method in self.compression_methods:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Compression Method: {method.upper()}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            for ratio in self.compression_ratios:\n",
    "                print(f\"\\nCompression Ratio: {ratio*100:.0f}%\")\n",
    "                print(\"-\"*40)\n",
    "                \n",
    "                # Extract features with compression\n",
    "                X_train_compressed = []\n",
    "                X_test_compressed = []\n",
    "                X_train_original = []\n",
    "                X_test_original = []\n",
    "                \n",
    "                # Process training data\n",
    "                print(\"Processing training data...\")\n",
    "                for path in tqdm(train_paths, desc=f\"Train {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, _ = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_train_compressed.append(compressed_feat)\n",
    "                    X_train_original.append(original_feat)\n",
    "                \n",
    "                # Process test data\n",
    "                print(\"Processing test data...\")\n",
    "                for path in tqdm(test_paths, desc=f\"Test {method} {ratio}\"):\n",
    "                    compressed_feat, original_feat, comp_info = extractor.extract_features_with_compression(\n",
    "                        path, method, ratio\n",
    "                    )\n",
    "                    X_test_compressed.append(compressed_feat)\n",
    "                    X_test_original.append(original_feat)\n",
    "                \n",
    "                # Convert to arrays\n",
    "                X_train = np.array(X_train_compressed)\n",
    "                X_test = np.array(X_test_compressed)\n",
    "                X_train_orig = np.array(X_train_original)\n",
    "                X_test_orig = np.array(X_test_original)\n",
    "                \n",
    "                # Pad sequences if necessary (for variable length from compression)\n",
    "                if X_train.ndim == 1 or X_test.ndim == 1:\n",
    "                    # Find max length\n",
    "                    max_len = max(\n",
    "                        max([len(x) for x in X_train]) if len(X_train) > 0 else 0,\n",
    "                        max([len(x) for x in X_test]) if len(X_test) > 0 else 0\n",
    "                    )\n",
    "                    \n",
    "                    # Pad sequences\n",
    "                    X_train = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                      for x in X_train])\n",
    "                    X_test = np.array([np.pad(x, (0, max_len - len(x)), 'constant') \n",
    "                                     for x in X_test])\n",
    "                \n",
    "                # Normalize features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Calculate reconstruction error if not 'none'\n",
    "                if method != 'none' and ratio < 1.0:\n",
    "                    avg_error = self.calculate_reconstruction_error(\n",
    "                        X_train_orig, X_train_compressed, method, ratio, comp_info\n",
    "                    )\n",
    "                    reconstruction_errors.append({\n",
    "                        'method': method,\n",
    "                        'ratio': ratio,\n",
    "                        'reconstruction_error': avg_error\n",
    "                    })\n",
    "                \n",
    "                # Train and evaluate classifiers\n",
    "                for clf_name, clf in self.classifiers.items():\n",
    "                    print(f\"  Training {clf_name}...\")\n",
    "                    \n",
    "                    # Train classifier\n",
    "                    start_time = time.time()\n",
    "                    clf.fit(X_train_scaled, y_train_full)\n",
    "                    train_time = time.time() - start_time\n",
    "                    \n",
    "                    # Predict\n",
    "                    start_time = time.time()\n",
    "                    y_pred = clf.predict(X_test_scaled)\n",
    "                    test_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    accuracy = accuracy_score(y_test_full, y_pred)\n",
    "                    precision = precision_score(y_test_full, y_pred, average='weighted')\n",
    "                    recall = recall_score(y_test_full, y_pred, average='weighted')\n",
    "                    f1 = f1_score(y_test_full, y_pred, average='weighted')\n",
    "                    \n",
    "                    # ROC-AUC if available\n",
    "                    try:\n",
    "                        if hasattr(clf, 'predict_proba'):\n",
    "                            y_proba = clf.predict_proba(X_test_scaled)\n",
    "                            roc_auc = roc_auc_score(y_test_full, y_proba, multi_class='ovr', average='weighted')\n",
    "                        else:\n",
    "                            roc_auc = np.nan\n",
    "                    except:\n",
    "                        roc_auc = np.nan\n",
    "                    \n",
    "                    # Store results\n",
    "                    result = {\n",
    "                        'compression_method': method,\n",
    "                        'compression_ratio': ratio,\n",
    "                        'classifier': clf_name,\n",
    "                        'accuracy': accuracy,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1_score': f1,\n",
    "                        'roc_auc': roc_auc,\n",
    "                        'train_time': train_time,\n",
    "                        'test_time': test_time,\n",
    "                        'feature_dim_original': X_train_orig.shape[1] if len(X_train_orig.shape) > 1 else X_train_orig.shape[0],\n",
    "                        'feature_dim_compressed': X_train.shape[1] if len(X_train.shape) > 1 else X_train.shape[0],\n",
    "                        'compression_rate': (1 - ratio) * 100,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    print(f\"    Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Time: {train_time:.2f}s\")\n",
    "        \n",
    "        # Save all results\n",
    "        self.save_results(all_results, reconstruction_errors)\n",
    "        \n",
    "        # Generate comprehensive reports\n",
    "        self.generate_reports(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def calculate_reconstruction_error(self, original_features, compressed_features, \n",
    "                                      method, ratio, comp_info):\n",
    "        \"\"\"\n",
    "        Calculate reconstruction error between original and compressed features\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for orig, comp in zip(original_features, compressed_features):\n",
    "            if method == 'bernoulli':\n",
    "                # Reconstruct using compressor\n",
    "                if 'compressor' in comp_info:\n",
    "                    reconstructed = comp_info['compressor'].reconstruct_l1(comp[:int(len(orig) * ratio)])\n",
    "                    # Pad or truncate to match original length\n",
    "                    if len(reconstructed) < len(orig):\n",
    "                        reconstructed = np.pad(reconstructed, (0, len(orig) - len(reconstructed)), 'constant')\n",
    "                    elif len(reconstructed) > len(orig):\n",
    "                        reconstructed = reconstructed[:len(orig)]\n",
    "                    \n",
    "                    # Calculate error\n",
    "                    error = np.mean((orig - reconstructed) ** 2)\n",
    "                    errors.append(error)\n",
    "            \n",
    "            elif method == 'dwt':\n",
    "                if 'compressor' in comp_info and 'mask' in comp_info:\n",
    "                    # Reconstruct compressed coefficients\n",
    "                    full_coeffs = np.zeros(len(orig))\n",
    "                    full_coeffs[comp_info['mask']] = comp\n",
    "                    reconstructed = comp_info['compressor'].reconstruct(full_coeffs)\n",
    "                    \n",
    "                    # Calculate error\n",
    "                    error = np.mean((orig - reconstructed) ** 2)\n",
    "                    errors.append(error)\n",
    "        \n",
    "        return np.mean(errors) if errors else np.nan\n",
    "    \n",
    "    def save_results(self, all_results, reconstruction_errors):\n",
    "        \"\"\"\n",
    "        Save experiment results to files\n",
    "        \"\"\"\n",
    "        # Save main results\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv(os.path.join(self.output_dir, 'compression_results.csv'), index=False)\n",
    "        \n",
    "        # Save reconstruction errors\n",
    "        if reconstruction_errors:\n",
    "            recon_df = pd.DataFrame(reconstruction_errors)\n",
    "            recon_df.to_csv(os.path.join(self.output_dir, 'reconstruction_errors.csv'), index=False)\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary = self.create_summary_statistics(results_df)\n",
    "        with open(os.path.join(self.output_dir, 'summary.json'), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults saved to {self.output_dir}/\")\n",
    "    \n",
    "    def create_summary_statistics(self, results_df):\n",
    "        \"\"\"\n",
    "        Create comprehensive summary statistics\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'experiment_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'total_experiments': len(results_df),\n",
    "            'compression_methods_tested': list(results_df['compression_method'].unique()),\n",
    "            'compression_ratios_tested': list(results_df['compression_ratio'].unique()),\n",
    "            'classifiers_tested': list(results_df['classifier'].unique()),\n",
    "        }\n",
    "        \n",
    "        # Best results by compression method\n",
    "        best_by_method = {}\n",
    "        for method in summary['compression_methods_tested']:\n",
    "            method_df = results_df[results_df['compression_method'] == method]\n",
    "            if not method_df.empty:\n",
    "                best_idx = method_df['accuracy'].idxmax()\n",
    "                best_by_method[method] = method_df.loc[best_idx].to_dict()\n",
    "        \n",
    "        summary['best_by_method'] = best_by_method\n",
    "        \n",
    "        # Compression vs Accuracy analysis\n",
    "        compression_analysis = {}\n",
    "        for ratio in summary['compression_ratios_tested']:\n",
    "            ratio_df = results_df[results_df['compression_ratio'] == ratio]\n",
    "            if not ratio_df.empty:\n",
    "                compression_analysis[f'ratio_{ratio}'] = {\n",
    "                    'avg_accuracy': ratio_df['accuracy'].mean(),\n",
    "                    'avg_f1': ratio_df['f1_score'].mean(),\n",
    "                    'avg_feature_dim': ratio_df['feature_dim_compressed'].mean(),\n",
    "                    'compression_rate': (1 - ratio) * 100\n",
    "                }\n",
    "        \n",
    "        summary['compression_analysis'] = compression_analysis\n",
    "        \n",
    "        # Overall best configuration\n",
    "        overall_best_idx = results_df['accuracy'].idxmax()\n",
    "        summary['overall_best'] = results_df.loc[overall_best_idx].to_dict()\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_reports(self, all_results):\n",
    "        \"\"\"\n",
    "        Generate visual reports and analysis\n",
    "        \"\"\"\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # 1. Accuracy vs Compression Ratio plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for method in results_df['compression_method'].unique():\n",
    "            method_df = results_df[results_df['compression_method'] == method]\n",
    "            for clf in results_df['classifier'].unique():\n",
    "                clf_df = method_df[method_df['classifier'] == clf]\n",
    "                if not clf_df.empty:\n",
    "                    plt.plot(clf_df['compression_ratio'], clf_df['accuracy'], \n",
    "                            marker='o', label=f'{method}-{clf}')\n",
    "        \n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title('Classification Accuracy vs Compression Ratio', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'accuracy_vs_compression.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Feature Dimension Reduction plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        unique_ratios = sorted(results_df['compression_ratio'].unique())\n",
    "        avg_dims = []\n",
    "        \n",
    "        for ratio in unique_ratios:\n",
    "            ratio_df = results_df[results_df['compression_ratio'] == ratio]\n",
    "            avg_dim = ratio_df['feature_dim_compressed'].mean()\n",
    "            avg_dims.append(avg_dim)\n",
    "        \n",
    "        plt.plot(unique_ratios, avg_dims, 'b-o', linewidth=2)\n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Average Feature Dimension', fontsize=12)\n",
    "        plt.title('Feature Dimension Reduction', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'feature_dimension_reduction.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Heatmap of accuracy by method and ratio\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot_table = results_df.pivot_table(\n",
    "            values='accuracy',\n",
    "            index='compression_method',\n",
    "            columns='compression_ratio',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                   cbar_kws={'label': 'Accuracy'})\n",
    "        plt.title('Average Accuracy by Compression Method and Ratio', fontsize=14)\n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Compression Method', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'accuracy_heatmap.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Training Time Comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for clf in results_df['classifier'].unique():\n",
    "            clf_df = results_df[results_df['classifier'] == clf]\n",
    "            for method in results_df['compression_method'].unique():\n",
    "                method_clf_df = clf_df[clf_df['compression_method'] == method]\n",
    "                if not method_clf_df.empty:\n",
    "                    plt.plot(method_clf_df['compression_ratio'], \n",
    "                            method_clf_df['train_time'], \n",
    "                            marker='s', label=f'{method}-{clf}')\n",
    "        \n",
    "        plt.xlabel('Compression Ratio', fontsize=12)\n",
    "        plt.ylabel('Training Time (seconds)', fontsize=12)\n",
    "        plt.title('Training Time vs Compression Ratio', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'training_time.png'), dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # 5. Create detailed report HTML\n",
    "        self.create_html_report(results_df)\n",
    "    \n",
    "    def create_html_report(self, results_df):\n",
    "        \"\"\"\n",
    "        Create HTML report of experiment results\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Compression-Classification Experiment Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                h1, h2, h3 {{ color: #333; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #f2f2f2; }}\n",
    "                tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "                .best {{ background-color: #d4edda; }}\n",
    "                .summary {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; }}\n",
    "                .image {{ max-width: 100%; height: auto; margin: 20px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Audio Compression-Classification Experiment Report</h1>\n",
    "            <p>Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "            \n",
    "            <div class=\"summary\">\n",
    "                <h2>Experiment Summary</h2>\n",
    "                <p><strong>Total Experiments:</strong> {len(results_df)}</p>\n",
    "                <p><strong>Compression Methods:</strong> {', '.join(results_df['compression_method'].unique())}</p>\n",
    "                <p><strong>Compression Ratios:</strong> {', '.join([str(r) for r in sorted(results_df['compression_ratio'].unique())])}</p>\n",
    "                <p><strong>Classifiers:</strong> {', '.join(results_df['classifier'].unique())}</p>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Overall Best Configuration</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Metric</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find overall best\n",
    "        best_idx = results_df['accuracy'].idxmax()\n",
    "        best_row = results_df.loc[best_idx]\n",
    "        \n",
    "        for metric in ['compression_method', 'compression_ratio', 'classifier', \n",
    "                      'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc',\n",
    "                      'train_time', 'test_time', 'feature_dim_compressed']:\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{metric.replace('_', ' ').title()}</td>\n",
    "                    <td>{best_row[metric]:.4f if isinstance(best_row[metric], float) else best_row[metric]}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Top 10 Performances</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Rank</th>\n",
    "                    <th>Method</th>\n",
    "                    <th>Ratio</th>\n",
    "                    <th>Classifier</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>F1 Score</th>\n",
    "                    <th>Feature Dim</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sort by accuracy and get top 10\n",
    "        top_results = results_df.sort_values('accuracy', ascending=False).head(10)\n",
    "        for idx, (_, row) in enumerate(top_results.iterrows(), 1):\n",
    "            html_content += f\"\"\"\n",
    "                <tr class=\"{'best' if idx == 1 else ''}\">\n",
    "                    <td>{idx}</td>\n",
    "                    <td>{row['compression_method']}</td>\n",
    "                    <td>{row['compression_ratio']}</td>\n",
    "                    <td>{row['classifier']}</td>\n",
    "                    <td>{row['accuracy']:.4f}</td>\n",
    "                    <td>{row['f1_score']:.4f}</td>\n",
    "                    <td>{int(row['feature_dim_compressed'])}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Visualizations</h2>\n",
    "            <p>The following visualizations have been generated:</p>\n",
    "            <ul>\n",
    "                <li>accuracy_vs_compression.png - Accuracy vs Compression Ratio</li>\n",
    "                <li>feature_dimension_reduction.png - Feature Dimension Reduction</li>\n",
    "                <li>accuracy_heatmap.png - Accuracy Heatmap</li>\n",
    "                <li>training_time.png - Training Time Analysis</li>\n",
    "            </ul>\n",
    "            \n",
    "            <h2>Detailed Results Table</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Method</th>\n",
    "                    <th>Ratio</th>\n",
    "                    <th>Classifier</th>\n",
    "                    <th>Accuracy</th>\n",
    "                    <th>Precision</th>\n",
    "                    <th>Recall</th>\n",
    "                    <th>F1</th>\n",
    "                    <th>Train Time</th>\n",
    "                    <th>Test Time</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in results_df.iterrows():\n",
    "            html_content += f\"\"\"\n",
    "                <tr>\n",
    "                    <td>{row['compression_method']}</td>\n",
    "                    <td>{row['compression_ratio']}</td>\n",
    "                    <td>{row['classifier']}</td>\n",
    "                    <td>{row['accuracy']:.4f}</td>\n",
    "                    <td>{row['precision']:.4f}</td>\n",
    "                    <td>{row['recall']:.4f}</td>\n",
    "                    <td>{row['f1_score']:.4f}</td>\n",
    "                    <td>{row['train_time']:.2f}s</td>\n",
    "                    <td>{row['test_time']:.2f}s</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <footer>\n",
    "                <p>Experiment conducted using UrbanSound8K dataset</p>\n",
    "                <p>Compression methods: Bernoulli, DWT (Haar), Hybrid</p>\n",
    "            </footer>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save HTML report\n",
    "        with open(os.path.join(self.output_dir, 'experiment_report.html'), 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"\\nHTML report generated: {self.output_dir}/experiment_report.html\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Main Execution\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"AUDIO COMPRESSION-CLASSIFICATION EXPERIMENTAL PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize paths\n",
    "    DATASET_PATH = \"UrbanSound8K\"\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "        print(\"\\nPlease download UrbanSound8K dataset from:\")\n",
    "        print(\"https://urbansounddataset.weebly.com/urbansound8k.html\")\n",
    "        print(\"\\nExtract it to the current directory as 'UrbanSound8K'\")\n",
    "        return\n",
    "    \n",
    "    # Create experiment instance\n",
    "    experiment = CompressionExperiment(DATASET_PATH)\n",
    "    \n",
    "    # Run experiments\n",
    "    results = experiment.run_experiments()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load and display summary\n",
    "    summary_path = os.path.join('compression_results', 'summary.json')\n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\nTotal experiments conducted: {summary['total_experiments']}\")\n",
    "        \n",
    "        # Show overall best\n",
    "        best = summary['overall_best']\n",
    "        print(\"\\nOVERALL BEST CONFIGURATION:\")\n",
    "        print(f\"  Method: {best['compression_method']}\")\n",
    "        print(f\"  Ratio: {best['compression_ratio']}\")\n",
    "        print(f\"  Classifier: {best['classifier']}\")\n",
    "        print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "        print(f\"  Feature Dimension: {int(best['feature_dim_compressed'])}\")\n",
    "        print(f\"  Compression Rate: {best['compression_rate']:.1f}%\")\n",
    "        \n",
    "        # Show best by method\n",
    "        print(\"\\nBEST BY COMPRESSION METHOD:\")\n",
    "        for method, config in summary['best_by_method'].items():\n",
    "            print(f\"  {method}: {config['accuracy']:.4f} \"\n",
    "                  f\"(Ratio: {config['compression_ratio']}, \"\n",
    "                  f\"Classifier: {config['classifier']})\")\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"\n",
    "    Quick test to verify compression algorithms\n",
    "    \"\"\"\n",
    "    print(\"Running quick compression test...\")\n",
    "    \n",
    "    # Generate test signal\n",
    "    np.random.seed(42)\n",
    "    test_signal = np.random.randn(1024)\n",
    "    \n",
    "    print(f\"Original signal shape: {test_signal.shape}\")\n",
    "    \n",
    "    # Test Bernoulli compression\n",
    "    print(\"\\n1. Bernoulli Compression (50%):\")\n",
    "    bernoulli = BernoulliCompressor(compression_ratio=0.5)\n",
    "    compressed = bernoulli.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {compressed.shape}\")\n",
    "    print(f\"   Compression: {len(compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test DWT compression\n",
    "    print(\"\\n2. DWT (Haar) Compression (50%):\")\n",
    "    dwt = DWTCompressor(compression_ratio=0.5)\n",
    "    compressed_coeffs, mask = dwt.compress(test_signal)\n",
    "    print(f\"   Compressed coefficients: {np.sum(mask)}\")\n",
    "    print(f\"   Compression: {np.sum(mask)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    # Test hybrid compression\n",
    "    print(\"\\n3. Hybrid Compression (50%):\")\n",
    "    hybrid = HybridCompressor(compression_ratio=0.5)\n",
    "    hybrid_compressed = hybrid.compress(test_signal)\n",
    "    print(f\"   Compressed shape: {hybrid_compressed.shape}\")\n",
    "    print(f\"   Compression: {len(hybrid_compressed)/len(test_signal)*100:.1f}% of original\")\n",
    "    \n",
    "    print(\"\\nQuick test completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Audio Compression-Classification Pipeline\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # First run quick test\n",
    "    quick_test()\n",
    "    \n",
    "    # Ask user if they want to run full experiments\n",
    "    response = input(\"\\nDo you want to run full compression-classification experiments? (yes/no): \")\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        main()\n",
    "    else:\n",
    "        print(\"\\nQuick test completed. Run full experiments when ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7966830-69e5-4159-bc65-45c5f12ee87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
